{"posts":[{"title":"8月21日","content":"明天是田中井律的生日，看轻音少女时会关注她和澪，澪是超人气角色，但是看番时还是希望大家能多关注律，狗门！ ","link":"https://porterlu.github.io/post/8-yue-21-ri/"},{"title":"记录","content":" Life is not about finding yourself, but creating yourself. 荔园有一种日本街道的美感。 ","link":"https://porterlu.github.io/post/ji-lu/"},{"title":"29夜","content":"又看了《提问的智慧》，多一点思考而不是无头苍蝇似地乱蹦，我不知道什么时候已经开始下意思忽略这一点了，和志同道合的人做一点喜欢的事总是好的，接下来希望能够持续输出一些东西。 狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门狗门... ","link":"https://porterlu.github.io/post/29-ye/"},{"title":"7月28夜有感","content":"低血压和低血糖的问题愈加严重了，少熬夜，进入低功耗模式，狗门 ","link":"https://porterlu.github.io/post/7-yue-28-ye-you-gan/"},{"title":"暑期思考","content":" 科研的本质是否在于炼丹 世界上只有一种英雄注意 狗门。 ","link":"https://porterlu.github.io/post/shu-qi-si-kao/"},{"title":"VIRTIO Debug感想","content":"已经Debug好几天了，抓耳挠腮，自我折磨，将Virtio换成非legacy就成功了。还是得多看spec，抓紧科研板砖，要不要办个港澳通行证呢，找学姐买个打折的iphone。狗门！！！！！！！！！！！！！！！ ","link":"https://porterlu.github.io/post/virtio-debug-gan-xiang/"},{"title":"可信执行环境的安全启动和远程认证","content":"Sanctum Bootloader ​ 在可信执行环境中，如SGX，提供了向远程用户证明用户的程序被安全启动的功能。这里主要分为两个步骤： 证明平台是可信的 证明用户的程序被安全启动了 ​ 第一个步骤主要涉及安全启动，第二个主要涉及远程证明。在RISC-V平台上，Sanctum处理器提出了Sanctum Bootloader，被之后许多RISC-V可信执行环境采用。完整的Sanctum的Bootloader具有比SGX的安全引导更高的安全性，它甚至将芯片厂商作为一个半诚信的存在，即相信硬件厂商会诚信地制造安全硬件但会试图获取私钥信息，而传统的SGX需要我们完全相信Intel。 背景 ​ 为了向远程证明一个程序已经被安全地执行，需要验证程序的宿主系统的信息。远程的用户总是希望他们的计算可以跑在安全的设备上，如Intel SGX和Sanctum处理器，它们都为计算提供了机密性和完整性的保证。如果用户想确保它的计算应用可以跑在一个这样的系统上，那么就需要为硬件创造一个独一无二并且不可改变的密码学身份。使用这种硬件的系统可以向它的用户证明其身份，具体是通过和硬件厂商提供的认证平台列表进行比对就可以确认。 ​ 为了保证真实性，处理器需要证明自己被正确地启动了，特别是涉及到之后远程证明地代码不能修改，于是需要一个安全引导来做这个保证。这个Bootloader需要保证系统已经配置到远程用户想要的状态，并且启动之后，处理器也需要向用户证明执行程序的安全容器Enclave被设置为正确的状态。为了完成后面一点，处理器需要提供这个容器的测量值证书来供远程用户进行验证。 ​ 在一个通用的设置中，处理器硬件中写死在Bootrom中的固件会对Bootloader和Bootloader加载的软件进行一次密码学测量，之后处理器使用私钥对这个测量值进行签名，就有了一份证书。那么远程或者本地的用户就可以使用处理器厂商提供的公钥对于这个签名进行验证，并对于这个测量值进行验证来保证系统的代码没有被修改。 ​ 为了生成如上的一份证书，处理器需要有一对公私钥。通常来说，处理器厂商会为每一个芯片生成一对公私钥并且将私钥嵌入到处理器中，通常来说会将其存入到一处安全的非易失性处理器中。在这个场景中，这个硬件制造商会知道这个芯片的私钥，所以生产阶段就从某种程度上说泄露了。取而代之的是，Sanctum处理会用一个硬件随机数生成器（TRNG）来生成一个随机数种子存储到安全的非易失性存储器中，之后用于生成处理器的公私钥。这时候硬件提供者对生成的公钥进行签名，之后将其存储一个可以被读取到非易失性存储器中，但是私钥本身不会离开处理器，所以即使是硬件提供商也不知道这个私钥。因此，这里的硬件提供者是诚实的，但是他会试图获取机密信息。 ​ Sanctum处理器使用开源的RISC-V Rocket chip架构作为基础，这个处理器不需要处理器厂商显式地赋予一个私钥。取而代之的是，处理器从PUF（物理不可克隆单元，可以当作一个PRF）中提取自己的密码学身份。这个生成的密钥可以用于签名引导的可信软件，之后就会被马上销毁。这个签名过的可信Payload程序负责向远程进行证明。一个远程用户可以通过Diffie-Hellman来建立安全通信同时要求获取Payload程序的签名。我们可以通过验证签名来保证这个系统的真实性和完整性。之后一旦接收到用户的代码，就需要证明用户的程序已经被正确地设置了。 威胁模型 ​ 我们的目标就是使用PUF结合生产者生产的信根来生成提升密钥生成的安全性。在这个过程中生产者不再存储密钥，所以不会存在多个系统使用同一个密钥。我们考虑这个厂商会诚实地构造PUF和CPU不会去留后门。厂商必须为生产可信根做背书。但是这个厂商可以是“好奇的”，提可以试图获取更多的信息。具体例子就是，一个厂商可以通过多次运行PUF生产密钥来获取信息。 ​ 信根（Boot ROM中的代码）可以正确从PUF的输出来生成密钥，并且测量之后的Payload代码。我们假设系统内存是可信的，只能通过Processor访问，并且可以在初始时清空，保护密钥和其他机密信息不被访问。如果DRAM内存在TCB可信基之外，我们可以使用内存加密，就像XOM和Aegis处理器一样。 信根 ​ 在启动时，处理器会执行可信的&quot;ROM&quot;中的一系列代码。这个过程被称为一阶段引导，是我们的可信根，之后将加载Payload，是从不可信的存储设备进行读取，所以要进行验证Payload的哈希值；这个过程还会对Payload使用处理器的私钥进行签名。 ​ 如果处理器拥有多个核，我们可以使用Core 0来执行可信的引导代码。 ​ 因为内存在Boot阶段内存是未定义的，所以通过传统的使用内存来实现的锁来进行同步是不可行的；取而代之的是，这时需要依赖等待中断。Core 0通核间中断会唤醒其他的处理器。所有的核心会清空处理器状态来防止攻击者获取一些机密信息。同时，信根会清除为初始化的内存。在信根的实现过程中，我们依赖一个SHA3哈希和ed25519椭圆曲线加密。这一节将介绍我们如何实现信根来生成密钥。 ​ 考虑云FPGA的场景，这个平台是多个客户分时共享的。在这个场景下，云服务的操作员就可以视为那个硬件厂商，非易失性存储的密钥在这个场景就没有意义也不容易实现。如图Fig.2. (PKD,SKD)(PK_D,SK_D)(PKD​,SKD​) 是从一个哈希随机值生成的安全处理器的密码学实体，重启之后就会消失。处理器在Boot阶段就会将生成一个PKDPK_DPKD​, 厂商这里就是云FPGA连接的服务器主机就会对这个公钥进行签名。于是这个签名在厂商的保证下就可以得到远程用户的信任。处理器最终会使用设备私钥生成一个签名证书，并且提供SignM(PKDEV)Sign_M(PK_{DEV})SignM​(PKDEV​) 来使得远程用户信任这个平台，而不是有人恶意对验证协议进行模拟。 ​ 在某些场景下，不存在可信的一方可以在处理器启动时进行本地验证，机器部署在一个不可信的环境中，远程验证建立一个可信的公钥上。同时这个密钥必须重启后仍然能够保持，但是这个密钥不能被攻击者获得。为了解决如上 的威胁模型，如图3中所示，P256和P512会使用Identical Ring Oscillator Pair和一个Trapdoor Fuzzy Extractor来生成一个可以重复的M-bit机密信息e，即M可以是256或者512位。 ​ 1）使用PUF来初始化预分配密钥：一旦安全处理器被生产了，可信根中就会有一个私密的128bits信息。b=As+eb = As + eb=As+e 。为了防止这个厂商重复生成这个密钥，信根需要一次性非易失性存储fuse实现。但是在FPGA平台上，在选择s之前，程序会确定fuse并没有被设置，程序会阻塞直到这个值被设置。如果这个fuse被设置，信根就不会再设置一个新的s，之后会试图通过辅助数据AAA将其从不可信存储中复原。 ​ 2）密钥复原过程：在每一次的重启过程中，图4中展示复原的过程。 ​ 信根会根据Payload生成(PKP,SKP)(PK_P, SK_P)(PKP​,SKP​), 并且是设备私钥进行签名，这样就可以作为一个证书。处理器会使用PUF生成的对称密钥对(SKD,SKPayload)(SK_D,SK_{Payload})(SKD​,SKPayload​)进行加密，将其存储在不可信的非易失性存储器上。 远程验证 ​ 一个不可变的硬件可信根可以进行远程证明。使用PUF来生成平台的密钥，这样密钥就绝对不会被第三方知道。 通过Payload进行远程验证 ​ 这个系统阶段的Payload将负责远程验证，Payload的状态在(PKP,SKP)(PK_P,SK_P)(PKP​,SKP​)生成阶段也将被验证。远程的客户首先需要发起一次Diffe-Hellman协议，来建立和平台之间的安全通信，一旦接收到了安全参数g、gA、pg、g^A、pg、gA、p , 那么Payload就会选择一个随机数B，并且计算gBg^BgB。之后这个Payload会发送SignSKDEV(PKpayload,H(payload),Diffe−Hellman参数)Sign_{SK_{DEV}}(PK_{payload},H(payload),Diffe-Hellman参数)SignSKDEV​​(PKpayload​,H(payload),Diffe−Hellman参数)返回给远程客户。用户根据其中的PKpayloadPK_{payload}PKpayload​ 和 H(payload)H(payload)H(payload)来验证这正是想要的机器。建立好Diffe-Hellman的可信通道后，就可以传输想要执行的代码和其数据，服务器平台会返回一个程序哈希的签名。这样就可以建立远程客户对于服务器平台的信任。 Sanctum处理器的远程验证 ​ 图7就是一个具体的例子，它实现在Sanctum处理器上。和上面中介绍的不同就是，这里Payload是Security Monitor可信软件，它负责进行Client Enclaves之间的隔离，同时有一个特殊的“Signing Enclave”。只有“Signing Enclave”可以访问Payload生成的密钥，在初始化一个Client Enclave后，它的哈希值会被传送到“Signing Enclave”，使用SKPSK_PSKP​进行签名。我们依赖Sanctum处理器提供的隔离来保证签名的机密性。在Enclaves之间传送消息并不需要密码学操作，Sanctum实现了消息传送机制。 匿名验证 ​ 之前介绍的协议都不支持匿名认证，但是现在的构造并不能兼容匿名认证。为了将现在的方案转化为匿名认证方案，需要一个诚信放来处理组成员认证和撤回。这个诚信方往往指的就是硬件厂商，它负责管理公钥组进行设备认证。厂商还需要跟踪已经被攻陷的平台，来保证这些公钥已经被撤回。举个例子就是，为了实现直接证明验证，一个平台应该是可以被证明是真实的，但是厂商应该对于公钥对应的私钥应该是零知识的。可以对于加密的消息进行签名，允许厂商去验证这个平台是真实的，但是并不知道是哪个平台。验证时，平台会发送一个签名和厂商给予的证书，客户会验证基于厂商的证书。 RISC-V Keystone ​ Sanctum的Bootloader也在RISC-V keystone上实现，但是它没有具体的硬件实现。和上面介绍的相似它的Payload验证包含Security Monitor和Security Monitor Public Key的签名报告。而Enclave的验证需要Enclave本身哈希值和Enclave数据的签名报告。前者使用处理器私钥签名，后者使用Security Monitor私钥签名。这样远程用户可以验证处理器本身和程序是否被正确启动。 ","link":"https://porterlu.github.io/post/ke-xin-zhi-xing-huan-jing-de-an-quan-qi-dong-he-yuan-cheng-ren-zheng/"},{"title":"Sok：Hardware-supported Trusted Execution Environment","content":"概述 ​ 现代计算模型日益复杂还有软件对于强隔离机制的需求使得越来越多的系统采用了可信执行环境。近几年，已经出现了许多工业界和学术界的可信执行框架，它们仍然很难进行比较对比。更普遍地说，现有的可信执行环境架构没有被全面系统化，不能很好理解各个可信执行环境之间不同和优缺点。 ​ 因此，本篇工作中，我们分析了现有可信执行环境的设计并且系统化了可信执行环境中用实现它们安全目标的机制，包括可验证的启动、运行时隔离、可信IO和安全存储。更确切地说，我们分析了在可信执行环境方案下的典型体系结构构建模块，和各个组件可供的选择及其之间的权衡。我们专注于硬件辅助可信执行环境，并且覆盖了一系列工业界和学术界的可信执行环境。我们的分析显示即使各种可信执行环境的安全目标、使用模型和指令集体系结构不同，但是它们在设计中使用很多相同的构件。 ​ （可信执行环境后文中称为TEE) 介绍 ​ 今天的计算平台有各种各样的架构，软件模型和支持的应用类型。从云计算中的大规模服务器到智能家居设备。这些设备逐渐需要存储和处理一些安全敏感的数据来完成一些诸如医疗服务、金融服务领域的工作。因此，现代计算平台一定会实现一些机制来保护安全敏感数据来防止非授权访问和修改。 ​ 在当今计算系统中，数据的机密性和完整性不仅需要防御来自网络的攻击，还需要防御那些来自同一平台下软硬件的攻击或者直接的物理攻击。这是因为在系统上存在多个不可信的软件组件。例子包括在相同的云计算平台上有来自不同客户的数据，在一部手机上为网络提供商和用户的数据。于是，如今的计算平台需要将敏感数据和一些潜在的软件攻击还有硬件攻击者相隔离。实际上，这个需求会扩展到任何涉及到敏感数据的应用中。 ​ 而面对这些需要保护敏感数据计算的解决方案就是TEE，现在不同TEE的安全目标会有不同 但是大部分有如下四个安全目标：1）机密数据和代码的可验证启动，这样远程就可以确认应用被安全并且正确地启动了。2）运行时对于数据和代码机密性和完整性的保护。3）可信IO，这样就可以安全地访问外设或者加速器。4）TEE数据的安全存储。 ​ 现在存在一系列工业界和学术界的TEE，它们底层的安全假设、指令集架构以及使用模型会有不同。在这篇工作中，我们分析了现有TEE的设计，并且系统化地分析了这些TEE中的通性和独特之处。即使许多TEE对于底层的机制使用不同名字和描述，但是这些机制是十分相似的，我们将底层的机制进行分组并且强调一些例外。 ​ 在我们开始学习这些TEE解决方案之前，我们先定义通常大部分TEE所考虑的敌手。我们专注于硬件辅助TEE并且覆盖了验证启动、运行时隔离、安全IO和安全存储。分析不用TEE所支持的这些特性，以及不同TEE是如何实现这些特性的，并分析这些TEE所考虑的攻击者。 ​ 第一个分析的TEE安全目标就是可验证的启动，就是提供TEE初始状态的正确性证明。这个典型地，是通过建立可信根的测量值完成的，之后通过它建立TEE中代码和数据的测量值。之后在一个叫做attestation的过程中进行验证。标准的测量和attestation过程是由可信平台模块（TPM）完成的。从很大程度上，今天的attestation方案包含了相似的机制和协议，但是根据不同架构又做了很多的演变。例如，在一些TEE中，attestation密钥和测量值存储在芯片内部的模块，但是也很多存在芯片外的模块。密钥的层次涉及了需要现代attestation协议，可能和标准的TPM-based协议不同，一些需要引入对称密钥来解决性能问题。 ​ 之后我们将关注TEE如何实现运行时的隔离来保证数据的机密性和完整性。我们对于隔离机制进行了分类，分类从资源分区和强制隔离两个维度进行分析。我们之后使用这些维度对于不同TEE使用的技术进行分析。即使每一种策略对于每一种资源都有相应的优缺点，但是我们分析对于各种攻击者的攻击下，CPU和内存的隔离在哪种策略有更好的适应性。之后通过描述架构上的隔离组件来总结不同TEE如何使用这些策略。相比之下，内存的保护机制是多样的，一些TEE会采用机制来对应不同的攻击者。 ​ 安全IO或者说可信IO从支持用户IO到各种加速器。大多数的可信IO方案都包含两个主要组件：一条到设备的可信路径和可信设备用于像CPU一样提供敏感数据的保护。我们分析两种TEE使用的可信路径，分别是逻辑上的和密码学上的。之后，我们介绍不同方法去实现可信路径，包括它们在不同场景下的适用性还有架构上的支持。最后我们使用在上一节中的分配策略去理解不同可信设备架构。 ​ 安全存储就是保证持久存储的敏感数据只能被可信实体获取。这个概念涉及到sealing。和测量和attestation相似，早期的sealing机制依赖于TPM。我们发现sealing机制为了适应需求已经演进了好几代，现在只依赖芯片内的组件。我们揭示了现在TEE只有三分之一有sealing支持，并且大都采用了相似机制。 ​ 综上所述，这篇文章对于理解现在的TEE架构做出了如下的贡献： 这篇文章介绍了各种攻击模型，包括软件的、硬件的和它们对应的能力。我们认为对于攻击者的分类，我们认为这样的分类在设计TEE时，对于选择怎么样选择机制并分析其安全性是很大的意义的。因为这根源于TEE的设计不仅要考虑到保护的是什么资源，还要考虑攻击者的能力。 据我们了解，我们这篇文章是第一篇对于TEE为了达到安全目标时选择的机制进行分析分类的文章。我们认为这种系统化的分析可以作为设计新的TEE时的基础。 基于调查的TEE架构，我们总结出了四个安全目标的设计空间是相对小的，新的设计往往利用之前的设计选择，总是在统一组件下做出很小的修改。 研究范围 ​ 近年来提出了各种TEE架构，奇怪的是，即使出现了大量的TEE解决方案，并没一个被广泛接受的TEE定义。在这篇文章中，我们不准备去寻找这样一个统一的定义。取而代之的是，我们调研了现在的一系列方案并系统化的根据四个安全目标进行分析。根据现有TEE设计的不同，我们试图找到底层设计决策来连接这些相似的方案。因为TEE的性能往往是否处理器本身决定的，我并不调研各种TEE之间的性能差异。 我们专注于硬件辅助TEE，并且不调研更加复杂的软件方法，比如基于可信虚拟机。 我们考虑不同架构上的TEE，我们选择了四种处理器架构：x86、ARM、POWER和RISC-V。同时我们也分析在SPARC和OpenRISC上方案。 一些提案有时候不被视为TEE(如ARM Trustzone)，我们会将这些尽可能放到TEE的框架中。 我们故意地不考虑一些基于协处理器的方案如Google Titan和Apple's Secure Enclaves，或者基于硬件安全模块。我们专注于通用处理器上的方案。 术语： ​ 在学术界和工业界，下列几个名字往往用于指代TEE。 Intel SGX，TEE实体叫做Enclave TDX和AMD SEV-SNP，实体叫做安全虚拟机 ARM CCA ​ 这篇文章中使用enclave指代TEE实体，使用TCB指代底层的可信组件。我们使用TEE指代整个用于创建Enclave的架构。 系统和敌手模型 A. 系统模型 ​ 大多数TEE都是定位在一个通用处理器上，并且包含了一个SoC，SoC中会有片外存储，外设等等。SoC上可以包含一个或者多个核，同时共享一个Cache，这个Cache下连接到一个内存控制器，同时也有一个IO多选器用于连接外设到SoC中。典型的软件栈包含操作系统，多个用户程序。如果实现了虚拟化，还会有一个hypervisor，其上会运行一个或者多个虚拟机。用于完成完成TEE安全保护机制的软硬件称为TCB。 B. 敌手模型 ​ TEE用于在同一平台对一系列攻击者进行防御。这些不可信的软硬件包括其他enclave，操作系统，不可信的IO设备，物理攻击者等等。 ​ Enclave攻击者：这种攻击发生一个平台支持同时运行多个Enclave的情况。这种情况十分常见，例如，在多用户的云服务平台，在同一硬件平台上，不同用户可以在之上运行Enclave。在移动平台上也有这种情况，多个服务提供商也会同时运行Enclave。 ​ 来自非特权软件的攻击：例子有，在云服务平台上，控制客户虚拟机启动和Enclave运行在同一特权级的程序。 ​ 特权软件的攻击：这里的攻击者指的是具有系统管理功能的软件，如OS。同样这里的攻击者也包括云服务平台上的hypervisor，移动平台上的OS。 ​ 启动阶段的攻击：这种攻击指的是控制系统的启动阶段，之后就可以控制TEE。这种攻击者通过例如内存或者IO的错误配置，来导致整个TEE的错误。这种攻击者可以是一个恶意的BIOS。 ​ 外设攻击：很多外设并在Enclave的TCB内。这些外设可以是在SoC内的，也可以是通过外部总线接入的。这些外设通常都假设为不可信。 ​ 来自总线的攻击。 ​ 侵略性的攻击：攻击可以发动侵略性的攻击，诸如探测底层芯片、操纵时钟和发动电压攻击来窃取密钥或者控制执行路径。 C. 侧信道攻击 ​ 多个基于物理的、微结构的侧信道攻击已经可以在TEE平台上发起。这种攻击是是因为Enclave使用一些没有专门分配给Enclave的共享的计算资源。TEE往往依赖专门的对策来应对这类攻击。例如，专门的软件对抗方法包括消除机密信息的内存访问或消除依赖机密信息的分支。 可验证的启动 ​ Enclave安全执行的第一步就是安全的设置过程，这样才能保证Enclave的执行环境被正确配置。流行的验证方法就是Enclave测量和attestation。从直觉上讲，对于一个Enclave的测量就是记录一个初始状态，更加具体地讲，就是一系列密码学的哈希操作。这个测量过程本身必须是可信的。这个过程必须开始于可信测量根RTM，最终用于测量Enclave。这个Enclave测量值之后也将用于数字签名报告交给一个验证器进行验证。 A. RTM ​ 可信启动的核心就是RTM，目前的TEE存在三种方案，静态的、动态的、硬件的。 ​ 静态的，即SRTM。这个可信链条，要从系统启动到enclave运行保持住。这种方案进行时各种不可信部分往往还未启动，除了基于总线的攻击者，所以这种启动方式要做到不使用片外的设备。典型的SRTM仅仅依赖硬件或者不可变的BootROM中的软件。 ​ 相对的，DRTM可以不信任在它之前的代码，建立一个新的RTM。这种实现必须实现体系结构上的拓展，使得Enclave启动时有办法对抗一些不可信的组件。这个通过特殊的指令挂起其他的不可信的软件同时disable所有的IO设备和中断。之后，硬件会加载、测量、验证经过签名的代码模块来作为Enclave的可信基。 B.测量 ​ 在STRM和DRTM的过程中，每一个在信任链上的实体都是Enclave的可信基，在转移到下一个组件前都会对其进行测量，同时还会进行进行完整性检查。几乎所有的TEE都采用了类似的方式。一旦一个组件的测量完毕，必须将这个测量存储到一个攻击者无法访问到的地方。 C. Attestation ​ 这是可验证启动的最后一个环节，验证器会验证一个Enclave的初始状态是否正确。就是一个Enclave的Hash值和可信基是否符合预期的值。有两种attestation，一种是本地的，就是enclave和验证器在同一个平台上。相反，如果是远程的attestation，enclave和验证器将不在一个平台上。远程attestation依赖非对称加密，会引发更大的检查开销。相反，本地的attestation会使用对称加密，这会更加高效。事实上，大对数TEE都支持远程attestation，只有少数TEE同时支持两个。 运行时隔离 ​ 在正确设置完Enclave后，就可以开始执行。为了防止攻击者干扰Enclave的执行，所有Enclave拥有的资源如CPU和内存，不能被未授权的软件访问。这种保护机制我们称之为运行时隔离。接下来，我们将介绍一系列的隔离策略分类，之后介绍如何将这种策略应用到CPU和内存的隔离上。最后，我们调研了现有的TEE使用的隔离策略。 A. 隔离策略的分类 ​ 总的来说，隔离机制是为了实现受保护资源的机密性和完整性。 ​ 资源分区：大体分为时间、空间上的隔离。 ​ 对时间上的隔离，往往是将资源分配到一个时间域上，随着时间将资源分给不同的执行上下文。在任何一个时间点，只有一个执行上下文能够访问对应的资源。时间上的分区隔离需要一个机制去安全切换上下文。这个安全切换需要较快，这样才不会影响系统的整体性能。时间上的隔离往往用于那些空间上进行隔离很困难，多个上下文同时进行访问是不必要的情况。 ​ 对空间上的隔离，资源往往分为可信上下文和非可信上下文的部分，使用独立的分区。这用于同一资源有多个或者一个资源可以细分为多个的情况。同时一个上下文可能被给与多个资源实体。从结果上，空间隔离技术往往用于那些相对容易复制或者分离的设备上。 ​ 时空隔离：提供了更多灵活性和性能优化。 ​ 另外一种就是强制隔离：分为逻辑上的和基于密码学的 ​ 逻辑上的隔离利用了一些访问控制机制来实现强制隔离，这些机制会防止攻击者访问受保护的数据。例如，在一个可信的上下文切换中，这个机制将保证下一个执行的线程不能任何受保护的数据。另一方面，许多隔离机制会拦截访问请求并且检查访问控制信息。这里的访问控制信息，必须由系统的可信组件进行生成和管理。如果要实现灵活的管理，会需要在运行时修改控制信息，用于实现灵活的再分配。对于这些资源的访问控制信息需要根据访问的粒度和资源的类型进行变化。更进一步地，这些访问控制信息本身需要被保护。 ​ 另外一种是基于密码学方法的隔离，通过机密可以实现机密性，只有经过认证过的下文可以得到密钥解密数据。而完整性认证可以通过消息认证码的方式实现。这样就可以阻止恶意的程序去修改密文发起攻击。对于完整性的保护需要对抗重放攻击。 B. CPU隔离 ​ 在本篇文章的讨论中，专注于保护CPU中架构相关的寄存器状态。 ​ CPU隔离策略的选择，往往使用时间上的隔离，因为这样不用添加任何运行时的检查。 ​ 而密码学的隔离方法有很大的性能开销。 ​ 对于CPU隔离的架构支持，就像之前提到的那样，实现安全的上下文切换的方法就是利用分时复用，对于上下文中的寄存器进行存储、清除、恢复。这个切换过程必须保证Enclave中的数据不被泄露。另外，这个CPU恢复Enclave执行状态的过程也不能被干扰。为了保证TCB可以充分协调每一次上下文切换，TEE利用多个特权级来完成这件事。 ​ 除了上下文切换，CPU模式和特权级是TCB组件和安全运行Enclave所必要的，这允许运行在高特权级的TCB可以实现一些安全机制例如测量，attestation，内存隔离，可信IO，安全存储等等。 C. 内存隔离 ​ TEE设计需要保证Enclave使用的内存在运行时不被未授权地访问或者修改。这种保护不仅仅覆盖了off-chip的保护，也包括片内的一些指令或者Cache。于此同时，大多数TEE支持虚拟存储，这个地址转化过程的可信是内存隔离的关键。同时对于处理器Cache，TLB存有最近的地址翻译，一定得保护其不被误用或者错误配置。 ​ 对于内存隔离策略的选择： ​ 所有之前讨论的隔离策略都可以用到内存隔离中，但是TEE使用的内存隔离策略会更加灵活。对于采用内存完全分区的策略，有专门的内存区域划分给TCB和Enclave。这种策略在只用支持有限数量的Enclave的系统中可以很好地工作。这种内存资源要求相对静态并且预先已经知道，同时对于隔离粒度不高的应用很适合这种策略。 ​ 完全分时隔离需要允许内存只能被当前活跃的Enclave访问，并且在上下文进行切换时需要能安全地存储和恢复寄存器。这种隔离策略适用的场景十分有限，需要无论何时只有一个执行上下文是有效的情况。因此在需要并行运行Enclave的情况不是很搞笑。另外，同时将内存的内存存入磁盘需要耗费大量的时间。所以，这种策略很少被使用。 ​ 时空同时隔离的策略，也是更为常用，更为灵活的。内存区域可以被不同Enclave分时复用。 ​ 对于逻辑强制隔离依赖访问控制信息，来使得只有经过授权的访问才能访问enclave的资源。这种隔离需要对于每一次访问都进行检查，比如通过MMU就可以实现。 ​ 对于密码学的强制隔离机密性通过加密实现，通过MAC来完成完整性的保证。为了防御重放攻击还需要记录一些刷新的信息，比如counter，我们常常使用merkle-tree来进行具体的实现。和上面的隔离策略形成对比的是，基于密码学的隔离可以对抗总线攻击者。但是，基于密码学的隔离很难进行扩展，特别是在不同的上下文需要使用不同的密钥的时候，因为它需要在SoC上存储大量密钥。 ​ 在调研TEE的过程中，我们发现了一系列的策略。许多的TEE会同时使用多种策略。比如Intel SGX会使用时空隔离加上逻辑隔离保护Enclave Memory。但是它用了空间隔离隔离的方法保护TCB，同时使用加密的方法对抗总线攻击者。 ​ 对于内存隔离的架构支持：通过一个访存控制检查可以进行一次检查，在调研的TEE中，它们有两个选项，一个是内存保护单元，另外一个是MMU。两者主要的不同就是，前者直接操作物理地址，而后者会基于虚拟地址。同时，MPU往往只会支持有限的规则，而MMU则更加得灵活。我们发现使用MMU的TEE会更加复杂，需要更加深入的安全性分析。另一方面，MPU则相对简单，因此可以简化安全性分析。许多的现代学术界TEE依赖MPU进行隔离。另一方面，许多商业TEE会更加倾向于使用复杂的MMU。 ​ 我们也发现这些访问控制信息，如可信页表、MPU的访问控制规则，其他的辅助元数据需要自己防御未授权访问。这种情况往往是同启动时配置MPU实现的。 ​ Caches: Cache中会保存有最近访问过的数据来提升软件性能。通常，CPU中包含多个cache层次，有一些是专门属于一个核，也有的共享的。在一些TEE架构中，（SGX中可信实体的Cache数据不被不可信部分访问）？，Arm TrustZone会添加额外的保护机制来使得Enclave的Cache数据不被访问。 ​ 基于空间的Cache隔离，将一部分Cache变成一个Enclave的专属Cache，这会降低资源的利用率，同时降低系统的性能。但是这样的作法可以用于消除侧信道。分时复用Cache也不是十分高效，因为这需要在切换上下文时刷新整个Cache，因此，写TEE会将一小部分专属于一个Core。并且密码学操作并不适合于Cache这类微架构，因为密码学硬件会带来的面积开销和延迟。 可信IO ​ 早期的TEE设计专注于CPU，最近也将兴趣转移到如何和外设进行可信的交互。可信IO分为两个部分：1）建立一条可信路径，Enclave对于设备进行访问的机密性和完整性；2）在可信外设上保护Enclave的数据。 A. 建立可信路径 ​ 这条可信路径用于建立enclave和外设之间的通信。通过逻辑上的和密码学上的操作可以建立一条可信路径。如果这个可信路径有多跳，那么可以在每一跳可以实现一种隔离。注意在每一跳都可以使用逻辑上的或者密码学上的方法，除了基于总线的攻击，只能使用密码学的方法。 ​ 一种构建逻辑上可信路径的方法就是建立访问控制过滤器，这样就可以过滤一些MMIO请求。这个过滤可以是静态的，也可以是可编程的。许多依赖trustZone Protection Controller进行过滤。另外一种选项就是通过MPU配置，来保证Enclave访问时只有其能进行访问。 ​ 体系结构上对于密码学安全路径的支持：对于在两个端点之间实现密码学安全路径，这种方法会造成一些难以避免的开销。为了建立这个安全路径，Enclave和外设之间要预分配证书和密钥。 B.可信设备架构 ​ 对于任何一种可信IO用途，都可以建立一条从enclave到设备的路径。但是新出现的设备可能需要计算处理用户的数据，所以这种设备可能也需要像CPU一样保护用户数据的机密性和完整性。 ​ 今天，一系列的加速器，如专用AI处理加速芯片、FPGA、GPGPU等等。这些系统底层架构有很大的不同，因此需要分别实现特定的机制来实现enclave数据。但是这些机制并不在本篇文章的讨论范围内。可是仍然有一些更为统一的策略。 ​ 空间隔离：为每一个Enclave使用专门的加速器。但是这种在办法需要为Enclave一个整个生命周期中分配一个设备，这样开销是很大的。因此，这种策略并不是很实用。只能说在有专门的设备存在的情况，可以建立一条达到设备的路径。 ​ 时间隔离：直到最近，加速器仍然假设在任何一个时刻，只有单个上下文正在执行。因此可以对设备进行分时共享。这种策略需要一个安全上下文切换机制，这可以通过软件实现，也可以通过加速器硬件实现。 ​ 时空隔离和逻辑强制隔离：这种策略用于多个enclave需要并行地访问加速器。这种策略需要硬件的支持，即硬件支持多用户运行。这里的策略是设备定制的，但是它是包含保持记录资源所有权的访问控制信息。 ​ 基于密码学的隔离：这个不适用于于保护片上加速器资源。 安全存储 ​ 在许多引用场景下，enclave需要在不断启动后，维持一个持久的状态。这个过程需要将数据进行加密。这个过程叫做sealing，反过来进行解密的过程称之为unsealing。 ​ 即使安全存储是一个很常见的需求，但是现在大多数TEE并没有描述地很确切。一些TEE提供了原语来支持安全存储，但没有完整描述一整套方案。因此我们在这里就不再过多描述这些TEE。所以接下来的小节中，我们将专注于那些提供了一整套机密方案的TEE：Flicker、SEA、IBM-PEF、Intel SGX、TIMBERV、Keystone和Sanctuary。 A. Sealing方案及其权衡 ​ 所有调研TEE中的sealing方案都基于TPM。Flicker，SEA和IB,-PEF都直接依赖TPM的原始sealing机制。这包括生成一对非对称密钥，并且使用它们进行加密一个机密信息，这样只有系统被指和加密时一致才能正确进行解密。这里sealing和unsealing过程中使用的系统配置信息使用的是记录在TPM's Platform Configuaration寄存器中的测量值。因为TPMs的存储空间十分有限，一个典型的保护大量数据的方法就是生成一个对称密钥，使用对称密钥对大量的数据进行加密，seal这个密钥到TPM中。 ​ 现在存在很多方法可以unseal之前seal的数据，一些TEE只允许有相同TCB下且拥有相同测量值的Enclave对数据进行unseal。其他方法，有的允许所有被同一个开发者签名过的Enclave对数据进行unseal。还有一些以云为场景的TEE允许enclave迁移seal的数据到不同host上。 B. 对于Sealing架构上的支持 ​ 像OP-TEE、Keystone、TIMBERV和Sanctury使用了软件TCB为每一个Enclave提供了sealing支持。这里，TCB会暴露一个接口用于Enclave创建Sealing密钥。这种方案不需要二外的硬件或者架构支持，可以部署到任何运行时软件TCB中。对于TPM-based的解决方案，需要一个TPM芯片来支持sealing。因为TPM往往是片外的组件，Intel SGX使用特殊的硬件指令来进行sealing，通过绑定的方法进行sealing密钥的使用。 TCB ​ 这一节将概述现存TEE设计的TCB。通用的做法是使用代码行数作为TCB衡量的尺度，但是并不是所有的TCB组件都能这么做。论文中还讨论了TCB可变的情况，因为有一些组件可能是可变的，并且改变会在attestation阶段反应出来。总的来说可变的部分往往实现在软件，而硬件的部分不可变。这不是绝对的，比如ROM中的代码，CPU中的微代码等等。 ​ 如上表中所示，所有TEE的RTM都是不可变的。在安全引导的过程中，它会测量下一阶段的TCB组件，并且由这个TCB组件测量enclave。而运行时隔离组件的TCB使用一般是一个可变的软件TCB，用于管理上下文的切换和内存隔离。Secure IO的TCB，大多数的TEE并不支持可信IO，诸如cure、Trustlite都依赖了可变的软件TCB。安全存储的TCB，上表中10个TEE显示地支持了这个特性，如SEA和IBM PEF使用了TPM来完成这些工作，而Timber-V、Keystone和Trustlie使用了软件来完成。即使SGX完成了这个功能，但是还不清楚它的这部分是否是可变的。 ​ 通常来说TEE会试图减小TCB，来减小存在安全漏洞的风险，如果想从理论上保证安全就得依靠形式化证明。然而，在实践过程中，更好的做法是可以实现对TCB的更新，而不是期待TCB是bug-free的。 总结 ​ 这边文章分析了工业界和学术界的TEE底层设计，来完成：1）可验证启动，2）运行时隔离，3）安全IO，4）安全存储。即使不同的机器上看起很不同，但大都采用相似的设计选择。 ","link":"https://porterlu.github.io/post/sokhardware-supported-trusted-execution-environment/"},{"title":"IO虚拟化","content":" 为什么需要IO虚拟化 ​ 如果让VM直接管理物理网卡： 正确性问题：所有的VM都使用相同的MAC地址、IP地址无法正常接受发送网络包 安全性问题：恶意VM可以直接读取其他VM的数据 ​ 而I/O虚拟化的目标就是，为虚拟机提供正常使用虚拟外部设备，隔离不同虚拟机对于外部设备的访问，提高物理设备的资源利用率。 如何实现IO虚拟化 设备模拟 ​ 当虚拟机中的程序试图访问网卡时，就会陷入虚拟机，KVM会使用QEMU，QEMU会发起一次系统调用，请求物理机上的网卡驱动。之后再原路返回。 ​ 这样的话就可以模拟任何设备，还允许在QEMU的层面检查网络包的内容，还不用修改硬件。但是性能非常差。 半虚拟化方式 ​ 让虚拟机知道运行在虚拟化环境下，虚拟机运行前端驱动，VMM运行后端驱动，VMM主动提供Hypercall给VM，通过共享内存传递指令和命令。 ​ VMM和VM之间存在一个传递IO请求的队列： ​ 通过一个IO队列直接将请求发送给VMM，可以将多个IO合并为一个，同时实现简单。但是需要修改虚拟机和操作系统内核。 设备直通 ​ 如果让虚拟机直接管理物理设备，会存在诸如DMA恶意读写内存的问题。 ​ SMMU是ARM中IOMMU的实现，这里也存在两阶段地址翻译，IOVA-&gt;GPA，第二个阶段进行GPA-&gt;HPA。 ​ 除了隔离问题，还要解决设备独占的问题，满足SR-IOV标准的设备，可以使用如下的复用。 ","link":"https://porterlu.github.io/post/io-xu-ni-hua/"},{"title":"内存虚拟化","content":" 三种地址 客户虚拟地址Guest Virtual Address 客户物理地址Guest Physical Address 主机物理地址Host Physical Address 将GPA和HPA进行管理，是VMM需要负责的工作。 内存虚拟化的方式 影子页表 set_cr3 (guest_page_table): for GVA in 0 to pow(2, 20) if guest_page_table[GVA] &amp; PTE_P: GPA = guest_page_table[GVA] &gt;&gt; 12 HPA = host_page_table[GPA] &gt;&gt; 12 shadow_page_table[GVA] = (HPA &lt;&lt; 12) | PTE_P else shadow_page_table[GVA] = 0 CR3 = PHYSICAL_ADDR(shadow_page_table) ​ 这里遍历了页表，将影子页表设置为了将GVA翻译为HPA的结构，这个过程需要遍历所有GVA项，最后将影子页表基地址设置到CR3中。 ​ 这样VMM就可以拥有一个独立的影子页表，相应的，一旦guest OS修改页表，影子页表也要做相应的更新。 ​ 为了达到这样的目的，这里使用一些手法，将这些页中的所有页都设置为只读，如果guest OS修改了这些页，那么硬件就会触发缺页异常。这样VMM就可以处理缺页异常，更新影子页表。如何将Guest OS和Guest app进行隔离？在影子页表的实现中，需要同时维护两个页表，当guest os切换到U，VMM也要做切换影子页表的操作。 ​ 如果切换到U，我们需要调用set_ptp(current, 0), 这样只有标注为User，会被添加到影子页表中。 直接映射 ​ 这是一种半虚拟化的方式，我们需要直接修改guest OS代码，只用GVA和HPA，Guest OS接着操纵它的HPA空间，然后使用hypercall将自己要修改的信息告诉VMM， VMM来更新页表，CR3指向这个GUEST OS的页表。 硬件虚拟化对于内存翻译的支持 ​ 创建一个新的页表将GPA翻译为HPA，这个表由VMM直接控制，每个VM一个表，Intel称之为EPT，ARM上叫Stage-2 Page Table。这样在翻译完成GVA-&gt;GPA后会自动进一步的翻译。 由于本身一次非虚拟化页表的页表访问需要4次内存访问，但是在虚拟化场景下，这里的地址都变为了GPA了，需要进行进一步的翻译。不同的是，每一级的都要将GPA翻译为HPA，总共需要24次访存。 ​ 这里TLB可以缓存GVA-&gt;HPA，大大加速了翻译过程。同时一阶段页表缺失不会引起VM Exit. ","link":"https://porterlu.github.io/post/nei-cun-xu-ni-hua/"},{"title":"硬件辅助虚拟化","content":" VT-x VMCS ​ 使用VMM(Virtual Machine Control Structure)控制虚拟机的状态，它包含了6个部分的内容： Guest-state area: 当发生一次VM exit， CPU现在的状态被自动保存到这个区域中; VM Entry时，又将这个区域的内容加载到CPU中。 Host-state area: 当发生VM exit时，可这个区域信息加载到CPU中; VM entry, 相反。 VM-execution control field: 可以控制non-root模式中虚拟机的行为。 VM-exit control field: 控制VM exit的行为。 VM-entry control field: 控制VM entry的行为。 VM-exit information field： VM exit的原因，方便VMM进行处理。 VM Entry and VM Exit ​ VM Entry会从root进入Non-root模式，第一次使用VMLAUNCH指令，之后会使用VMRESUME指令。 ​ VM Exit会回到root模式，用于敏感指令检测和中断陷入。 ARM 虚拟化技术 VM Entry and VM Exit ​ 对于eret 从VMM进入VM，这和操作系统进入用户态的操作是一样的。VMM虚拟化需要自己准备VM的上下文并加载，最后通过eret进入到VM中。 ​ 同样地，在vm exit时，即执行敏感指令或者受到中断时，需要VMM保存所有的状态，然后再执行VMM相关的处理程序。 ARMv8.0 Type-2 VMM架构 ​ 在EL2只能运行VMM, host os需要大量修改才能运行。这里的设计在EL2做了接受转发的lowvisor, 依靠原本Host OS的VMM模块进行处理。 ARMv8.1 Type-2 VMM架构 QEMU/KVM ​ 由ARMv8.1提出的特性，我们可以设计出如上的架构，QEMU运行在用户态，负责实际的处理。同时KVM作为Linux的一个模块运行在EL2，负责对于敏感事件的捕捉，同时直接使用Linux提供的一些功能。 ​ qemu使用ioctl向KVM传递命名，如CREATE_VM, CREATE_VCPU, KVM_RUN等。 open(&quot;/dev/kvm&quot;); ioctl(KVM_CREATE_VM); ioctl(KVM_CREATE_VCPU); while(true) { ioctl(KVM_RUN); exit_reason = get_exit_reason(); switch (exit_reason) { case KVM_EXIT_IO: //.... break; case KVM_EXIT_MMIO: //... break; } } 在x86架构中，需要从VCPU中找到VMCS结构，使用CPU指令加载VMCS，使用VMLAUNCH或者VMRESUME进入到Non-root, 硬件会自动加载VMCS状态，跳转到GUEST_RIP执行。 对于ARM，KVM会主动加载VCPU的所有状态，之后使用eret跳到guest程序执行。 ​ 上图中解释这个过程，KVM运行在高特权级充当媒介进行切换。下面是一个更加具体的例子 ​ 当一个VM中的程序执行了WFI时，就会被捕获VM Exit 到KVM，KVM处理程序主动将VCPU信息进行保留，根据handler执行相应的函数，它可以使用Linux中现有的一些机制，等待Kernel结果返回后，KVM还可以调度下一个VM程序。 ​ 对于一个IO类型的VM Exit的陷入还射击QEMU，QEMU运行在用户态，但是由于v8.1中提到的特性这是允许的，QEMU处理完IO可以通过系统调用返回。 ","link":"https://porterlu.github.io/post/ying-jian-fu-zhu-xu-ni-hua/"},{"title":"虚拟机类型","content":" 虚拟机的类型 Type-1 虚拟机监视器 ​ 直接运行在硬件上，充当了操作系统的角色，像操作系统一样实现调度、内存管理、驱动硬件等，典型的有Xen。 Type-2 虚拟机监视器 ​ 依赖主机的操作系统，主机操作系统依旧管理物理资源，将虚拟机当作应用在运行，复用了主机上操作系统的大部分功能。 系统ISA ​ 系统ISA指的是读写敏感寄存器、控制处理器行为（如WFI）、控制虚拟或物理内存（打开、配置、安装页表）、控制外设（DMA和中断）。 ​ 为此必须提供捕捉系统ISA, 内存虚拟化、设备虚拟化。 最直接的实现方法 ​ 将虚拟机放在操作系统原本的特权级，这样客户操作系统和进程都运行在用户态，操作系统。当用户态的程序执行特权指令，会陷入到VMM进行模拟。但是不是所有的敏感指令都是特权指令，如何处理这些不会下陷的敏感指令： 解释执行 二进制翻译 半虚拟化 硬件虚拟化 解释执行 ​ 使用软件的方法对于任何一条指令进行模拟，没有任何指令直接在硬件上运行，同时使用内存维护虚拟机的状态。 ​ 解决了敏感指令不下陷的问题，实现简单，但是效率非常低。 二进制翻译 通过翻译虚拟机指令，之后以基本块为单位进行执行。但是它不能处理自修改的代码，且中断插入的粒度较大，因为每次执行都是将翻译完的代码放到硬件上直接执行，所以VMM只能在边界进行检查。 半虚拟化 ​ 提供hypercall，修改操作系统的源码，将所有的敏感指令替换为vmm的调用。 ​ 提升了敏感指令不下陷的问题，提升I/O等场景的性能。缺点是需要修改操作系统。 硬件辅助虚拟化 ​ x86 引入了全新的特权级 root模式和non-root模式，将VMM运行在root模式，虚拟机运行在非root模式。 ​ 而在ARM中VMM运行在EL2，同时操作系统和应用程序分别运行在EL1和EL0。 ","link":"https://porterlu.github.io/post/xu-ni-ji-lei-xing/"},{"title":"CLEPSYDRACACHE – Preventing Cache Attacks with Time-Based Evictions ","content":"背景 ​ 近年来，对于CPU微架构的攻击层出不穷。特切实针对Cache的攻击扮演了很重要的角色，这些攻击都利用了Cache地址冲突。这篇文章中提出了Clepsydra Cache ，通过Cache Decay和随机化方法来消除Cache Side Channel。文章提出了一种动态调度机制Time To Life，来保持消除Side Channel的同时保证性能。 ​ Cache是主存和CPU之间的结构，用于缓解由于访问内存的高延迟。现代处理器往往每个核心都有自己的单独的L1和L2 Cache，但是LLC是在所有核之间共享的。因为Cache不能容纳所有的数据，所以必须要合适替换Cache块来给新的数据。 ​ 从架构的角度来讲，Cache对于软件来说是透明的。但是由于Cache是基于局部性原理的，基于Cache Timing Attack可以看见Cache。通过测量一次内存访问的时间，可以判断访问的数据是否被Cache了。因为共享Cache的存在，所以攻击者可以很好地利用起发起进程级，甚至虚拟机级的攻击。 ​ 因为Cache是为了加速主存访问出现的，所以Timing Side Channel的消除往往不能避免性能的损失。Detection-based的机制不能很好地解决问题，因为不同软件的访问模式有很大的不同。因此最近的设计都改变了新数据被Cache的方式。如Cache partitioning将Cache分为不同的安全域从而达到隔离的效果。另一方面，index随加化则是通过随机化地址到Cache entry的映射，可以加大找到驱逐集的难度，从而加大PRIME+PROBE的难度。但是最近的研究显示，只是Index随机化不能充分地保证安全，需要不断地进行重新映射。攻击者仍旧可以构建一个驱逐集来驱逐一个目标地址。但是不幸的是，频繁地进行重新映射是不切实际地，因为它会导致非常大的性能开销，甚至整个Cache的Flush。 ​ 这篇文章提出了Clepsydra Cache，提出了一种安全的Index-Based随机化方案，可以防御最先进的Cache Based Attack并且保证Cache的性能。通过Cache Decay加上Index-Based随机化可以防御PRIME + PRUNE + PROBE的攻击。Index-Based随机化指的是随机化地址到Cache Entry的映射，而Cache Decay是通过硬件集成的Time-to-Live来进行Cache Evict来使得对于Cache Conflict的观察变得困难。 基础知识及相关工作 ​ Cache是否命中导致内存访问延迟可以利用于侧信道攻击，一个攻击者可以观察其他进程的Cache访问时间，来推断它访问的数据。在PRIME+PROBE攻击中，一个在Cache的区域中填充上自己的数据，接下来被攻击的进程会在访问内存的过程中，驱逐一些Cache块。最后一步，攻击者通过重新测量自己填入数据的访问时间来得知被攻击进程的访问地址。一开始PRIME+PROBE的攻击都用于和CPU比较近的Cache，因为Evict LLC大部分表项的代价非常大。但是最近PRIME+PROBE攻击使用最小驱逐集可以实现对于LLC的攻击（驱逐集指的就是映射到同一个Cache Set的地址的集合）。而最小驱逐集合指的是集合中地址的数量等于Cache way的数量的驱逐集。如果攻击者可以创建这样一个集合，那么就可以快速清空被攻击程序在Cache中的数据包括LLC。但是通过 地址很难控制物理地址的映射，为了克服这些，已经有论文设计高效获取最小驱逐集的算法。同时FLUSH+RELOAD攻击不需要获取驱逐集，并且不需要和被攻击者共享内存。 ​ 最近的PRIME+PRUNE+PROBE还可以用于攻击使用使用Index随机化的架构。通过构建一个普遍的驱逐集，可以在一个合理的时间内驱逐一个Cache Entry。但是文章中说明这种攻击对于本篇工作是无效的。 威胁模型 我们假设攻击在一个理想的，无噪声的环境中，因此攻击者可以完美地区分Cache命中和Cache缺失。 攻击者可以访问任何数量的地址。 可以测量执行程序的执行时间。 假设随机化使用的函数是伪随机。 不考虑一系列的物理攻击。 Clepsydra Cache ​ 在传统的Cache中，Cache Entry要么可以被一条指令进行替换，要么因为Cache conflict被替换。现在的攻击都基于上面两点中的任何一点。前一点，可以通过不提供这样的指令解决（实际上很多ISA不提供单独刷新某一个Cache Entry的指令）；后面一点，则是现在Cache设计中的基本问题，由它引起的漏洞很难消除。 ​ Clepsydra Cache的设计目标就是大幅度减少总的Cache Conflict，这样Cache访问和Cache替换之间的直接关系就可以大大减弱。在文章的设计中，达到了这一目标，于是可以很好地防御PRIME+PROBE和PRIME+PRUNE+PROBE的攻击。在Clepsydra Cache，除了因为Cache Conflict进行替换，每一个Cache Entry添加了一个TTL(Time to life)项。这个TTL被随机初始化，然后以RTTLR_{TTL}RTTL​的速率减少，如果TTL变为0就意味着这个Entry需要被驱逐。第二点，Clepsydra Cache使用一个高度随机的地址映射机制，这样不通过观察Conflict就得知驱逐集就变得不可能了。 ​ 为了实现高性能的Cache，Clepsydra Cache不通过主动检查TTL是否为0，将Cache Entry置为无效，而是通过在Cache访问时检查TTL位而不是Valid实现。同时高性能的Cache也要求这个随机映射函数的延迟必须很低。 TTL ​ 在每个Entry中，都有一个TTL表项意味着这个Entry还可以在Cache中滞留多久。一旦一个Cache Miss发生，需要将一个Cache块载入到Cache中，随机映射函数会决定映射的Entry，同时一个随机的TTL会被写入Entry。TTL会随时间不断减少，一个Cache Access如果TTL &gt; 0， 意味访问成功，之后设置一个新的TTL；反之失败。 地址映射和替换 ​ 传统的Cache中地址中的Index部分决定了数据被缓存到的Cache Line。因此在已知映射策略的情况下，获取最小驱逐集就变成一件平常的事。文章中将地址映射到一个Dynamic Set，即使冲突是不可避免的，文章仍然设计了一个伪随机函数，这个伪随机函数是和真正的随机函数不可区分的。 fw:(F2t,F2i)→(F2t′,F2i)f_w: (F_2^t, F_2^i) \\rightarrow (F_2^{t^{&#x27;}},F_2^{i}) fw​:(F2t​,F2i​)→(F2t′​,F2i​) ​ 这里函数会将一个t Bits的Tag和一个i Bits的Index映射到t′t^{&#x27;}t′ Bits的Tag和i Bits的Cache Index。函数的性质如下： 可逆性：即两个不同的地址不能最后函数的输出相同。 Index伪随机：不同的地址映射同一个Cache Line的概率必须接近1n\\frac{1}{n}n1​ 独立性：函数在不同Cache Way的映射情况必须是独立的。 这里使用的是一个轻量级的Block Chiper，具体来说是PRINCE，可以在两个Round做到全混淆。 冲突解决 ​ Clepsydra Cache中可以动态地控制Entry 表项全局的速率，RTTLR_{TTL}RTTL​用表示TTL下降的速率。每个周期RTTLR_{TTL}RTTL​ 会自动减1直到一个最小值，一旦一个Cache Conflict发生时，RTTLR_{TTL}RTTL​ 的翻倍。通过这样的策略在减少Cache Conflict和维持性能之间达到平衡。 安全性分析 对TTL进行统计观察 ​ 文章中对于攻击者是否能区分Cache Conflict和Time-based Cache eviction进行了分析。在传统的Cache攻击中，一次冲突检测分为两个阶段。第一个阶段，攻击者会准备一些Candidate Addresses，之后访问目标地址，就会将一个Candidate Address驱逐出Cache。下一个阶段，通过测量访问Candidate Address得知那些Address被替换出Cache。但是在Clepsydra Cache中，就会引入一定程度的不确定性，攻击者无法确定一次访问的表项驱逐到底是Time-based还是Conflict-based。但是，在Candidate Address的集合很小的情况下，在驱逐和观察的时间间隔会很短，于是Time-based的驱逐可能性就变得很低。文章中指出，一次有效的攻击不可能执行用很小的Candidate Address集合，这个将在下一节中进行说明。 ​ 一个强大的攻击者还可能估计RTTLR_{TTL}RTTL​的值，从而获取Cache Conflict的信息。为了做到这一点，攻击者需要有一个地址集并且间歇地访问它们，判断是否有Cache Miss发生。 Prime+Prune+Probe 攻击 ​ Prime+Prune+Probe攻击需要构建一个驱逐集G=∪i=1wGiG = \\cup_{i=1}^{w}G_iG=∪i=1w​Gi​, GiG_iGi​ 表示和目标地址在Cache中冲突且在WayiWay_iWayi​的一个地址。在Clepsydra Cache构建这样的集合是困难的，主要是如下三个原因： 初始的Prime过程，需要占据所有的地址的Cache表项。 Time-based的驱逐策略，要求两次访问的间隔的时间不能太长。 最终的驱逐集要占据一个Dynamic Set的所有表项。 Catching的概率 ​ 在开始，攻击者需要不断地访问一个地址集合kkk, 最终k′k^{&#x27;}k′是kkk的一个子集，表示去除了一些Cache表项重复的地址，获取较小的一个地址集合。之后文章中使用pcp_cpc​去表示在使用∣k′∣|k^{&#x27;}|∣k′∣个地址作为填充表项时，地址x一定可以驱逐k′k^{&#x27;}k′中的一个表项的概率。由上面的分析可以知道，如果一个Dynamic Set如果没有被完全填满，那么Catching的概率就是0，因为这样不会导致任何的Cache Conflict。而又Clepsydra的设计原则可以知道，它总是尽量保证每一个Dynamic Set都有空闲的Entry。随意为了进行Catching，我们对它的概率进行计算。假设在一个没有噪声的环境中，地址x映射到某一个Dynamic Set，k′k_{&#x27;}k′​可以完全包含这个Dynamic Set的概率满足超几何分布，这里作者对于公式进行了化简： Pc(k′)=C∣k′∣wCNwP_c(k^{&#x27;}) = \\frac{C_{|k^{&#x27;}|}^w}{C_N^w} Pc​(k′)=CNw​C∣k′∣w​​ ​ 文章中将几个典型的数据代入，为了达到50%的概率，要填充Cache 95.8%的表项，这相较于另外一片工作的Scatter Cache无疑是安全性的巨大提升。 Time-Based驱逐 ​ 地址集合k′k^{&#x27;}k′是从原始集合k缩小得来的，经过上一节的讨论可以知道，我们的目标是几乎填充满Cache Entry。同时攻击者需要尽量缩小Time-Based驱逐带来的影响，这样才能检测到Conflict。这需要将RTLLR_{TLL}RTLL​保持在一个较小的值，才能保证攻击对于Conflict的观测。但是根据传统的攻击，在探测的步骤中，需要对整个地址集合进行遍历，这样RTLLR_{TLL}RTLL​将会指数级上升。因此更好的办法是，渐进地使用Prime + Probe，在迭代的过程中，需要不断地访问已有的k′k^{&#x27;}k′来刷新RTLLR_{TLL}RTLL​，使得RTLLR_{TLL}RTLL​保持在一个较低的值。 ​ 现在假设攻击将Cache prime到了一个足够的程度，于是访问x可以极大概率发生Conflict。但是攻击者现在需要逐个进行Probe来检测，到底是哪一个地址被x驱逐出了Cache。文章中指出了，到了Probe到一个Cache Miss发生时，已经无法保证这到底时Time-Based驱逐还是Conflict-Based驱逐，即攻击者还是没能攻破TTL机制，这可能需要设计好TTL的值，一个太大的TTL无法做大这个保证，作者还给出要完成Prime到这个程度所需要的访问次数公式，可以作为设计TTL的参考。于是攻击只能将所有潜在的地址加入驱逐集G中，毫无疑问，这大大增加了G的大小。 驱逐的概率 ​ 为了实施一次Cache Timing Attack需要找到和x冲突许多Cache Entry。文章中接下来计算了到底现需要多少地址来以pep_epe​的概率检测到地址x的访问。 ​ ​ 在G中每一个地址都1w\\frac{1}{w}w1​的概率覆盖地址x的Dynamic Set中的一个地址，于是有如下的公式： pe=(1−(1−1w)∣G∣w)wp_e = (1 - (1 - \\frac{1}{w})^{\\frac{|G|}{w}})^w pe​=(1−(1−w1​)w∣G∣​)w ​ 其中(1−(1−1w)∣G∣w)(1 - (1 - \\frac{1}{w})^{\\frac{|G|}{w}})(1−(1−w1​)w∣G∣​) 的意思是G中有地址命中x的Dynamic Set中的WayiWay_iWayi​的概率，所以整个Dynamic Set被占满，由独立性可以得到，可以直接取w的幂次。文章同样在这里带入了典型数据进行说明 ​ 在一个16路的Cache中如果要以95%的概率占满一个Dynamic Set，这个G的大小要达到1425。 Profile性能估计 ​ 文章中还保守估计了攻击者在Clepsydra Cache构造一个驱逐集G所要花费的时间。这里做了假设一次Cache Hit的时间为10 ns，Cache Miss的延迟为20ns。 ​ 攻击者使用上文描述的方法进行攻击，每一次新地址的访问一定会操作一次Cache Miss，由于是递进式地扩大k，同时每次访问时添加地新地址都有pcp_cpc​和k中已有的地址发生碰撞。为了防止RTTLR_{TTL}RTTL​的快速增加同时保持k原本包含的地址，需要再次访问原本k集合中的地址，这个过程又有pcp_cpc​的概率触发碰撞。 tpp=∑i=1k(tmiss+∑j=1∞pc(i)j×((i−1)×thit+tmiss))t_{pp} = \\sum_{i=1}^k(t_{miss} + \\sum_{j=1}^{\\infin}p_c(i)^j \\times ((i-1)\\times t_{hit} + t_{miss})) tpp​=i=1∑k​(tmiss​+j=1∑∞​pc​(i)j×((i−1)×thit​+tmiss​)) ​ 首先第一个全加符号表示这个集合k的大小是从1到|k|的。每一次集合扩大新的地址一定带来一次Miss，同时在重现访问k中地址的过程中触发配装，如果触发碰撞要继续访问直到地址都在Cache中稳定。 tk=∣G∣×1pc(k)×tppt_k = |G| \\times \\frac{1}{p_c(k)} \\times t_{pp} tk​=∣G∣×pc​(k)1​×tpp​ ​ 每一次tppt_{pp}tpp​只能构建G中的一个地址，根据G所需要的大小，可以计算总的时间。使用这个估计，如果要以50%的概率占满一个攻击地址的Dynamic Set，同时假设Cache的大小为8 MB且是16路的。在k只是用Cache的70%是，需要使用1个小时，k只是用50%的Cache表项，这个时间将扩大到19个小时。 实现 ​ Clepsydra Cache在gem5上实现了对于微架构的模拟，同时分析了实际硬件的实现开销。对于随机函数的实现采用了轻量级的Block Cipher PRINCE，使用三轮的混淆和一个64位的密钥。至于TTL的实现，文章中为每一个表项实现了一个计数器，同时设定为50ms。而硬件实现上，作者模拟了65nm工艺下的情况，面积开销为0.1平方毫米，同时功耗增加了12毫瓦。 测试 ​ Parsec benchmark使用了全模拟的模式，之后的benchmark由于时间开销太大不会使用全模拟。可以看到在Parsec benchmark中使用Clepsydra Cache会有 -0.37% 到 5.25%的开销。 ​ 而在SPEC CPU 2017的测试，文中绘制了一个表格如下： 总结 ​ 文中利用TTL机制达到了估计者对于Conflict和TTL Evict的不可区分，防御了Prime + Prune + Probe攻击。同时证明了硬件实现上的可行性，还有运行可以小开销甚至无开销。 ","link":"https://porterlu.github.io/post/clepsydracache-preventing-cache-attacks-with-time-based-evictions/"},{"title":"rCore第八章","content":"用户线程 数据结构 use std::arch::asm; const DEFAULT_STACK_SIZE: usize = 1024 * 1024 * 2; const MAX_THREADS: usize = 4; static mut RUNTIME: usize = 0; 声明一些全局变量。 pub struct Runtime { threads: Vec&lt;Thread&gt;, current: usize, } #[derive(PartialEq, Eq, Debug)] enum State { Available, Running, Ready, } struct Thread { id: usize, stack: Vec&lt;u8&gt;, ctx: ThreadContext, state: State, } #[derive(Debug, Default)] #[repr(C)] struct ThreadContext { rsp: u64, r15: u64, r14: u64, r13: u64, r12: u64, rbx: u64, rbp: u64, } ​ 声明用户态线程所需要的数据结构，由于用户态线程对于内核是不可知的，我们需要一个Runtime进行调度，这里Runtime中维护了一个Thread队列。由于用户态线程除了单独的栈，调用和普通函数一样，我们在保存寄存器状态时，只需要保存callee保存寄存器。 Runtime 在运行用户态线程之前，我们需要对运行时进行初始化。 impl Thread { fn new(id: usize) -&gt; Self { Thread { id, stack: vec![0_u8; DEFAULT_STACK_SIZE], ctx: ThreadContext::default(), state: State:;Available, } } } impl Runtime { pub fn new() -&gt; Self { let base_therad = Thread { id: 0, stack: vec![0_u8; DEFAULT_STACK_SIZE], ctx: ThreadContext::default(), state: State::Running, } let mut threads = vec![base_thread]; let mut available_threads: Vec&lt;Thread&gt; = (1..MAX_THREADS) .map(|i| Thread::new(i)) .collect(); threads.append(&amp;mut available_threads); Runtime { threads, current: 0, } } pub fn init(&amp;self) { unsafe { let r_ptr: *const Runtime = self; RUNTIME = r_ptr as usize; } } } 这里初始化了主线程，这个线程用于调度其他的线程，是一开始默认调度的线程。同时将所有线程模板加入到队列中。运行时就是这个队列和当前线程号组成的。 impl Runtime { #[inline(never)] fn t_yield(&amp;mut self) -&gt; bool { let mut pos = self.current; while self.threads[pos].state != State::Ready { pos += 1; if pos == self.threads.len() { pos = 0; } if pos == self.current { return false; } } if self.threads[self.current].state != State::Available { self.threads[self.current].state = State::Ready; } self.threads[pos].state = State::Running; let old_pos = self.current; self.current = pos; unsafe { let old: *mut ThreadContext = &amp;mut self.threads[old_pos].ctx; let new: *const ThreadContext = &amp;self.threads[pos].ctx; asm!(&quot;call switch&quot;, in(&quot;rdi&quot;) old, in(&quot;rsi&quot;) new, clobber_abi(&quot;C&quot;)); } self.threads.len() &gt; 0 } } 首先介绍t_yield，这个方法会遍历thread队列，找到一个状态是Ready的线程，我们会将自己的current的状态设置为Ready。同时将我们选择的线程状态设置为running, 之后交换新旧线程的上下文。 pub fn spawn(&amp;mut self, f: fn()) { let available = self .threads .iter_mut() .find(|t| t.state == State:;Available) .expect(&quot;no available thread.&quot;) let size = available.stack.len(); unsafe { let s_ptr = available.stack.as_mut_ptr().offset(size as isize); let s_ptr = (s_ptr as usize as !15) as *mut u8; std::ptr::write(s_ptr.offset(-16) as *mut u64, guard as u64); std::ptr::write(s_ptr.offset(-24) as *mut u64, skip as u64); std::ptr::write(s_ptr.offset(-32) as *mut u64, f as u64); available.ctx.rsp = s_ptr.offset(-32) as u64; } available.state = State::Ready; } #[naked] unsafe extern &quot;C&quot; fn skip() { asm!(&quot;ret&quot;, options(noreturn)) } fn guard() { unsafe { let rt_ptr = RUNTIME as *mut Runtime; (*rt_ptr).t_return(); }; } 这里spawn会遍历队列，找到一个没有被占位的Thread结构体，往线程对应的栈中压入f、skip、guard函数，之后将上下文中的sp指针指向f的位置，那么这个Thread就被创建完成加入到了队列中。 当所有的线程创建完毕时，在主线程中会调用run方法， impl runtime { pub fn run(&amp;mut self) -&gt; ! { while self.t_yield() {} std::process::exit(0); } fn t_return(&amp;mut self) { if self.current != 0 { self.threads[self.current].state = State::Available; self.t_yield(); } } } run中将会不断地调用我们如下分析过的t_yield方法。 #[naked] #[no_mangle] unsafe extern &quot;C&quot; fn switch() { asm!( &quot;mov [rdi + 0x00], rsp&quot;, &quot;mov [rdi + 0x08], r15&quot;, &quot;mov [rdi + 0x10], r14&quot;, &quot;mov [rdi + 0x18], r13&quot;, &quot;mov [rdi + 0x20], r12&quot;, &quot;mov [rdi + 0x28], rbx&quot;, &quot;mov [rdi + 0x30], rbp&quot;, &quot;mov rsp, [rsi + 0x00]&quot;, &quot;mov r15, [rsi + 0x08]&quot;, &quot;mov r14, [rsi + 0x10]&quot;, &quot;mov r13, [rsi + 0x18]&quot;, &quot;mov r12, [rsi + 0x20]&quot;, &quot;mov rbx, [rsi + 0x28]&quot;, &quot;mov rbp, [rsi + 0x30]&quot;, &quot;ret&quot;, options(noreturn) ); } 如上是switch函数，我们需要交换所有的线程中的callee保存寄存器。 内核线程 ​ 这里存在三种资源分配器，一个是pid，kstack_id和userRes_id, 由于userRes在每个pid分配的资源中进行再分配，所以userRes从0号开始分配。同时kstack_id则是所有进程的线程是同等，需要一个跨越不同进程的分配器，让不同的进程中线程分配的内核栈空间不重合。 ​ 对于一个ProcessControlBlock 其中根据上面的描述，它有一个独立的tid_allocator来管理线程号，还有一个地址空间映射，进程中的所有的线程共享这个地址空间。同时tasks队列中有一些线程控制块TaskControlBlock， 线程控制块中保存了一些线程独有的信息，如tid, task_cx, trap_cx_ppn。 ​ 当我们创建一个进程时，代码如下： pub fn new(elf_data: &amp;[u8]) -&gt; Arc&lt;Self&gt; { let (memory_set, ustack_base, entry_point) = MemorySet::from_elf(elf_data); let pid_handle = pid_alloc(); let process = Arc::new(Self { pid: pid_handle, inner: unsafe { UPSafeCell::new(ProcessControlBlockInner { is_zombie: false, memory_set: None, parent: None, children: Vec::new(), exit_code: 0, fd_table: vec![ Some(Arc::new(Stdin)), Some(Arc::new(Stdout)), Some(Arc::new(Stdout)), ], signals: SignalFlags::empty(), tasks: Vec::new(), task_res_allocator: RecycleAllocator::new(), mutex_list: Vec::new(), semaphore_list: Vec::new(), condvar_list: Vec::new(), }) }, }); let task = Arc::new(TaskControlBlock::new( Arc::clone(&amp;process), ustack_base, true, )); let task_inner = task.inner_exclusive_access(); let trap_cx = task_inner.get_trap_cx(); let ustack_top = task_inner.res.as_ref().unwrap().ustack_top(); let kstack_top = task.kstack.get_top(); drop(task_inner); *trap_cx = TrapContext::app_init_context( entry_point, ustack_top, KERNEL_SPACE.exclusive_access().token(), kstack_top, trap_handler as usize, ); let mut process_inner = process.inner_exclusive_access(); process_inner.tasks.push(Some(Arc::clone(&amp;task))); drop(process_inner); insert_into_pid2process(process.getpid(), Arc::clone(&amp;process)); add_task(task); process } ​ 根据ELF文件的内容获取到地址空间memory_set, 接着申请一个pid，根据这些结构申请一个process control Block结构，这个结构中有memory_set, 父子关系，exit_code, 文件描述符表，信号队列，线程队列，线程号分配器，互斥锁，信号量，条件变量。 ​ 其中部分资源需要进行初始化，比如线程队列，我们需要压入一个主线程，它的线程号为0，我们需要设置线程的入口地址，栈空间，satp, 陷入处理函数。最后将这个线程压入队列，同时将这个线程加入整个调度队列中，最后返回这个process pub fn exec(self: &amp;Arc&lt;Self&gt;, elf_data: &amp;[u8], args: Vec&lt;String&gt;) { assert_eq!(self.inner_exclusive_access().thread_count(), 1); let (memory_set, ustack_base, entry_point) = MemorySet::from_elf(elf_data); let new_token = memory_set.token(); self.inner_exclusive_access().memory_set = memory_set; let task = self.inner_exclusive_access().get_task(0); let mut task_inner = task.inner_exclusive_access(); task_inner.res.as_mut().unwrap().ustack_base = ustack_base; task_inner.res.as_mut().unwrap().alloc_user_res(); task_inner.trap_cx_ppn = task_inner.res.as_mut().unwrap().trap_cx_ppn(); let mut user_sp = task_inner.res.as_mut().unwrap().ustack_top(); user_sp -= (arg.len() + 1) * core::mem::size_of::&lt;usize&gt;(); let mut argv: Vec&lt;_&gt; = (0..=args.len()) .map(|arg| { translated_refmut( new_token, (argv_base + arg * core::mem::size_of::&lt;usize&gt;()) as *mut usize, ) }) .collect(); *argv[args.len()] = 0; for i in 0..args.len() { user_sp -= args[i].len() + 1; *argv[i] = user_sp; let mut p = user_sp; for c in args[i].as_bytes() { *translated_refmut(new_token, p as *mut u8) = *c; p += 1; } *translated_refmut(new_token, p as *mut u8) = 0; } user_sp -= user_sp % core::mem::size_of::&lt;usize&gt;(); let mut trap_cx = TrapContext::app_init_context( entry_point, user_sp, KERNEL_SPACE.exclusive_access().token(), task.kstack.get_top(), trap_handler as usize, ); trap_cx.x[10] = args.len(); trap_cx.x[11] = argv_base; *task_inner.get_trap_cx() = trap_cx; } ​ 对于执行一个ELF文件，我们通过要解析ELF文件后获取memory_set， 替换原来的memory_set, 之后替换原来主线程的用户栈（栈底的位置），分配tid号同时分配对应的资源，设置新的trap_cx_ppn。压入exec的传给用户进程的参数。最后我们要对刚才的trap_cx_ppn处的内容进行初始化，设置完返回值后，返回。 //对于fork我们只支持只有一个线程的进程 pub fn fork(self: &amp;Arc&lt;Self&gt;) -&gt; Arc&lt;Self&gt; { let mut parent = self.inner_exclusive_access(); assert_eq!(parent.thread_count(), 1); let memory_set = MemorySet::from_existed_user(&amp;parent.memory_set); let pid = pid_alloc(); let mut new_fd_table: Vec&lt;Option&lt;Arc&lt;dyn File + Send + Sync&gt;&gt;&gt; = Vec::new(); for fd in parent.fd_table.iter() { if let Some(file) = fd { new_fd_table.push(Some(file.clone())); } else { new_fd_table.push(None); } } let child = Arc::new(Self { pid, inner: unsafe { UPSafeCell::new(ProcessControlBlockInner{ is_zombie: false, memory_set, parent: Some(Arc::downgrade(self)), children: Vec::new(), exit_code: 0, fd_table: new_fd_table, signals: SignalFlags::empty(), tasks: Vec::new(), tasks_res_allocator: RecycleAllocator::new(), mutex_list: Vec::new(), semaphore_list: Vec::new(), condvar_list: Vec::new(), }) }, }); parent.children.push(Arc::clone(&amp;child)); let task = Arc::new(TaskControlBlock::new( Arc::clone(&amp;child), parent .get_task(0) .inner_exclusvie_access() .res .as_ref() .unwrap() .ustack_base(), false, )); let mut child_inner = child.inner_exclusive_access(); child_inner.tasks.push(Some(Arc::clone(&amp;task))); drop(child_inner); let task_inner = task.inner_exclusive_access(); let trap_cx = task_inner.get_trap_cx(); trap_cx.kernel_sp = task.kstack.get_top(); drop(task_inner); insert_into_pid2process(child.getpid(), Arc::clone(&amp;child)); add_task(task); child } ​ 首先复制父进程的地址空间，申请一个pid, 之后复制父进程的文件描述符表。接着我们需要压入子进程。和上面的函数类似，我们需要为我们的进程创建主线程，将这个线程压入process control block的线程队列中。最后将这个进程的主线程压入到调度队列中。 ​ 但是在加入多线程后更加复杂的是退出的功能，我们要对退出的是主线程的情况进行区分。 pub fn exit_current_and_run_next(exit_code: i32) { let task = take_current_task().unwrap(); let mut task_inner = task.inner_exclusive_access(); let process = task.process.upgrade().unwrap(); let tid = task_inner.res.as_ref().unwrap().tid; task_inner.exit_code = Some(exit_code); task_inner.res = None; drop(task_inner); drop(task); if tid == 0 { let pid = process.getpid(); if pid == IDLE_PID { println!( &quot;[kernel] Idle process exit with exit_code {} ...&quot;, exit_code ); if exit_code != 0 { crate::board::QEMU_EXIT_HANDLE.exit_failure(); } else { crate::board::QEMU_EXIT_HANDLE.exit_success(); } } remove_from_pid2process(pid); let mut process_inner = process.inner_exclusive_access(); process_inner.is_zombie = true; process_inner.exit_code = exit_code; { let mut initproc_inner = INITPROC.inner_exclusive_access(); for child in process_inner.children.iter() { child.inner_exclusive_access().parent = Some(Arc::downgrade(&amp;INITPROC)); initproc_inner.children.push(child.clone()); } } let mut recycle_res = Vec::&lt;TaskUserRes&gt;&gt;::new(); for task in process_inner.tasks.iter().filter(|t| t.is_some()) { let task = task.as_ref().unwrap(); remove_inactive_task(Arc::clone(&amp;task)); if let Some(res) = task_inner.res.take() { recycle_res.push(res); } } drop(process_inner); recycle_res.clear(); let mut process_inner = process.inner_exclusive_access(); process_inner.children.clear(); process_inner.memory_set.recycle_data_pages(); process_inner.fd_table.clear(); process_inner.tasks.clear(); } drop(process); let mut _unused = TaskContext::zero_init(); schedule(&amp;mut _unused as *mut _); } ​ 首先获取当前线程的tid， 设置线程的exit_code， 之后如果tid为0，我们需要设置进程的exit_code, 之后将子进程都转移到init进程下。接下来我们需要回收线程的资源。我们生成一个TaskUserRes， 接着遍历进程的task队列，将这些线程移除队列，并回收对应的线程号。 ​ 最后将文件描述表，子进程表，页资源回收。调度下一个线程。 ​ 接下来是系统调用是如何实现的代码： pub fn sys_thread_create(entry: usize, arg: usize) -&gt; isize { let task = current_task().unwrap(); let process = task.process.upgrade().unwrap(); let new_task = Arc::new(TaskControlBlock::new( task.inner_exclusive_access() .res .as_ref() .unwrap() .ustack_base, true, )); add_task(Arc::clone(&amp;new_task)); let new_task_inner = new_task.inner_exclusive_access(); let new_task_res = new_task_inner.res.as_ref().unwrap(); let new_task_tid = new_task_res.tid; let mut process_inner = process.inner_exclusive_access(); let tasks = &amp;mut process_inner.tasks; while tasks.len() &lt; new_task_tid + 1 { tasks.push(None) } tasks[new_task_tid] = Some(Arc::clone(&amp;new_task)); let new_task_trap_cx = new_task_inner.get_trap_cx(); *new_task_trap_cx = TrapContetx::app_init_context( entry, new_task_res.ustack_top(), kernel_token(), new_task.kstack.get_top(), trap_handler as usize, ) (*new_task_trap_cx.x[10]) = arg; new_task_tid as usize } ​ 线程的创建我们需要获取现在的父进程，之后创建一个空的线程结构，分配一个tid，将这个线程加入进程的队列中，接下来将这个线程的trap上下文设置好就可以返回。 pub fn sys_waittid(tid: usize) -&gt; i32 { let tasks = current_task().unwrap(); let process = tasks.process.upgrade().unwrap(); let task_inner = task.inner_exclusive_access(); let mut process_inner = processor.inner_exclusive_access(); if task_inner.res.as_ref().unwrap().tid == tid { return -1; } let mut exit_code: Option&lt;i32&gt; = None; let waited_task = processor_inner.tasks[tid].as_ref(); if let Some(waited_task) = waited_task { if let Some(waited_exit_code) = waited_task.inner_exclusive_access().exit_code { exit_code = Some(waited_exit_code); } } else { // waited thread does not exist return -1; } if let Some(exit_code) = exit_code { // dealloc the exited thread process_inner.tasks[tid] = None; exit_code } else { // waited thread has not exited -2 } } ​ 获取进程控制块后，会检查子进程中的exit_code，进行返回。 同步互斥 ​ 这里互斥锁的实现是一个自旋锁或者阻塞锁： pub struct MutexSpin { locked: UPSafeCell&lt;bool&gt;, } impl MutexSpin { pub fn new() -&gt; Self { Self { locked unsafe { UPSafeCell} } } } impl Mutex for MutexSpin { fn lock(&amp;self) { loop { let mut locked = self.locked.exclusive_access(); if *locked { drop(locked); suspend_current_and_run_next(); continue; } else { *locked = true; return ; } } } fn unlock(&amp;self) { let mut locked = self.locked.exclusive_access(); *locked = false; } } ​ 对于spin形式的锁，会直接判断这个锁是否已经获取，如果已经被其他的线程获取了，那么就调度其他线程进行执行。而MutexBlocking内部维护了一个队列： pub struct MutexBlocking { inner: UPSafeCell&lt;MutexBlockingInner&gt;, } pub struct MutexBlockingIner { locked: bool, wait_queue: VecDeque&lt;Arc&lt;TaskControlBlock&gt;&gt;, } impl MutexBlocking { pub fn new() -&gt; Self { Self { inner: unsafe { UPSafeCell::new(MutexBlockingInner{ locked: false, wait_queue: VecDeque::new(), }) }, } } } impl Mutex for MutexBlocking { fn lock(&amp;self) { let mut mutex_inner = self.inner.exclusive_access(); if mutex_inner.locked { mutex_inner.wait_queue.push(current_task().unwrap()); drop(mutex_inner); block_current_and_run_next(); } else { mutex_inner.locked = true; } } fn unlock(&amp;self) { let mut mutex_inner = self.inner.exclusive_access(); assert!(mutex_inner.locked); if let Some(waking_task) = mutex_inner.wait_queue.pop_front() { wakeup_task(waking_task); } else { mutex_inner.locked = false; } } } ​ 这里可以发现调用的是block_current_and_run_next， 所说这个线程将被拿出调度队列，于是内核调度时就不会考虑这个线程。而对锁解锁时，就可以拿出一个wait_queue中的阻塞线程加入到调度队列中。 ​ 下面是信号量的实现，也十分类似： pub struct Semaphore { pub inner: UPSafeCell&lt;Semaphore&gt;, } pub struct SemaphoreInner { pub count: isize, pub wait_queue: VecDeque&lt;Arc&lt;TaskControlBlock&gt;&gt;, } impl Semaphore { pub fn new(res_count: usize) -&gt; Self { Self { inner: unsafe { UPSafeCell:new(SemaphoreInner { count: res_count as isize, wait_queue: VecDeque::new(), }) }, } } pub fn up(&amp;self) { let mut inner = self.inner.exclusive_access(); inner.count += 1; if inner.count &lt;= 0 { if let Some(task) = inner.wait_queue.pop_front() { wakeup_task(task); } } } pub fn down(&amp;self) { let mut inner = self.inner.exclusive_access(); inner.count -= 1; if inner.count &lt; 0 { inner.wait_queue.push_back(current_task().unwrap()); drop(inner); block_current_and_run_next(); } } } ​ 同样我们维护了一个队列，相当于对于mutexBlock进行了扩展, 只要信号量小于0，就要压入队列中。 ​ 下面是条件变量： pub struct Condvar { pub inner: UPSafeCell&lt;CondvarInner&gt;, } pub struct CondvarInner { pub wait_queue: VecDeque&lt;Arc&lt;TaskControlBlock&gt;&gt;, } impl Condvar { pub fn new() -&gt; Self { inner: unsafe { UPSafeCell::new(CondvarInner { wait_queue: VecDeque::new(), }) }, } pub fn signal(&amp;self) { let mut inner = self.inner.exclusive_access(); if let Some(task) = inner.wait_queue.pop_front() { wakeup_task(task); } } pub fn wait(&amp;self, mutex: Arc&lt;dyn Mutex&gt;) { mutex.unlock(); let mut inner = self.inner.exclusive_access(); inner.wait_queue.push_back(current_task().unwrap()); drop(inner); block_current_and_run_next(); mutex.lock(); } } ​ 这里采用比较常用的方式，使用mutex和阻塞实现条件变量。 pub fn sys_mutex_create(blocking: bool) -&gt; isize { let process = current_process(); let mutex: Option&lt;Arc&lt;dyn Mutex&gt;&gt; = if !blocking { Some(Arc::new(MutexSpin::new())) } else { Some(Arc::new(MutexBlocking::new())) }; let mut process_inner = process.inner_exclusive_access(); if let Some(id) = process_inner .mutex_list .iter() .enumerate() .find(|(_, item)| item.is_none()) .map(|(id, _)| id) { process_inner.mutex_list[id] = mutex; id as isize } else { process_inner.mutex_list.push(mutex); process_inner.mutex_list.len() as isize - 1 } } ​ 这里将mutex设置为一个Spin或者Block的锁，之后遍历process中的mutex队列，将这个申请的mutex放入队列中。最后会返回锁的ID。 pub fn sys_mutex_lock(mutex_id: usize) -&gt; isize { let process = current_process(); let process_inner = process.inner_exclusive_access(); let mutex = Arc::clone(process_inner.mutex_list[mutex_id].as_ref().unwrap()); drop(process_inner); drop(process); mutex.lock(); 0 } ​ 这里会对应的锁进行锁住，在锁被占用的情况，如果是spin的锁将直接调度下一个进程，如果是block锁，会将对应的线程加入到锁的阻塞队列中。 pub fn sys_mutex_unlock(mutex_id: usize) -&gt; isize { let process = current_process(); let process_inner = process.inner_exclusive_access(); let mutex = Arc::clone(process_inner.mutex_list[mutex_id].as_ref().unwrap()); drop(process_inner); drop(process); mutex.unlock(); } ​ 对于信号量，我们需要up和down函数，来对统计量进行控制。 pub fn sys_semaphore_create(res_count: usize) -&gt; isize { let process = current_process(); let mut process_inner = process.inner_exclusive_access(); let id = if let Some(id) = process_inner .semaphore_list .iter() .enumerate() .find(|(_, item)| item.is_none()) .map(|(id, _)| id) { process_inner.semaphore_list[id] = Some(Arc::new(Semaphore::new(res_count))); id } else { process_inner .semaphore_list .push(Some(Arc::new(Semaphore::new(res_count)))); process_inner.semaphore_list.len() - 1 }; id as isize } pub fn sys_semaphore_up(sem_id: usize) -&gt; isize { let process = current_process(); let process_inner = process.inner_exclusive_access(); let sem = Arc::clone(process_inner.semaphore_list[sem_id].as_ref().unwrap()); drop(process_inner); sem.up(); 0 } pub fn sys_semaphore_down(sem_id: usize) -&gt; isize { let process = current_process(); let process_inner = process.inner_exclusive_access(); let sem = Arc::clone(process_inner.semaphore_list[sem_id].as_ref().unwrap()); drop(process_inner); sem.down(); 0 } ​ 信号量的创建过程和mutex类似，但是这里需要up和down两个函数对于count进行维护，一旦up 表示一类资源进行了释放，可以从队列中取出一个阻塞的任务进行执行。 pub fn sys_condvar_create() -&gt; isize { let process = current_process(); let mut process_inner = process.inner_exclusive_access(); let id = if let Some(id) = process_inner .condvar_list .iter() .enumerate() .find(|(_, item)| item.is_none()) .map(|(id, _)| id) { process_inner.condvar_list[id] = Some(Arc::new(Condvar::new())); id } else { process_inner .condvar_list .push(Some(Arc::new(Condvar::new()))); process_inner.condvar_list.len() - 1 }; id as isize } pub fn sys_condvar_signal(condvar_id: usize) -&gt; isize { let process = current_process(); let process_inner = process.inner_exclusive_access(); let condvar = Arc::clone(process_inner.condvar_list[condvar_id].as_ref().unwrap()); drop(process_inner); condvar.signal(); 0 } pub fn sys_condvar_wait(condvar_id: usize, mutex_id: usize) -&gt; isize { let process = current_process(); let process_inner = process.inner_exclusive_access(); let condvar = Arc::clone(process_inner.condvar_list[condvar_id].as_ref().unwrap()); let mutex = Arc::clone(process_inner.mutex_list[mutex_id].as_ref().unwrap()); drop(process_inner); condvar.wait(mutex); 0 } ​ 对于条件变量，创建过程一样，需要在进程的条件变量队列中进行申请。这里分析signal和wait，可以发现这里不同的一点是wait需要传入一个mutex_id, 这里wait是会将这个锁打开，让后将线程控制块加入到队列中。一旦一个signal发生，会将一个线程控制块取出，之后线程控制块的代码从sys_condvar_wait代码中恢复，将锁再次锁住。 ​ 这样使用一个条件变量时，就必须先使用wait。等待一个事件进行signal。 ","link":"https://porterlu.github.io/post/rcore-di-ba-zhang/"},{"title":"rCore第六章练习  ","content":"实践作业 硬链接 硬链接要求两个不同的目录项指向同一个文件，在我们的文件系统中也就是两个不同名称目录项指向同一个磁盘块。 本节要求实现三个系统调用 sys_linkat、sys_unlinkat、sys_stat 。 linkat： syscall ID: 37 功能：创建一个文件的一个硬链接， linkat标准接口 。 Ｃ接口： int linkat(int olddirfd, char* oldpath, int newdirfd, char* newpath, unsigned int flags) Rust 接口： fn linkat(olddirfd: i32, oldpath: *const u8, newdirfd: i32, newpath: *const u8, flags: u32) -&gt; i32 参数： olddirfd，newdirfd: 仅为了兼容性考虑，本次实验中始终为 AT_FDCWD (-100)，可以忽略。flags: 仅为了兼容性考虑，本次实验中始终为 0，可以忽略。oldpath：原有文件路径newpath: 新的链接文件路径。 说明： 为了方便，不考虑新文件路径已经存在的情况（属于未定义行为），除非链接同名文件。返回值：如果出现了错误则返回 -1，否则返回 0。 可能的错误 链接同名文件。 unlinkat: syscall ID: 35 功能：取消一个文件路径到文件的链接, unlinkat标准接口 。 Ｃ接口： int unlinkat(int dirfd, char* path, unsigned int flags) Rust 接口： fn unlinkat(dirfd: i32, path: *const u8, flags: u32) -&gt; i32 参数： dirfd: 仅为了兼容性考虑，本次实验中始终为 AT_FDCWD (-100)，可以忽略。flags: 仅为了兼容性考虑，本次实验中始终为 0，可以忽略。path：文件路径。 说明： 为了方便，不考虑使用 unlink 彻底删除文件的情况。 返回值：如果出现了错误则返回 -1，否则返回 0。 可能的错误 文件不存在。 fstat: syscall ID: 80 功能：获取文件状态。 Ｃ接口： int fstat(int fd, struct Stat* st) Rust 接口： fn fstat(fd: i32, st: *mut Stat) -&gt; i32 参数： fd: 文件描述符st: 文件状态结构体 #[repr(C)] #[derive(Debug)] pub struct Stat { /// 文件所在磁盘驱动器号，该实验中写死为 0 即可 pub dev: u64, /// inode 文件所在 inode 编号 pub ino: u64, /// 文件类型 pub mode: StatMode, /// 硬链接数量，初始为1 pub nlink: u32, /// 无需考虑，为了兼容性设计 pad: [u64; 7], } /// StatMode 定义： bitflags! { pub struct StatMode: u32 { const NULL = 0; /// directory const DIR = 0o040000; /// ordinary regular file const FILE = 0o100000; } } 实现 link ​ link的实现会调用inode模块中的add_a_link方法，它是ROOT_NODE的一个方法 pub fn add_a_link(&amp;self, oldpath: &amp;str, newpath:&amp;str) -&gt; isize { let mut fs = self.fs.lock(); self.modify_disk_inode(|root_inode| { let file_count = (root_inode.size as usize) / DIRENT_SZ; let new_size = (file_count + 1) * DIRENT_SZ; for i in 0..file_count { let mut dirent = DirEntry::empty(); root_inode.read_at(i * DIRENT_SZ, dirent.as_bytes_mut(), &amp;self.block_device); if dirent.name() == oldpath { self.increase_size(new_size as u32, root_inode, &amp;mut fs); root_inode.write_at(new_size * DIRENT_SZ, DirEntry::new(newpath, dirent.inode_number()).as_bytes(), &amp;self.block_device); } } }); 0 } ​ 首先，代码中会尝试获取锁，之后通过回调函数，对自己这个inode进行操作。由于是根inode，我们可以遍历其中的目录，遍历其中的文件名，如果发现文件名和old_path相同的，可以添加一个记录，由于是硬链接公用一个inode即可。 unlink ​ unlink的实现会稍微复杂一点， pub fn rm_a_link(&amp;self, path: &amp;str) -&gt; isize { let mut fs = self.fs.lock(); self.modify_disk_inode(|root_inode| { let file_count = (root_inode.size as usize) / DIRENT_SZ; for i in 0..file_count { let mut dirent = DirEntry::empty(); root_inode.read_at(i * DIRENT_SZ, dirent.as_bytes_mut(), &amp;self.block_device); if dirent.name() == path { for j in i+1..file_count{ root_inode.read_at(i*DIRENT_SZ, dirent.as_bytes_mut(), &amp;self.block_device); root_inode.write_at((i-1) * DIRENT_SZ, dirent.as_bytes(), &amp;self.block_device); } root_inode.size -= 1; break; } } }); 0 } ​ 我们仍然遍历根目录，如果找到一个和我们要找的文件名相同的项，我们就根据比较排序类似的方法， 将读到的后一项覆盖到前一项，这样目录中就不会出现空项。 fstat ​ 我们需要在File中实现一个新的方法，用于获取这个实现File Trait的结构体的信息。 fn stat(&amp;self, st:&amp;mut Stat) -&gt; usize { let inner = self.inner.exclusive_access(); inner.inode.stat(unsafe { core::slice::from_raw_parts_mut(st as *mut Stat as usize as *mut u8, core::mem::size_of::&lt;Stat&gt;(), ) }); 1 } ​ 我们实现了fstat方法，Stat结构体如下 pub struct Stat { pub dev: u64, pub ino: u64, pub mode: StatMode, pub nlink: u32, pad: [u64; 7], } ​ 由于要在这里没有在easy-fs实现Stat结构体，我选择将Stat转化为字节数组引用进行传递 第一步，&amp;mut Stat可以转化为* mut Stat指针 指针可以转化为usize地址 再将usize转化为一个*mut u8的指针 ​ 最后构造一个引用 pub fn stat(&amp;self, st: &amp;mut [u8]) -&gt; usize{ let mut fs = self.fs.lock(); let st_trans:&amp;mut Stat = unsafe { ( st.as_mut_ptr() as usize as *mut Stat).as_mut().unwrap()}; let inode_id = ((self.block_id - self.fs.lock().inode_area_start_block as usize) * BLOCK_SZ + self.block_offset) / core::mem::size_of::&lt;Inode&gt;(); st_trans.ino = inode_id as u64; let mut link_num = 0; let mut _type = StatMode::FILE; EasyFileSystem::root_inode(&amp;self.fs.read_disk_inode(|disk_inode| { let file_count = (disk_inode.size as usize) / DIRENT_SZ; for i in 0..file_count { let mut dirent = DirEntry::empty(); if dirent.inode_number() == inode_id as u32 { link_num += 1; } } if(disk_inode.is_dir()){ _type = StatMode::DIR; } }); st_trans.nlink = link_num; st_trans.mode = _type; 1 } 最后在easy-fs侧实现了这样一个函数 第一步，首先根据自己的inode的结构体信息计算出自己在块设备中的位置 遍历根目录中的文件，如果发现inode号相同，则将链接数加1 最后设置文件类型 ","link":"https://porterlu.github.io/post/rcore-di-liu-zhang-lian-xi/"},{"title":"rCore 第七章","content":"管道 ​ 在引入文件系统后，我们使用File Trait对于一个进程可以读写的文件进行说明，基于File Trait我们实现管道，它可以将不同进程的输入和输出进行连接。我们为用户提供sys_pipe接口，用户可以通过这个接口，获取到一对读写口。 pub fn sys_pipe(pipe: *mut usize) -&gt; isize; ​ 我们只需要将pipe设置为一种特殊的File Trait, 在内核空间中会有如下过程， pub fn make_pipe() -&gt; (Arc&lt;Pipe&gt;, Arc&lt;Pipe&gt;) { let buffer = Arc::new(unsafe { UPSafeCell::new(PipeRingBuffer::new()) }); let read_end = Arc::new(Pipe::read_end_with_buffer(buffer.clone())); let write_end = Arc::new(Pipe::write_end_with_buffer(buffer.clone())); buffer.exclusive_access().set_write_end(&amp;write_end); (read_end, write_end) } ​ 这个读口和写口会独自占用一个fd_table中文件描述符号，通过文件描述符可以读写内核空间中的管道，这样一旦调用fork就可以读写内核中同一ringbuffer。为了保证写口被关闭时，直接返回，我们可以通过ringbuffer中的weak连接判断写口是否已经关闭。 ​ 如下是这个可阻塞的ringbuffer的部分代码示例， let mut ring_buffer = self.buffer.exclusive_access(); let loop_read = ring_buffer.available_read(); if loop_read = 0 { if ring_buffer.all_write_ends_closed() { return already_read; } drop(ring_buffer); suspend_current_and_run_next(); continue; } for _ in 0..loop_read { if let Some(byte_ref) = buf_iter.next() { unsafe { *byte_ref = ring_buffer.read_byte(); } already_byte += 1; if already_read == want_to_read { return want_to_read; } } else { return already_read; } } ​ 如果缓冲区中已经为空，那么就要进行阻塞调度下一个进程，但是写口关闭我们直接返回；否则，在buffer中有多少字符读多少字符。 输入输出重定向 为了实现cat工具来验证我们的输入输出重定向，我们必须介绍参数是如何通过exec系统调用传入最后进程的用户栈的。首先shell要将ls -lia分成两个字符串进行传入，那么在exec系统调用是会将上图所示的结构压入到用户栈，由于一些机器不支持非对其访存，可能引发异常，所以我们这里进行一次对齐。之后，执行这个应用程序时，运行时库会将exec传入的参数argc和argv进行分析，构造成str引用的各式，最后传入到我们的应用程序。 ​ 之后就是重定向本身的实现，我们需要新添加一个系统调用sys_dup，它接受一个文件描述符号，并且拷贝它。我们由于关闭了stdout，那么新的fd号就可以替代原来stdout的位置，从而达成重定向。 信号 这里是rCore中所用到的信号的数据结构，我们用bitflags来实现一个信号变量，每一个bit表示一个信号，同时定义SignalAction, 定义来信号对应的处理函数和执行这个信号handler时需要屏蔽的信号。最后是添加了信号后，我们需要扩展我们的TaskControlBlock: 这里的signals 表示pending等待处理的信号。 signal_mask, 表示进程屏蔽的信号。 handling_sig, 表示正在处理的信号。 killed，表示目前进程是否要被杀除。 frozen, 表示目前进程是否因为信号被阻塞。 trap_cx_backup，表示调用用户的signal handler时，我们需要存储trap上下文。 根据上图的流程，我们展示trap文件中的内容 //... Trap::Exception(Exception::StoreFault) | Trap::Exception(Exception::StorePageFault) | Trap::Exception(Exception::InstructionFault) | Trap::Exception(Exception::InstructionPageFault) | Trap::Exception(Exception::LoadFault) | Trap::Exception(Exception::LoadPageFault) =&gt; { println!( &quot;[kernel] {:?} in application, bad addr = {:#x}, bad instruction = {:#x}, kernel killed it.&quot;, scause.cause(), stval, current_trap_cx().sepc ); //exit_current_and_run_next(-2); current_add_signal(SignalFlags::SIGSEGV); } Trap::Exception(Exception::IllegalInstruction) =&gt; { println!(&quot;[kernel] IllegalInstruction in application, kernel killed it.&quot;); //exit_current_and_run_next(-3); current_add_signal(SignalFlags::SIGILL); } Trap::Interrupt(Interrupt::SupervisorTimer) =&gt; { set_next_trigger(); suspend_current_and_run_next(); } //... ​ 我们可以看到在发生一些异常时，我们将信号添加到了该进程的信号pending队列中，这样之后我们就可以检查到了。 ​ 同时信号除了上述的产生方式，还有两种异步的产生方式： 进程通过kill给自己或者其他的进程发送信号 内核检测到某些事件，然后给某些进程发信号，这里没有实现。 pub fn sys_kill(pid: usize, signum: i32) -&gt; isize { if let Some(task) = pid2task(pid) { if let Some(flag) = SignalFlags::from_bits(1 &lt;&lt; signum) { //... task_ref.signals.insert(flag); 0 } else { -1 } } //... } ​ 而信号的处理，由两个主要函数： pub fn handle_signals() { loop { check_pending_signals(); let (frozen, killed) = { let task = current_task().unwrap(); let task_inner = task.inner_exclusive_access(); (task_inner.frozen, task_inner.killed) }; if !frozen || killed { break; } suspend_current_and_run_next(); } } ​ 在trap结束前，会执行handle_signals这个函数。同时 fn check_pending_signals() { for sig in 0..(MAX_SIG+1) { let task = current_task().unwrap(); let task_inner = task.inner_exclusive_access(); let signal = SignalFlags::from_bits(1 &lt;&lt; sig).unwrap(); if task_inner.signals.contains(signal) &amp;&amp; (!task_inner.signal_mask.contains(signal)) { let mut masked = true; let handling_sig = task_inner.handling_sig; if handling_sig == -1 { masked = false; } else { let handling_sig = handling_sig as usize; if !task_inner.signal_actions.table[handling_sig] .masl .contains(signal) { masked = false; } } if !masked { drop(task_inner); drop(task); if signal == SignalFlags::SIGKILL || signal == SignalFlags::SIGSTOP || signal == SignalFlags::SIGCONT || signal == SignalFlags::SIGDEF { call_kernel_signal_handler(signal); } else { call_user_signal_handler(sig, signal); return; } } } } } ​ 在check_pending_signals这个函数中，所有的信号都会被检查一遍，如果这个信号在pending队列中，并且没有被屏蔽。如果现在这个进程没有正在处理信号，那么这个信号肯定没屏蔽的；否则，正在处理的信号可能将这个判断的信号进行了屏蔽。 ​ 接着在处理函数进行了写死，SIGKILL、SIGSTOP、SIGCONT、SIGDEF由内核进行处理： fn call_kernel_signal_handler(signal: SignalFlags) { let task = current_task().unwrap(); let mut task_inner = task.inner_exclusive_access(); match signal { SignalFlags::SIGSTOP =&gt; { task_inner.frozen = true; task_inner.signals ^= SignalFlags::SIGSTOP; } SignalFlags::SIGCONT =&gt; { if task_inner.signals.contains(SignalFlags::SIGCONT) { task_inner.signals ^= SignalFlags::SIGCONT; task_inner.frozen = false; } } _ =&gt; { task_inner.killed = true; } } } ​ 除了SIGSTOP和SIGCONT进行特殊处理，其他的直接将killed置位进行杀除。 ​ 否则调用user的处理函数： fn call_user_signal_handler(sig: usize, signal: SignalFlags) { let task = current_task().unwrap(); let mut task_inner = task.inner_exclusive_access(); let handler = task_inner.signal_actions.table[sig].handler; if handler != 0 { task_inner.handling_sig = sig as isize; task_inner.signals ^= signal; let mut trap_ctx = task_inner.get_trap_cx(); task_inner.trap_ctx_backup = Some(*trap_ctx); trap_ctx.sepc = handler; trap_ctx.x[10] = sig; } else { //... } } ​ 这里如果是user_handler的话，需要获取处理函数的地址，同时保存现在的上下文，最后将上下文中的返回值设置为sig。执行完这些函数check_pending_signals将会返回。 handle_signals(); if let Some((errno, msg)) = check_signals_error_of_current() { println!(&quot;[kernel] {}&quot;, msg); exit_current_and_run_next(errno); } trap_return(); ​ 我们看到，如果发现信号中包含的错误信息，会直接退出该进程。 ​ 最后是用户如何在处理完信号之后进行返回， pub fn sys_sigreturn() -&gt; isize { if let Some(task) = current_task() { let mut inner = task.inner_exclusvie_access(); inner.handling_sig = -1; let trap_ctx = inner.get_trap_cx(); *trap_ctx = inner.trap_ctx_back.unwrap(); trap_ctx.x[10] as isize } else { -1 } } ","link":"https://porterlu.github.io/post/rcore-di-qi-zhang/"},{"title":"RVWMO观他人博客总结","content":"#RISC-V弱内存模型 这里RISC-V内存模型为了保证执行顺序和程序的一致性，利用了四大类，13个规则。这里下面的指令是指访存指令，我们可以看看下面的规则，如何保证执行时保证前后两条访存的顺序。 重叠地址 一、 store 在后 不像TSO对于store的顺序要求很严格，这里只要求对于同一个地址的顺序。 a) load/store addr0 b) store addr0 对于后面b是store的情况，我们必须保证这个顺序。 二、CoRR 对于两条load之间没有写指令的情况，我们保证后一个load不会读取到更老的值。 a) load a0,addr0 b) load a1,addr0 即使其他核上对addr0，进行了写入，也只可能让b读取到更新的值。 三、原子指令 前一个指令是原子指令，后一条指令是读取，那么后一条指令一定能读取到前一条指令写入的值。 显示同步 四、FENCE指令 如果两条访存指令之间有一条fence指令，那么必须根据fence的要求进行顺序的确定。 五、acquire 如果前一条指令有acquire，那么必须要求后面的指令不能超前。 六、release 如果后面一条指令有release，那么要求前面的指令不能延后。 七、两条指令都有acquire和release标志 当然也必须顺序 八、LR/SC 这里LR是Load reserved，SC是store condition，会要求只用先进行LR之后才能 SC。 语法依赖 语法依赖感觉有点像流水线中的依赖关系 九，后一条指令的地址依赖于前一条指令 十，后一条指令的数据依赖于前一条指令 十一，后一条指令在控制流上依赖前一条指令，即一个分支是否流向了后一条指令。 流水线依赖 十二 a) store/load &lt;memory operation depend on a&gt; b) load value stored by last memory op 十三 a) load/store &lt;memory operation address depend on a&gt; b) store 例子 int x = 0; int y = 0; void t1() { y = 1; int res = x; printf(&quot;x=%d\\n&quot;, res); } void t2() { x = 1; int res = y; printf(&quot;y=%d\\n&quot;,res); } 这个程序在x86上会出现0, 0的情况，因为它是TSO, 在如上的RISC-V机器上也可能出现。 Sequential Consistency 在程序顺序上的前后，在插入全局内存序，依旧保持，而且部分是否是同一地址。 但是SC也无法保证不同核上程序之间到底是什么顺序，我们可能需要一些原子指令。 int sum = 0; void t1() { sum += 1; } void t2() { sum += 2; } 这个程序由于没有原子性的保证所以，结果可能不是3。在RISC-V中，我们可以使用LR和SC进行。 TSO 在流水线设计中，我们往往会加入一个store buffer, load可以使用buffer转发的方式获取最新的值，store buffer大大提高了外设和内存的访问效率。当然如果是外设的情况，不能进行转发，因为IO必须保证顺序。 如果是这样，为了适应store buffer的加入，load就可以乱序到store前面，那么就有了TSO这种模型进行说明。 ","link":"https://porterlu.github.io/post/rvwmo-guan-ta-ren-bo-ke-zong-jie/"},{"title":"Keystone","content":" 概述 今天分享的论文是UCB团队在2019年发表的Keystone Enclave，它的卖点在于可以定制化，Keystone设计了一套流程，让硬件制造商，平台提供者，开发人员可以进行协作开发可信执行程序。只要硬件提供了最基础的隔离原语，就可以通过Keystone定义一套软件自定义的可信执行环境。它在论文中提出自己有这几点贡献，可定制、模块化、开源、低开销。 威胁模型 ​ 这里假设攻击者分为几种类型：基于硬件的攻击者，他可以修改、重发、拦截离开SoC的信号；基于软件方法的攻击者，它可以控制Host端的User Mode程序，不可信的操作系统，网络通信，不受TEE保护的内存和发送给Enclave的消息；使用侧信道漏洞的攻击者可以观测到可信组件和不可信组件通信过程中泄露的信息。但是不考虑拒绝服务攻击。 ​ 这里再一次提出，Keystone是一个可以定制的TEE框架，它可以基于平台的特性和应用的场景开启或者关闭一些特性，来达到想要的安全性和性能。 设计 接口 ​ 在可信组件中，Keystone介绍了Security Monitor和Runtime。Security Monitor负责将不可信的组件和我们所执行的tee环境隔开，而runtime负责代表我们开发的可信App和Security Monitor通信。如上是我们的security monitor接口，下面三个由于我们Runtime分则发起，而上面三个有我们的Linux内核发起用于撤销Enclave在内核中申请的资源。 隔离 ​ 尽管keystone不限于RISC-V平台，但是这里实现在RISC-V平台之上，所以使用的是PMP进行隔离。 ​ 如上就是论文中的配图，最左边是PMP配置寄存器和PMP寄存器用于可信区域的隔离，Keystone将Security Monitor、Enclave和不可信的OS通过PMP隔离开。而执行时又将PMP进行翻转，那么就可执行User Mode中的Enclave程序，Security Monitor连接起了可信区和不可信的区域。 ​ 同时Keystone在进行隔离域的建立和释放时还进行了核间同步，和之前的工作一样，Keystone也支持Secure Boot、安全随机源（目前没有）、可信时钟、远程验证、异常委托。 组件 ​ 如上图所示，这里构想了一种工作模式就是一旦硬件制造好了，平台的提供者可以根据已有的硬件对于Security Monitor继续配置，然后软件开发者可以根据SDK跟腱自己的程序。 ​ 而组件就是平台提供者可以自定的东西，但是虽然是组件，其实很多的组件基本是必须有的。 Free Memory，可以申请一段空间空间，供之后动态申请。 Dynamic Resizing，可能地的话，扩大region的大小。 Edge Call，由于可信App和Host之间无法之间访问，Keystone中构建了一个缓冲区，通过偏移信息。 Syscall，和Edge Call相似可以向Linux内核发送一些IO请求。 多线程，在一个核上一个Enclave内跑多核线程。 Cache分区，和以前的TEE论文中采用相同方法，可以根据平台而定。 实现和测试 ​ SM的TCB自己写的部分只有不到2000行，Runtime最小只用3000行，加上一些Security Monitor的库大概10000行代码。 ​ 测了RV8、Beebs、CoreMark、IOZone。 ​ 上图是IOZone的结果，并不是很好。但是在其他计算密集型任务中效果不错, 论文中说明只有10%的开销，毕竟Keystone并不用修改App原本的代码。 总结 ​ 论文中提到卖点，在今天看来可能不是Keystone受到重视的原因。模块化还是可定制，从设计层面Keystone并没有走的很远，但它是MIT的Sanctum的续作，第一个开源RISC-V TEE，它可以真正的运行，文档也较为详实。 ","link":"https://porterlu.github.io/post/keystone/"},{"title":"rCore 第6章","content":"概述 ​ 这一章节中我们要开发一个简易的文件系统，它结构层次如下图所示： 设备接口 ​ 在设备接口即图中的BlockDevice Trait处，我们只留下了接口，需要内核提供驱动。他的代码就是简单的带两个函数的Trait。 pub trait BlockDevice: Send + Sync + Any { fn read_block(&amp;self, block_id: usize, buf: &amp;mut [u8]); fn write_block(&amp;self, block_id: usize, buf: &amp;[u8]); } ​ 这里都以缓冲区为单位和块设备进行交互，我们在内存中设置一段内存作为缓冲区，缓存块设备中的数据。 块缓存 ​ 使用块缓存将所有的读写操作进行合并，如果在内存缓存区中命中，我们就不访问磁盘。同时我们使用一个数据结构将这些缓存块集中起来进行管理， pub struct BlockCache { cache: [u8; BLOCK_SZ], block_id: usize, block_device: Arc&lt;dyn BlockDevice&gt;, modified: bool, } ​ 其中一个块有512字节，同时使用modified表示这个块是否被修改过，如果被修改过，那么这个块在调出缓存时，将会将这个块写回块设备。 ​ 我们为缓存块实现了read和modify方法，供上层调用。 pub fn read&lt;T, V&gt;(&amp;self, offset: usize, f: impl FnOnce(&amp;T) -&gt; V) -&gt; V { f(self.get_ref(offset)); } pub fn modify&lt;T, V&gt;(&amp;mut self, offset: usize, f: impl FnOnce(&amp;mut T) -&gt; V) -&gt; V{ f(self.get_mut(offset)); } ​ 这里传入了一个闭包类型FnOnce，它将对当前缓存块进行操作。 ​ 我们设置了16个块缓存交给BlockCacheManager进行管理，我们获取一个块缓存需要调用BlockCacheManager的方法。 impl BlockCacheManager { pub fn get_block_cache( &amp;mut self, block_id: usize, block_device: Arc&lt;dyn BlockDevice&gt;, ) -&gt; Arc&lt;Mutex&lt;BlockCache&gt;&gt; { if let Some(pair) = self.queue .iter() .find(|pair| pair.0 == block_id){ Arc::clone(&amp;pair.1) } else { if self.queue.len() == BLOCK_CACHE_SIZE { if let Some((idx, _)) = self .queue .iter() .enumerate() .find(|(_, pair)| Arc::strong_count(&amp;pair.1) == 1){ self.queue.drain(idx..=idx); } else { panic!(&quot;Run out of BlockCache!&quot;); } } let block_cache = Arc::new(Mutex::new(BlockCache::new( block_id, Arc::clone(&amp;block_device), ))); self.queue.push_back((block_id, Arc::clone(&amp;block_cache))); block_cache } } } ​ 代码中会遍历所有的缓存块，匹配块号相同的块缓存进行返回。如果不存块号相同的缓存块则会对队列进行遍历，找到引用计数为1的块，对这个块进行回收，同时读入新的块并加入队列。 BitMap结构 ​ 这里所有的inode和数据块都通过BitMap进行管理，这里使用了两个类型对位图进行描述： type BitmapBlock = [u64; 64]; pub struct BitMap { start_block_id: usize, blocks: usize, } ​ 这里我们子需要记录位图的起始块和总的大小，具体的块使用BitmapBlock进行描述。我们使用get_block_cache方法获取到块缓存后可以，设置了方法alloc和dealloc进行对位图中bit进行置位和消除。 Layout ​ 这个文件实现了一个文件的块设备布局结构，我们设置了多级索引，直接索引存在28个，一级索引存在1个，2级索引1个，每个索引是4字节，所以一个block中最多128个索引。 ​ Layout的读写都需要对直接索引、一级索引和二级索引进行讨论， Easy File System ​ Easy File System对于整个文件系统的磁盘布局进行了描述，它的结构如下： pub struct EasyFileSystem { pub block_device: Arc&lt;dyn BlockDevice&gt;, pub inode_bitmap: Bitmap, pub data_bitmap: Bitmap, inode_area_start_block: u32, data_area_start_block: u32, } ​ 这里的代码实现了每个分区在磁盘的位置，块的回收和分配都将通过Easy File System进行分配和调度。创建一个这样的文件系统需要分为以下的几步。 计算各个分区需要包含多少的块，先需要计算inode位图的大小，获取inode区域的大小。剩余的区域要留给数据区和数据位图。 创建Easy File System的数据结构 对我们使用的块进行全部的清零。 对超级块根据文件系统的数据进行初始化。 创建0号iNode进行初始化的根目录。 Virtual File System ​ 在这一层提供暴露给用户的结构，对用户隐藏磁盘的操作。用户可以看见iNode的结构。 pub struct Inode { block_id: usize, block_offset: usize, fs: Arc&lt;Mutex&lt;EasyFileSystem&gt;&gt;, block_device: Arc&lt;dyn BlockDevice&gt;, } ​ 这里实现的方法大多都采用了传入一个函数的方法进行回调。 impl Inode { fn read_disk_inode&lt;V&gt;(&amp;self, f: impl FnOnce(&amp;DiskInode) -&gt; V) -&gt; V { get_block_cache ( self.block_id, Arc::clone(&amp;self.block_device) ).lock().read(self.block_offset, f) } } ​ 这个方法期待调用传入一个函数，这个函数的接受的参数为一个DiskInode的引用，起始就是get_block_cache().lock().read()之后自动换传给f进行回调的参数。 ​ 如我们用它实现一个find方法。 impl Inode { pub fn find(&amp;self, name:&amp;str) -&gt; Option&lt;Arc&lt;Inode&gt;&gt; { let fs = self.fs.lock(); self.read_disk_inode(|disk_inode|{ self.find_inode_id(name, disk_inode) .map(|inode_id|{ let (block_id, block_offset) = fs.get_disk_inode_pos(inode_id); Arc::new(Self::new( block_id, block_offset, self.fs.clone(), self.block_device.clone(), )) }) }) } fn find_inode_id( &amp;self, name: &amp;str, disk_inode: &amp;DiskInode, ) -&gt; Option&lt;u32&gt; { assert!(disk_inode.is_dir()); let file_count = (disk_inode.size as usize) / DIRENT_SZ; let mut dirent = DirEntry::empty(); for i in 0..file_count { assert_eq!( disk_inode.read_at( DIRENT_SZ * i, dirent.as_bytes_mut(), &amp;self.block_device, ), DIRENT_SZ, ); if dirent.name() == name { return Some(dirent.inode_numer() as u32); } } } } ​ 这里我们先从find_inode_id开始解析，对这个inode从磁盘进行读取后，可以调用这个函数进行分析，对于文件的数据进行遍历，找到名字相同的想返回这个目录的inode_id。 ​ 之后介绍find, 这里先获取了Easy File System的锁，所有对于文件系统的使用，全程都要持有互斥锁。获取完锁之后，会通过read_disk_inode方法调用匿名函数，通过名字获得id后生成一个inode进行返回。 测试 ​ 在用户态测试easy-fs的功能，我们需要在用户态为efs实现BlockDevice Trait，这样就可以使用Inode操作文件系统了。我们使用Linux的文件系统来模拟底层的块设备。 use std::fs::File; use easy-fs::BlockDevice; const BLOCK_SZ: usize = 512; struct BlockFile(Mutex&lt;File&gt;); impl BlockDevice for BlockFile { fn read_block(&amp;self, block_id: usize, buf: &amp;mut[u8]) { let mut file = self.0.lock().unwrap(); file.seek(SeekFrom::Start((block_id * BLOCK_SZ) as u64)) .expect(&quot;Error when seeking!&quot;); assert_eq!(file.read(buf).unwrap(), BLOCK_SZ, &quot;Not a complete block!&quot;); } fn write_block(&amp;self, block_id: usize, buf:&amp;[u8]) { let mut file = self.0.lock().unwrap(); file.seek(SeekFrom::Start((block_id * BLOCK_SZ) as u64)) .expect(&quot;Error when seeking&quot;); assert_eq!(file.write(buf).unwrap, BLOCK_SZ, &quot;Not a complete block!&quot;); } } ​ 这样我们实际操作就是Linux的File, 我们通过在文件中seek并进行读写就可以模拟对块设备进行读写。 ​ 之后我们需要创建文件镜像，实际上就是创建一个文件，内容是我们Easy File System的镜像内容。 use clap::{Arg, App} fn easy_fs_pack() -&gt; std::io::Result&lt;()&gt; { let matches = App::new(&quot;EasyFileSystem packer&quot;) .arg(Arg::with_name(&quot;source&quot;) .short(&quot;s&quot;) .long(&quot;source&quot;) .takes_value(true) .help(&quot;Executable source dir(with backslash)&quot;) ) .arg(Arg::with_name(&quot;target&quot;) .short(&quot;t&quot;) .long(&quot;target&quot;) .takes_value(true) .help(&quot;Executable target dir(with backslash)&quot;) ) .get_matches(); let src_path = matches.value_of(&quot;source&quot;).unwrap(); let target_path = matches.value_of(&quot;target&quot;).unwrap(); println!(&quot;src_path = {}\\ntarget_path = {}&quot;, src_path, target_path); let block_file = Arc::new(BlockFile(Mutex::new({ let f = OpenOptions::new() .read(true) .write(true) .create(true) .open(format!(&quot;{}{}&quot;, target_path, &quot;fs.img&quot;))?; f.set_len(8192 * 512).unwrap(); f }))); let efs = EasyFileSystem::create( block_file.clone(), 8192, 1 ); let root_inode = Arc::new(EasyFileSystem::root_inode(&amp;efs)); let apps: Vec&lt;_&gt; = read_dir(src_path) .unwrap() .into_iter() .map(|dir_entry|{ let mut name_with_ext = dir_entry.unwrap().file_name().into_string().unwrap(); name_with_ext.drain(name_with_ext.find('.').unwrap()..name_with_ext.len()); name_with_ext }) .collect(); for app in apps { let mut host_file = File::open(format!(&quot;{}{}&quot;, target_path, app)).unwrap(); let mut all_data: Vec&lt;u8&gt; = Vec::new(); host_file.read_to_end(&amp;mut all_data).unwrap(); let inode = root_inode.create(app.as_str()).unwrap(); inode.write_at(0, all_data.as_slice());3 } } ​ 这里首先使用clap 对输入进行了分析，-s表示输入的源文件，-t表示目标文件。获取完这些参数后，通过Linux的文件系统创建一个可读可写的大小为4MB的fs.img文件。 之后将这个文件作为块设备，创建Easy File System，并读取这个文件系统根目录。紧接着读取我们的源文件目录下的源文件名进行读取，生成目录下的文件名列表，之后通过在文件系统中创建inode，设置文件名，并将app的二进制内容写入对应的inode中。 接入内核 UserBuffer ​ 有了文件的抽象后，操作系统就可以将需要读写并持续存储的文件按文件来管理，并把文件分配给进程，让进程以很简洁的同一抽象接口File来读写文件。 pub trait File: Send + Sync { fn read(&amp;self, buf: UserBuffer) -&gt; usize; fn write(&amp;self, buf: UserBuffer) -&gt; usize; } ​ 这个接口建立起了内存和存储设备之间的联系，其中UserBuffer是我们mm模块中定义的在应用地址空间中的一段缓冲区。我们为OSInode实现了File Trait, 这样OSInode就可以调用read或者write和UserBuffer进行交互。 impl File for OSInode { //... fn read(&amp;self, mut buf: UserBuffer) -&gt; usize { let mut inner = self.inner.exclusive_access(); let mut total_read_size = 0usize; for slice in buf.buffers.iter_mut() { let read_size = inner.inode.read_at(inner.offset, *slice); if read_size == 0 { break; } inner.offset += read_size; total_read_size += read_size; } total_read_size } fn write(&amp;self, buf: UserBuffer) -&gt; usize { let mut inner = self.inner.exclusive_access(); let mut total_write_size = 0usize; for slice in buf.buffers.iter() { let write_size = inner.inode.write_at(inner.offset, *slice); assert_eq!(write_size, slice.len()); inner.offset += write_size; total_write_size += write_size; } total_write_size += write_size; } } ​ 读写时，由于UserBuffer中的buffers是一个Vec&lt;&amp;'static mut[u8]&gt;, 所以buf.buffers.iter()就是遍历vec，对每一个u8数组进行读写。read和write完成对于inode进行buffers的读和写，返回值是读写的字节数。 块设备 ​ 在qemu中我们使用了virtio设备。 #[cfg(feature = &quot;board_qemu&quot;)] type BlockDeviceImpl = virtio_blk::VirtIOBlock; #[cfg(feature = &quot;board_k210&quot;)] type BlockDeviceImpl = sdcard::SDCardWrapper; lazy_static! { pub static ref BLOCK_DEVICE: Arc&lt;dyn BlockDevice&gt; = Arc::new(BlockDeviceImpl::new()); } ​ 这里virtio_blk::VirtIOBlock实现了BlockDevice Trait，所以我们根据编译选项，确定使用哪一个实现。 ​ 同时在qemu启动时，我们要将我们的磁盘镜像挂载到qemu的mmio接口上。 -drive file=fs.img,if=none,format=raw,id=x0 \\ -device virtio-blk-device,drive=x0,bus=virtio-mmio-bus.0 ​ 将我们的fs.img命名为x0，之后，将x0作为virtio总线上的一个块设备接入，之后再设定通过mmio进行控制。 ​ 在qemu的代码中，我们可以代码，qemu的riscv版本，virtio的mmio地址空间为从0x10001000开始的4KB，于是我们必须进行如下的映射。 #[cfg(feature = &quot;board_qemu&quot;)] pub const MMIO: &amp;[(usize, usize)] = &amp;[ (0x10001000, 0x1000), ]; 在内核初始化时，这段内存已经被映射。 impl MemorySet { pub fn new_kernel() -&gt; Self { //... println!(&quot;mapping memory-mapped registers&quot;); for pair in MMIO { memory_set.push(MapArea::new( (*pair).0.into(), ((*pair).0 + (*pair).1).into(), MapType::Identical, MapPermission::R | MapPermission::W, ), None); } memory_set } } ​ 对这对区间进行恒等映射，并开放了读写权限。 pub struct VirtIOBlock(Mutex&lt;VirtIOBlk&lt;'static'&gt;&gt;); impl VirtIOBlock { pub fn new() -&gt; Self { Self(Mutex::new(VirtIOBlk::new( unsafe { &amp;mut *(VIRTIO0 as *mut VirtIOHeader) } ).unwrap())) } } impl BlockDevice for VirtIOBlock { fn read_block(&amp;self, block_id: usize, buf: &amp;mut [u8]) { self.0.lock().read_block(block_id, buf).expect(&quot;Error when reading VirtIOBlk&quot;); } fn write_block(&amp;self, block_id: usize, buf: &amp;[u8]) { self.0.lock().write_block(block_id, buf).expect(&quot;Error when writing VirtIOBlk&quot;); } } ​ 我们在VirtIOBlk进行进一步的封装，加上一层互斥锁变为我们的VirtIOBlk，传入的VirtIOHeader起始就是以mmio进行访问的VirtIO需要的一组寄存器。之后为VirtIOBlk实现BlockDevice的Trait即可。VirtIO设备需要占用部分内存，放置一个数据结构VirtQueue，CPU可以向VirtIO发送请求，也可以从队列中获取结构。 ​ qemu为我们暴露的接口如下： extern &quot;C&quot; { fn virtio_dma_alloc(pages: usize) -&gt; PhysAddr; fn virtio_dma_dealloc(paddr: usize, pages: usize) -&gt; i32; fn virtio_phys_to_virt(paddr: PhysAddr) -&gt; VirtAddr; fn virtio_virt_to_phys(vaddr: VirtAddr) -&gt; PhysAddr; } ​ 我们使用rCore中的相关方法进行实现。 lazy_static! { static ref QUEUE_FRAMES: Mutex&lt;Vec&lt;FrameTracker&gt;&gt; = Mutex::new(Vec::new()); } #[no_mangle] pub extern &quot;C&quot; fn virtio_dma_alloc(pages: usize) -&gt; PhysAddr { let mut ppn_base = PhysPageNum(0); for i in 0..pages { let frame = frame_alloc().unwrap(); if i == 0 { ppn_base = frame.ppn; } assert_eq!(frame.ppn.0, ppn_base.0 + i); QUEUE_FRAMES.lock().push(frame); } ppn_base.into() } ​ 这里连续地申请了pages个页帧，之后将这些页帧加入到了队列中，之后返回申请到的物理地址。之后几个方法也用rCore中的方法实现。 内核中的Inode ​ 内核中需要控制Inode的访问方式，这里进行进一步的封装。 pub struct OSInode { readable: bool, writable: bool, inner: Mutex&lt;OSInodeInner&gt;, } pub struct OSInodeInner { offset: usize, inode: Arc&lt;Inode&gt;, } impl OSInode { pub fn new( readable: bool, writable: bool, inode: Arc&lt;Inode&gt;, ) -&gt; Self { readable, writable, inner: Mutex::new(OSInodeInner { offset:0, inode, }) } } ​ offset和我们在C中的文件语义类似，不过这个offset这里不是一个每次打开就有一个。 文件描述符表 ​ 文件打开和关闭都是以进程为单位在进行，我们这里需要在进程控制块中加入文件描述符表 pub struct TaskControlBlock { //... pub fd_table: Vec&lt;Option&lt;Arc&lt;dyn File + Send + Sync&gt;&gt;&gt;, } ​ 这里的dyn表示实现了File + Send + Sync三个Trait的类型，我们调用其read等方法时，可以在运行时动态确定调用的函数。我们要添加入的文件类型，OSInode、Stdout、Stdin都实现了这三个方法。 为内核实现文件访问机制 ​ 应用程序访问文件前需要对文件系统进行初始化，可以通过应用程序发出系统调用如mount实现，也可以操作系统直接实现。我们这里只需要在初始化时进行实现即可。 lazy_static! { pub static ref ROOT_INODE: Arc&lt;Inode&gt; = { let efs = EasyFileSystem::open(BLOCK_DEVICE.clone()); Arc::new(EasyFileSystem::root_inode(&amp;efs)) }; } ​ 之后我们实现四个系统调用。 pub fn open_file(name: &amp;str, flags: OpenFlags) -&gt; Option&lt;Arc&lt;OSInode&gt;&gt; { let (readable, writable) = flags.read_write(); if flags.contains(OpenFlags::CREATE) { if let Some(inode) = ROOT_INODE.find(name) { // clear size inode.clear(); Some(Arc::new(OSInode::new( readable, writable, inode, ))) } else { // create file ROOT_INODE.create(name) .map(|inode| { Arc::new(OSInode::new( readable, writable, inode, )) }) } } else { ROOT_INODE.find(name) .map(|inode| { if flags.contains(OpenFlags::TRUNC) { inode.clear(); } Arc::new(OSInode::new( readable, writable, inode )) }) } } ​ 这里通过创建时的参数，判断是否要创建一个文件，是否要清空原本的文件。有了这个函数我们就可以很方便的实现sys_open。 pub fn sys_open(path: *const u8, flags: u32) -&gt; isize { let task = current_task().unwrap(); let token = current_user_token(); let path = translated_str(token, path); if let Some(inode) = open_file( path.as_str(), OpenFlags::from_bits(flags).unwrap() ) { let mut inner = task.acquire_inner_lock(); let fd = inner.alloc_fd(); inner.fd_table[fd] = Some(inode); fd as isize } else { -1 } } ​ ","link":"https://porterlu.github.io/post/rcore-di-6-zhang/"},{"title":"rCore第5章练习","content":"实践作业1 进程创建 大家一定好奇过为啥进程创建要用fork + execve这么一个奇怪的系统调用，就不能直接搞一个新进程吗？思而不学则殆，我们就来试一试！这章的编程练习请大家实现一个完全 DIY 的系统调用 spawn，用以创建一个新进程。 spawn 系统调用定义( 标准spawn看这里 )： fn sys_spawn(path: *const u8) -&gt; isize syscall ID: 400 功能：新建子进程，使其执行目标程序。 说明：成功返回子进程id，否则返回 -1。 可能的错误： 无效的文件名。进程池满/内存不足等资源错误。 TIPS：虽然测例很简单，但提醒读者 spawn 不必 像 fork 一样复制父进程的地址空间。 实验练习2 实践作业 stride 调度算法 ch3 中我们实现的调度算法十分简单。现在我们要为我们的 os 实现一种带优先级的调度算法：stride 调度算法。 算法描述如下: 为每个进程设置一个当前 stride，表示该进程当前已经运行的“长度”。另外设置其对应的 pass 值（只与进程的优先权有关系），表示对应进程在调度后，stride 需要进行的累加值。 每次需要调度时，从当前 runnable 态的进程中选择 stride 最小的进程调度。对于获得调度的进程 P，将对应的 stride 加上其对应的步长 pass。 一个时间片后，回到上一步骤，重新调度当前 stride 最小的进程。 可以证明，如果令 P.pass = BigStride / P.priority 其中 P.priority 表示进程的优先权（大于 1），而 BigStride 表示一个预先定义的大常数，则该调度方案为每个进程分配的时间将与其优先级成正比。证明过程我们在这里略去，有兴趣的同学可以在网上查找相关资料。 其他实验细节： stride 调度要求进程优先级 ≥2，所以设定进程优先级 ≤1 会导致错误。 进程初始 stride 设置为 0 即可。 进程初始优先级设置为 16。 为了实现该调度算法，内核还要增加 set_prio 系统调用 // syscall ID：140 // 设置当前进程优先级为 prio // 参数：prio 进程优先级，要求 prio &gt;= 2 // 返回值：如果输入合法则返回 prio，否则返回 -1 fn sys_set_priority(prio: isize) -&gt; isize; tips: 可以使用优先级队列比较方便的实现 stride 算法，但是我们的实验不考察效率，所以手写一个简单粗暴的也完全没问题。 我的实现 简述 spawn ​ 这里的spawn需要我们生成一个进程，并且将这个进程加入到队列中, 和fork不同的点在于它不会赋值父进程的空间，和exec不同的点在于它不会替换进程的执行流。这里实现的过程为在TaskControlBlock中新加入一个函数，它会直接根据path由elf生成一个地址空间，同时将这个进程加入到父进程下，在生成自己的trap上下文后，这个block会被加入到调度队列中。 stride调度 ​ 这里需要为每一个control block添加两个新的成员stride和priority，前一个初始值为0，后一个为16。这里我们将Manager中的队列替换为BinaryHeap, 还要为TaskControlBlock实现Eq，PartialEq, Ord和PartialOrd，我们通过比较stride获取到关系。修改外fetch等方法后，每一次获取的都是stride最小的任务。最后我们在run_task中每一调度一个新的任务时，更新我们的stride值，注意这里不要把BIG_STRIDE设置太大，否则会溢出。 具体代码 impl TaskControlBlock { pub fn spawn(self: &amp;Arc&lt;Self&gt;, elf_data: &amp;[u8]) -&gt; Arc&lt;Self&gt; { let mut parent_inner = self.inner_exclusive_access(); let (memory_set, user_sp, entry_point) = MemorySet::from_elf(elf_data); let trap_cx_ppn = memory_set .translate(VirtAddr::from(TRAP_CONTEXT).into()) .unwrap() .ppn(); let pid_handle = pid_alloc(); let kernel_stack = KernelStack::new(&amp;pid_handle); let kernel_stack_top = kernel_stack.get_top(); let task_control_block = Arc::new(TaskControlBlock { pid: pid_handle, kernel_stack, inner: unsafe { UPSafeCell::new(TaskControlBlockInner { trap_cx_ppn, base_size: user_sp, task_cx: TaskContext::goto_trap_return(kernel_stack_top), task_status: TaskStatus::Ready, memory_set, parent: Some(Arc::downgrade(self)), children: Vec::new(), exit_code: 0, stride: 0, priority: 16, }) }, }); let trap_cx = task_control_block.inner_exclusive_access().get_trap_cx(); *trap_cx = TrapContext::app_init_context( entry_point, user_sp, KERNEL_SPACE.exclusive_access().token(), kernel_stack_top, trap_handler as usize, ); parent_inner.children.push(task_control_block.clone()); task_control_block } } ​ 这里依次解析elf文件，获取了pid和内核栈，创建控制块，设置trap上下文，最后返回这个控制块，这个控制块会被加入到队列中。 impl PartialEq for TaskControlBlock { fn eq(&amp;self, other: &amp;Self) -&gt; bool { let self_control_block = self.inner_exclusive_access(); let other_control_block = other.inner_exclusive_access(); let self_stride = self_control_block.stride; let other_stride = other_control_block.stride; drop(self_control_block); drop(other_control_block); self_stride == other_stride } } impl Eq for TaskControlBlock {} impl Ord for TaskControlBlock { fn cmp(&amp;self, other: &amp;Self) -&gt; Ordering { let self_control_block = self.inner_exclusive_access(); let other_control_block = other.inner_exclusive_access(); let self_stride = self_control_block.stride; let other_stride = other_control_block.stride; drop(self_control_block); drop(other_control_block); other_stride.cmp(&amp;self_stride) } } impl PartialOrd for TaskControlBlock { fn partial_cmp(&amp;self, other: &amp;Self) -&gt; Option&lt;Ordering&gt; { Some(self.cmp(other)) } } ​ 这里会有一些繁琐，实际上可以直接将drop任务交给编译器。需要注意的是由于是大根堆，所以如果要最小值排在最前面，我们需要将比较对象的位置互换。 impl TaskManager { pub fn new() -&gt; Self { Self { ready_queue: BinaryHeap::new(), } } pub fn add(&amp;mut self, task: Arc&lt;TaskControlBlock&gt;) { self.ready_queue.push(task); } pub fn fetch(&amp;mut self) -&gt; Option&lt;Arc&lt;TaskControlBlock&gt;&gt; { self.ready_queue.pop() } } ​ 同时可以看到，我们将数据结构替换了堆。 ","link":"https://porterlu.github.io/post/rcore-di-5-zhang-lian-xi/"},{"title":"Rust链表","content":"初始实现 ​ 我们实现的目标是要将整个链表，除了指向链表的指针，其他部分全部放在堆上。我们使用Enum来实现链表结构。 pub struct List { head: Link, } enum Link { Empty, More(Box&lt;Node&gt;), } struct Node { elem: i32, next: Link, } ​ 这样我们只需要申请一个List，之后所有的内容都可以放在堆上，每个Link都可以是空或者一个具体节点的指针。 ​ 接下需要为List定义一系列的操作： impl List { pub fn new() -&gt; Self { List { head: Link::Empty } } pub fn push(&amp;mut self, elem: i32) { let new_node = Box::new(Node { elem: elem, next: std::mem::replace(&amp;mut self.head, Link::Empty), }); self.head = Link::More(new_node); } pub fn pop(&amp;mut self) -&gt; Option&lt;i32&gt; { match std::mem::replace(&amp;mut self, Link::Empty) { Link::Empty =&gt; { None } Link::More(node) =&gt; { self.head = node.next; Some(node.elem) } } } } ​ 这里实现了初始化，压入，弹出三个操作： 对于初始化操作，直接申请一个Link::Empty，并将其所有权传出。 对于压入操作，需要跟换头节点，并将当前节点设置为下一个节点。这里的replace操作，用将self.head中的值替换为Empty，并将其原本的值取出。这样只需将新申请的节点作为头节点即可。 pop操作用于将一个头节点取出 ​ 最后实现Drop用将申请的空间释放，本来编译器会自己实现Drop方法，不过由于这里作为一个链表，如果直接递归调用，会有爆栈的可能。 impl Drop for List { fn drop(&amp;mut self) { let mut cur_link = std::mem::replace(&amp;mut self.HEAD, Link::Empty); while let Link::More(mut boxed_node) = cur_link { cur_link = std::mem::replace(&amp;mut boxed_node.next, Link::Empty); } } } ​ 我们使用顺序遍历方式替代递归解决爆栈的问题。 使用Option pub struct List { head: Link, } type Link = Option&lt;Box&lt;Node&gt;&gt;; struct Node { elem: i32, next: Link, } ​ 这里定义了类型Link，Option&lt;Box&lt;Node&gt;&gt;和之前的结构实现了一样的结构。 impl List { pub fn new() -&gt; Self { List { head: None } } pub fn push(&amp;mut self, elem: i32) { let new_node = Box::new(Node { elem: elem, next: self.head.take(), }); self.head = Some(new_node); } pub fn pop(&amp;mut self) -&gt; Option&lt;i32&gt; { self.head.take().map(|node| { self.head = node.next; node.elem }) } impl Drop for List { fn drop(&amp;mut self) { let mut cur_link = self.head.take(); while let Some(mut boxed_node) = cur_link { cur_link = boxed_node.next.take(); } } } ​ 这里有了Option之后我们不用再使用replace，而是使用take，take可以直接将Option中的内容取出并替换为None。 ​ 同时对于 match option { None =&gt; None, Some(x) =&gt; Some(y) } ​ 可以直接使用map替代，可以将Some替换为Some(y)。 ​ 最后再实现泛型，支持更多的类型而不仅仅是i32。以pop为例子： impl&lt;T&gt; List&lt;T&gt; { pub fn pop(&amp;mut self) -&gt; Option&lt;T&gt; { self.head = node.next; node.elem } } ","link":"https://porterlu.github.io/post/rust-lian-biao/"},{"title":"rCore第四章练习","content":"mmap 和 munmap 匿名映射 mmap 在 Linux 中主要用于在内存中映射文件，本次实验简化它的功能，仅用于申请内存。 请实现 mmap 和 munmap 系统调用，mmap 定义如下： fn sys_mmap(start: usize, len: usize, prot: usize) -&gt; isize syscall ID：222 申请长度为 len 字节的物理内存（不要求实际物理内存位置，可以随便找一块），将其映射到 start 开始的虚存，内存页属性为 prot 参数： start 需要映射的虚存起始地址，要求按页对齐len 映射字节长度，可以为 0。prot：第 0 位表示是否可读，第 1 位表示是否可写，第 2 位表示是否可执行。其他位无效且必须为 0 返回值：执行成功则返回 0，错误返回 -1 说明： 为了简单，目标虚存区间要求按页对齐，len 可直接按页向上取整，不考虑分配失败时的页回收。 可能的错误： start 没有按页大小对齐prot &amp; !0x7 != 0 (prot 其余位必须为0) prot &amp; 0x7 = 0 (这样的内存无意义)[start, start + len) 中存在已经被映射的页物理内存不足 munmap 定义如下： fn sys_munmap(start: usize, len: usize) -&gt; isize syscall ID：215 取消到 [start,start + len) 虚存的映射 参数和返回值请参考 mmap 说明： 为了简单，参数错误时不考虑内存的恢复和回收。 可能的错误： [start, start + len) 中存在未被映射的虚存。 TIPS：注意 prot 参数的语义，它与内核定义的 MapPermission 有明显不同！ 我的实现 简述 ​ 这里需要实现两个系统调用mmap和munmap，其中munmap传入的参数不用考虑页表访问限制，直接将相关的映射删除即可。这里我在os的syscall下添加了memory模块，模块中会直接调用task模块中的接口函数，接口中使用TaskManager的方法，这些方法主要的原理是通过调用当前执行的控制块中的memory_set进行映射和拆除映射。映射的实现较为简单，因为已经检查了这个连续区间内没有已经被映射的虚拟页号，直接调用memory_set中的insert_framed_area即可。但是拆除映射会比较复杂，因为拆除映射需要考虑要移除几个area，是否需要将原本的area分为两个等问题。这里我记录最左area和最右area，将其中的所有区域释放，如果最边上的area 并不是[start, start+len)中的区域，则再insert刚才删除过多的部分。 具体代码 impl TaskManager { //... fn map(&amp;self, start: usize, len: usize, prot: usize) -&gt; isize { let mut inner = self.inner.exclusive_access(); let cur = inner.current_task; let mut map_perm = MapPermission::U; if (prot &amp; 1) != 0 { map_perm |= MapPermission::R; } if (prot &amp; 2) != 0 { map_perm |= MapPermission::W; } if (prot &amp; 4) != 0 { map_perm |= MapPermission::X; } inner.tasks[cur].memory_set.insert_framed_area(start.into(), (start+len).into(), map_perm); 0 } } ​ 这里直接根据prot生成MapPermission，调用insert_framed_area方法就完成了map，当然之前需要进行是否已经map过的检查。 impl for memory_set { //... pub fn unmap(&amp;mut self, start: usize, len: usize) -&gt; isize { let length = (len + PAGE_SIZE - 1) / PAGE_SIZE; if length == 0 { return 0; } let start_vaddr: VirtAddr = start.into(); let end_vaddr: VirtAddr = (start+len).into(); let mut left_area_index = 0; let mut right_area_index = 0; //获取左右边界 for i in 0..self.areas.len() { if self.areas[i].vpn_range.get_start() &lt;= end_vaddr.into() &amp;&amp; self.areas[i].vpn_range.get_end() &gt; end_vaddr.into() { right_area_index = i; } if self.areas[i].vpn_range.get_start() &lt;= start_vaddr.into() &amp;&amp; self.areas[i].vpn_range.get_end() &gt; start_vaddr.into() { left_area_index = i; } } let del_region_left = self.areas[left_area_index].vpn_range.get_start(); let del_region_right = self.areas[right_area_index].vpn_range.get_end(); let del_region_left_permission = self.areas[left_area_index].map_perm; let del_region_right_permission = self.areas[right_area_index].map_perm; //删除区域判断 for j in 0..self.areas.len() { if (self.areas[j].vpn_range.get_start() &lt;= del_region_left &amp;&amp; del_region_left &lt; self.areas[j].vpn_range.get_end()) || (self.areas[j].vpn_range.get_start() &lt;= del_region_right &amp;&amp; del_region_right &lt; self.areas[j].vpn_range.get_end()) || (self.areas[j].vpn_range.get_start() &gt;= del_region_left &amp;&amp; self.areas[j].vpn_range.get_end() &lt;= del_region_right) { self.areas[j].unmap(&amp;mut self.page_table) } } //多删部分补回 if del_region_left &lt; start.into() { self.insert_framed_area(del_region_left.into(), start_vaddr.into(), del_region_left_permission); } if del_region_right &gt; (start+len).into() { self.insert_framed_area(end_vaddr.into(), del_region_right.into(), del_region_right_permission); } return 0; } } ​ 这里要考虑unmap的区域是否会包括多个area，是否会删除一个完整的一个area的问题，我的方法是考虑最左能到哪个area, 最右到哪个area，删除之间所有area，最后判断左右的area是否应该完全取出，否的话则进行补回。 ","link":"https://porterlu.github.io/post/rcore-di-si-zhang-lian-xi/"},{"title":"rCore第三章练习","content":"获取任务信息 ch3 中，我们的系统已经能够支持多个任务分时轮流运行，我们希望引入一个新的系统调用 sys_task_info 以获取任务的信息，定义如下： fn sys_task_info(id: usize, ts: *mut TaskInfo) -&gt; isize syscall ID: 410 根据任务 ID 查询任务信息，任务信息包括任务 ID、任务控制块相关信息（任务状态）、任务使用的系统调用及调用次数、任务总运行时长。 struct TaskInfo { id: usize, status: TaskStatus, call: [SyscallInfo; MAX_SYSCALL_NUM], time: usize } 系统调用信息采用数组形式对每个系统调用的次数进行统计，相关结构定义如下： struct SyscallInfo { id: usize, times: usize } 参数： id: 待查询任务idts: 待查询任务信息 返回值：执行成功返回0，错误返回-1 说明： 相关结构已在框架中给出，只需添加逻辑实现功能需求即可。 提示： 大胆修改已有框架！除了配置文件，你几乎可以随意修改已有框架的内容。程序运行时间可以通过调用 get_time() 获取。系统调用次数可以考虑在进入内核态系统调用异常处理函数之后，进入具体系统调用函数之前维护。阅读TaskManager的实现，思考如何维护内核控制块信息（可以在控制块可变部分加入其他需要的信息） 我的实现 简述 ​ 在user目录中实现一个sys_task_info，系统调用返回后，传入的*mut TaskInfo 中将获得要查询的app的信息。而在os端，要从创建这个app的控制块开始就维护这个信息，我在task.rs中创建TaskInfo的结构体，并在TASK_MANAGER初始化时也对每个app的TaskInfo进行初始化。同时这里控制块处理再添加一个TaskInfo外，还要加入一个last_time，用于记录这个任务被调度的时间。于是suspend,exit,run等行为会修改控制块中的status和time，time可以每次加上get_time() - last_time。 ​ 而syscall的信息在syscall_handler的match语句前调用record(syscall_id)函数，对current的app的syscall_id调用加1。 代码分析 pub fn syscall(syscall_id: usize, args: [usize; 3]) -&gt; isize { set_syscall_record(syscall_id); match syscall_id { SYSCALL_WRITE =&gt; sys_write(args[0], args[1] as *const u8, args[2]), SYSCALL_EXIT =&gt; sys_exit(args[0] as i32), SYSCALL_YIELD =&gt; sys_yield(), SYSCALL_GET_TIME =&gt; sys_get_time(), SYSCALL_TASK_INFO =&gt; sys_task_info(args[0], args[1] as *mut TaskInfo), _ =&gt; panic!(&quot;Unsupported syscall_id: {}&quot;, syscall_id), } } ​ 这里第二条语句对于当前这个系统调用进行了记录，还有倒数第4行添加了sys_task_info的系统调用。 fn set_syscall_record(&amp;self, syscall_id: usize) { let mut inner = self.inner.exclusive_access(); let current = inner.current_task; inner.tasks[current].task_info.call[syscall_id].id = syscall_id; inner.tasks[current].task_info.call[syscall_id].times += 1; drop(inner); } fn get_task_info(&amp;self, id: usize, task_info: *mut TaskInfo) -&gt; isize { if id &lt; MAX_APP_NUM { let inner = self.inner.exclusive_access(); let current = inner.current_task; unsafe { (*task_info).id = id; (*task_info).status = inner.tasks[current].task_status; (*task_info).time = inner.tasks[current].task_info.time; (*task_info).call = inner.tasks[current].task_info.call; } drop(inner); 0 } else { -1 } } ​ 这里可以看到对应的实现，set_syscall_record将对应的syscall_id中的id赋值，表示这个系统调用被执行过。get_task_info将kernel记录的信息，一项一项地拷贝到了app传入的结构体中。 ","link":"https://porterlu.github.io/post/rcore-di-san-zhang-lian-xi/"},{"title":"rCore第二章练习","content":"sys_write 安全检查 ch2 中，我们实现了第一个系统调用 sys_write，这使得我们可以在用户态输出信息。但是 os 在提供服务的同时，还有保护 os 本身以及其他用户程序不受错误或者恶意程序破坏的功能。 由于还没有实现虚拟内存，我们可以在用户程序中指定一个属于其他程序字符串，并将它输出，这显然是不合理的，因此我们要对 sys_write 做检查： sys_write 仅能输出位于程序本身内存空间内的数据，否则报错。 我的实现 ​ 由于所有的程序都将加载到0x80400000运行。我们可以再sys_write中加入如下的代码 pub fn sys_write(fd: usize, buf: *const u8, len: usize) -&gt; isize { match fd { FD_STDOUT =&gt; { let slice = unsafe { core::slice::from_raw_parts(buf, len) } let str = core::str::from_utf8(slice).unwrap(); if check_bound(str.as_ptr() as usize) { print!(&quot;{}&quot;, str); len as isize } else { panic!(&quot;Only support for own space&quot;) } } //... } } ​ 这里对于每次的输出都检查str地址是否在加载运行程序的地址范围内。之后为APP_MANAGER实现check_bound方法, 要求 addr &gt;= APP_BASE_ADDRESS &amp;&amp; addr &lt; APP_BASE_ADDRESS + APP_SIZE_LIMIT ","link":"https://porterlu.github.io/post/rcore-di-er-zhang-shi-yan-lian-xi/"},{"title":"rCore第五章","content":"在这一章中将进一步解释进程的动态的概念，提供创建、销毁、等待、信息等系统调用，并开发一个shell作为user_app之一。我们会重新设计TaskControlBlock的结构，来满足我们对于进程创建，销毁的一系列需求。 进程ID ​ 进程ID的分配采用了和前面一样的栈式分配，不断向后移动current标志，表示最大已分配id号。如果要进行回收，则直接回收到recycled中，同时如果recycled中如果存在已经回收的id，则不会向后移动current。最后在这个模块还要说明如何获取kernel stack，和之前保持一致，不过app_id换成了pid。 进程控制块及其操作 ​ 对于进程控制块，分成初始化后不变和可变的两部分。结构如下： pub struct TaskControlBlock { pub pid: PidHandle, pub kernel_stack: KernelStack, inner UPSafeCell&lt;TaskControlBlockInner&gt;, } pub struct TaskControlBlockInner { pub trap_cx_ppn: PhysPageNum, pub base_size: usize, pub task_cx: TaskContext, pub task_status: TaskStatus, pub memory_set: MemorySet, pub parent: Option&lt;Weak&lt;TaskControlBlock&gt;&gt;, pub children: Vec&lt;Arc&lt;TaskControlBlock&gt;&gt;, pub exit_code: i32, } ​ new方法增加了pid的申请。同时增加了fork和exec方法，用于拷贝和替换上下文，达到再创建一个进程和执行一个程序的效果。 pub fn exec(&amp;self, elf_data: &amp;[u8]) { let (memory_set, user_sp, entry_point) = MemorySet::from_elf(elf_data); let trap_cx_ppn = memory_set .translate(VirtAddr::from(TRAP_CONTEXT).into()) .unwrap() .ppn(); let mut inner = self.inner_exclusive_access(); inner.memory_set = memory_set; inner.trap_cx_ppn = trap_cx_ppn; inner.base_size = user_sp; let trap_cx = inner.get_trap_cx(); *trap_cx = TrapContext::app_init_context( entry_point, user_sp, KERNEL_SPACE.exclusive_access().token(), self.kernel_stack.get_top(), trap_handler as usize, ); } ​ 这里直接将trap上下文和memory_set即映射空间进行替换，这样返回时会直接restore我们刚刚放入的上下文，达到执行一个程序的目的。 pub fn fork(self: &amp;Arc&lt;Self&gt;) -&gt; Arc&lt;Self&gt; { let mut parrent_inner = self.inner_exclusive_access(); let memory_set = MemorySet::from_existed_user(&amp;parent_inner.memory_set); let trap_cx_ppn = memory_set .translate(VirtAddr::from(TRAP_CONTEXT).into()) .unwrap() .ppn() let pid_handle = pid_alloc(); let kernel_stack = KernelStack::new(&amp;pid_handle); let kernel_stack_top = kernel_stack.get_top(); let task_control_block = Arc::new(TaskControlBlock{ pid: pid_handle, kernel_stack, inner: unsafe { UPSafeCell::new(TaskControlBlockInner { trap_cx_ppn, base_size: parent_inner.base_size, task_cx: TaskContext::goto_trap_return(kernel_stack_top), task_status: TaskStatus::Ready, memory_set, parent: Some(Arc::downgrade(self)), children: Vec::new(), exit_code: 0, }) }, }); parent_inner.children.push(task_control_block.clone()); let trap_cx = task_control_block.inner_exclusive_access().get_trap_cx(); trap_cx.kernel_sp = kernel_stack_top; task_control_block } ​ 这里调用了from_existed_user函数对于父进程的地址空间进行拷贝，同时获取新获取空间的root_ppn分配的物理帧，非陪内核占空间，重新申请一个task_control_block。同时设置父子进程关系，更换fork生成的trap上下文中的内核栈指针位置，这个指针的位置应该由新生成的pid决定。 数据结构关系 ​ 如上图所示，这是进程调度时的框架图，处理结构体内部只会维护一个idle_task_cx和current。idle_task_cx会不断循环让后只是调度下一个程序，schedule函数本身运行在操作系统的启动栈，切换的进程各自运行在自己的内核栈，进行切换时的调度对于两个进程的trap执行流就是不可知的。处理器结构体的任务就是维护当前正在运行的任务， ​ TaskManager内部就是一个VecDeque一个双向队列，可以从末尾加入一个进程控制块，从队首去取出进程控制块。 运行过程 ​ 这里起始于右上角的find_app_data_by_name，这个函数会根据app的名字来获取app在整个存储队列中的位置，TaskControlBlock的new方法中会调用from_elf解析数据，生成控制块后将这个结构压入Manager的队列。processor中会调度schedule启动这个进程控制块，执行initproc后，会执行fork系统调用，根据from_existed_user拷贝一份地址映射，并生成新的进程控制块，这个控制块也会被加入队列。initproc返回后会执行yield，这是idle会被用于调度下一个进程，user_shell会被调度执行，get_char会不断获取字符，如果已经键入一个命令，会先进行一次fork，根据结果，主进程进行wait，子进程执行结束后将状态设置为zombie，并设置exit_code。wait系统调用在检查到子进程执行完毕后，会结束loop。 ​ ","link":"https://porterlu.github.io/post/rcore-di-wu-zhang/"},{"title":"Rust包管理","content":"​ rust使用module的概念来管理软件。如一个mod最直接的形式，调用者和mod在同一个文件。 mod grandfather { pub mod father { pub fn son() { println!(&quot;son&quot;); } } } fn main () { grandfather::father::son(); } ​ 每一级都需要pub说明，前一级的pub不能覆盖子一层也为pub。为了进一步的简化，可以使用use进行重导出 fn main() { use grandfather::father::son; son(); } ​ 当然模块一般不和调用者一个文件，可以直接写成一个独立的文件如： //src/test.rs //src/main.rs mod test; fn main() { test::grandfather::father::son(); } ​ 如果这里工程本身就是一个crate，我们可以使用pub use进行向外导出。 mod test; pub use test::grandfather::father::son; fn main() { son(); } 或者直接 pub mod test; ​ 对于使用标准库和外部库的情况。我们可以直接用 use rand::Rng; //或者 use crate::rand::Rang; ","link":"https://porterlu.github.io/post/rust-bao-guan-li/"},{"title":"rCore第四章","content":"在内存中支持动态分配 ​ Rust中已经为我们提供了接口，alloc库需要我们为其提供一个全局动态内存分配器，它可以通过该分配器管理堆空间。具体地说，我们的内存分配器需要实现GlobalAlloc Trait, 它有两个接口。 pub unsafe fn alloc(&amp;self, layout: Layout) -&gt; *mut u8; pub unsafe fn dealloc(&amp;self, ptr: *mut u8, layout: Layout); ​ 这里使用现成的buddy分配器库，我们只需要为其提供可供分配的空间即可。这里bss段分配这个空间 static mut HEAP_SPACE: [u8; KERNEL_HEAP_SIZE] = [0; KERNEL_HEAP_SIZE]; ​ 然后说明我们的堆空间分配器为这个引入的buddy分配器 #[global_allocator] static HEAP_ALLOCATOR: LockedHeap = LockedHeap::empty(); ​ 初始化时将HEAP_SPACE分配给分配器，之后我们就可以使用Vec这种使用堆空间的数据结构了。 页帧分配器 ​ 这里实现了一个栈式分配器，结构如下 pub struct StackFrameAllocator { current: usize, end: usize, recycled: Vec&lt;usize&gt;, } pub fn init_frame_allocator() { extern &quot;C&quot; { fn ekernel(); } FRAME_ALLOCATOR.exclusive_access().init( PhysAddr::from(ekernel as usize).ceil(), PhysAddr::from(MEMORY_END).floor(), ) } ​ 这个栈分配器将从内核结束的地址到内存最后作为分配的空间，使用[current,end)[current, end)[current,end)记录当前仍然未分配的空间，使用recyclede用于回收已经分配的页。分配时是以ppn作为单位。 impl FrameAllocator for StackFrameAllocator { fn alloc(&amp;mut self) -&gt; Option&lt;PhysPageNum&gt; { if let Some(ppn) = self.recycled.pop() { Some(ppn.into()) } else if self.current == self.end { None } else { self.current += 1; Some((self.current - 1).into()) } } fn dealloc(&amp;mut self, ppn: PhysPageNum) { let ppn = ppn.0; if ppn &gt;= self.current || self.recycled.iter().any(|&amp;v| v == ppn) { panic!(&quot;Frame ppn={:#x} has not been allocated!&quot;, ppn); } self.recycled.push(ppn); } } ​ 每次分配首先判断recycled队列中是否已经有已经回收的页，有的话可以直接使用；否则，当帧空间没有分配，就可以让current往后面移动一个单位，将分配的空间地址转化为ppn进行返回。回收就是回收到recycled中。 ​ 最后使用一个FrameTracker对于得到的物理页帧进行包裹，将一个物理页帧的生命周期绑定到FrameTracker上。 impl FrameTracker { pub fn new(ppn: PhysPageNum) -&gt; Self { let bytes_array = ppn.get_bytes_array(); for i in bytes_array { *i = 0; } Self { ppn } } } impl Drop for FrameTracker { fn drop(&amp;mut self) { frame_alloc(self.ppn); } } 页表管理 ​ 我们的页表中需要存储页目录的ppn和所有用于映射的页表的FrameTracker结构，初始化时只用申请一个页目录结构并将其加入到页帧管理的vec中。之后要为PageTable实现页表映射的方法， impl VirtPageNum { pub fn indexes(&amp;self) -&gt; [usize; 3] { let mut vpn = self.0; let mut idx = [0usize; 3]; for i in (0..3).rev() { idx[i] = vpn &amp; 511; vpn &gt;&gt;= 9; } idx } } ​ 这里由于一次地址翻译需要将虚拟地址分为3份，所以这里将VirtPageNum转化为了一个[usize; 3]类型。同时为页表创建两个映射相关的函数。 impl PageTable { fn find_pte_create(&amp;mut self, vpn: VirtPageNum) -&gt; Option&lt;&amp;mut PageTableEntry&gt; { let idxs = vpn.indexes(); let mut ppn = self.root_ppn; let mut result: Option&lt;&amp;mut PageTableEntry&gt; = None; for i in 0..3 { let pte = &amp;mut ppn.get_pte_array()[idxs[i]]; if i == 2 { result = Some(pte); break; } if !pte.is_valid() { let frame = frame_alloc().unwrap(); *pte = PageTableEntry::new(frame.ppn, PTEFlags::V); self.frames.push(frame); } ppn = pte.ppn(); } result } fn find_pte(&amp;self, vpn: VirtPageNum) -&gt; Option&lt;&amp;mut PageTableEntry&gt; { let idxs = vpn.indexes(); let mut ppn == self.root_ppn(); let mut result: Option&lt;&amp;mut PageTableEntry&gt; = None; for i in 0..3 { let pte = &amp;mut ppn.get_pte_array()[idxs[i]]; if i == 2 { result = Some(pte); break; } if !pte.is_valid() { return None; } ppn = pte.ppn(); } result } } ​ 这两个函数find_pte_create和find_pte分别用于映射虚拟地址并创建映射页表和映射虚拟地址。在find_pte_create中可以发现这样一段代码： if !pte.is_valid() { let frame = frame_alloc().unwrap(); *pte = PageTableEntry::new(frame.ppn, PTEFlags::V); self.frames.push(frame); } ​ 如果发现页表项中并没有映射的页帧，则需要申请一个页帧作为页表，同时将这个页表ppn加入到页帧管理器中。有了者两个结构就很好实现map和unmap了。 impl PageTable { pub fn map(&amp;mut self, vpn: VirtPageNum, ppn: PhysPageNum, flags: PTEFlags) { let pte = self.find_pte_create(ppn).unwrap(); assert!(!pte.is_valid(), &quot;vpn {:?} is mapped before mapping&quot;, vpn); *pte = PageTableEntry::new(ppn, flags | PTEFlags::V); } pub fn unmap(&amp;mut self, vpn: VirtPageNum) { let pte = self.find_pte(vpn).unwrap(); assert!(pte.is_valid(), &quot;vpn {:?} is in valid before unmapping&quot;, vpn); *pte = PageTableEntry::empty(); } } ​ map中创建一个页表项，之后将得到的页表项填入一个物理页号。如果unmap的话，直接将对应的表项置空。 impl PageTable { pub fn from_token(satp: usize) -&gt; Self { Self { root_ppn: PhysPageNum::from(satp &amp; ((1usize &lt;&lt; 44) - 1)), frames: Vec::new(), } } pub fn translate(&amp;self, vpn: VirtPageNum) -&gt; Option&lt;PageTableEntry&gt; { self.find_pte(vpn).map(|pte| {pte.clone()}) } } ​ 这样PageTable的结构就介绍完毕了。 地址空间 ​ 有了页表的结构，我们需要创建一些数据结构来管理程序的虚拟地址空间。对于一段连续的虚拟地址空间使用MapArea进行描述，结构如下 pub struct MapArea { vpn_range: VPNRange, data_frames: BTreeMap&lt;VirtPageNum, FrameTracker&gt;, map_type: MapType, map_perm: MapPermission, } ​ 这里的VPNRange可以描述一段连续的虚拟地址空间，data_frames用于描述从虚拟地址到物理地址的映射，map_type用于描述是否时恒等映射，map_perm用于描述这个页的访问类型。其中可以使用页表的映射方法，来构造MapArea中的映射，映射又分为恒等映射和非恒等映射，对于恒等映射虚拟地址和物理地址相同。 impl MemoryArea { pub fn map_one(&amp;mut self, page_table: &amp;mut PageTable, vpn:VirtPageNum) { let ppn: PhysPageNum; match self.map_type { MapType::Identical =&gt; { ppn = PhysPageNum(vpn.0); } MapType::Framed =&gt; { let frame = frame_alloc().unwrap(); ppn = frame.ppn; self.data_frames.insert(vpn, frames); } } let pte_flags = PTEFlags::from_bits(self.map_perm.bits).unwrap(); page_table.map(vpn, ppn, pte_flags); } pub fn unmap_one(&amp;mut self, page_table: &amp;mut PageTable, vpn: VirtPageNum) { match self.map_type { MapType::Framed =&gt; { self.data_frames.remove(&amp;vpn); } _ =&gt; {} } page_table.unmap(vpn); } } ​ 对于map_one方法，根据map_type进行不同的映射，Identical表示恒等映射，Frame表示非恒等映射，对于非恒等映射我们需要申请一个页帧，由这个页帧的地址决定映射的ppn，最后将vpn和ppn进行联系。 ​ 接着我们对于这些连续的虚拟地址空间进行进一步的封装，于是有了MemorySet的结构。上图中MapArea中vpn_range并不是一个数组，这里有绘图错误。 pub struct MemorySet { page_table: PageTable, areas: Vec&lt;MapArea&gt;, } impl MemorySet { pub fn new_bare() -&gt; Self { Self { page_table: PageTable::new(), areas: Vec::new(), } } fn push(&amp;mut self, mut map_area: MapArea, data: Option&lt;&amp;[u8]&gt;) { map_area.map(&amp;mut self.page_table); if let Some(data) = data { map_area.copy_data(&amp;mut self.page_table, data); } self.areas.push(map_area); } pub fn insert_framed_area( &amp;mut self, start_va: VirtAddr, end_va: VirtAddr, permission: MapPermission ) { self.push(MapArea::new( start_va, end_va, MapType::Framed, permission, ), None); } //... } impl MemoryArea{ pub fn copy_data(&amp;mut self, page_table:&amp;mut PageTable, data: &amp;[u8]) { assert_eq!(self.map_type, MapType::Framed); let mut start: usize = 0; let mut current_vpn = self.vpn_range.get_start(); let len = data.len(); loop { let src = &amp;data[start..len.min(start + PAGE_SIZE)]; let dst = &amp;mut page_table .translate(current_vpn) .unwrap() .ppn() .get_bytes_array()[..src.len()]; dst.copy_from_slice(src); start += PAGE_SIZE; if start &gt;= len { break; } current_vpn.step(); } } } ​ 首先说明这里的copy_data方法，这个方法用将一个字节数组中的数据拷贝到当前MemoryArea的区域。current_vpn是这个Area的起始地址，将vpn经过页表的翻译就可以获得dst，于是就可以将src转移到dst中，每次将start一个PAGE_SIZE， 以PAGE_SIZE为单位进行拷贝。 ​ 于是MemorySet中的push方法将这个MemoryArea加入到自己的页表映射中，并通过copy_data进行数据拷贝。 内核地址空间和用户地址空间 这里使用rCore-Tutorial中的图片进行说明，内核的代码本身是进行恒等映射的。 pub fn new_kernel() -&gt; Self { let mut memory_set = Self::new_bare(); memory_set.map_trampoline(); // ... println!(&quot;.text &quot;) memory_set.push( MapArea::new( (stext as usize).into(), (etext as usize).into(), MapType: Identical, MapPermission::R | MapPermission::X, ), None, ); //... memory_set.push( MapArea::new( (ekernel as usize).into(), MEMORY_END.into(), MapType::Identical, MapPermission::R | MapPermission::W, ), None, ); for pair in MMIO { memory_set.push( MapArea::new( (*pair).0.into(), ((*pair).0 + (*pair).1).into(), MapType::Identical, MapPermission::R | MapPermission::w, ), None, ) } memory_set } ​ 这里对内核的各个部分进行了映射，同时映射的还有内核的剩下所有内存部分进行恒等映射，MMIO部分同样进行了恒等映射。同时开头对于trampoline这个部分映射在了内核高位地址的最后。 ​ 用户进程的初始化，由于有lazy_static所以会在运行时进行初始化： lazy_static! { pub static ref TASK_MANAGER: TaskManager = { println!(&quot;init TASK_MANAGER&quot;); let num_app = get_num_app(); println!(&quot;num_app = {}&quot;, num_app); let mut tasks: Vec&lt;TaskControlBlock&gt; = Vec::new(); for i in 0..num_app { tasks.push(TaskControlBlock::new(get_app_data(i), i)); } TaskManager { num_app, inner: unsafe { UPSafeCell::new(TaskManagerInner { tasks, current_task: 0, }) }, } }; } 为了生成这样一个结构，我们需要关注TaskControlBlock，这个结构会将elf文件的信息传入，调用new。 impl TaskControlBlock { //... pub fn new(elf_data:&amp;[u8], add_id:usize) -&gt; Self { let (memory_set, user_sp, entry) = MemorySet::from_elf(elf_data); let trap_cx_ppn = memory_set .translate(VirtAddr::from(TRAP_CONTEXT).into()) .unwrap() .ppn(); let task_status = TaskStatus::Ready; let (kernel_stack_bottom, kernel_stack_top) = kernel_stack_position(app_id); KERNEL_SPACE.exclusive_access().insert_framed_area( kernel_stack_bottom.into(), kernel_stack_top.into(), MapPermission::R | MapPermission::W, ); let task_control_block = Self { task_status, task_cx: TaskContext::goto_trap_return(kernel_stack_top), memory_set, trap_cx_ppn, base_size: user_sp, }; let trap_cx = task_control_block.get_trap_cx(); *trap_cx = TrapContext::app_init_context( entry_point, user_sp, KERNEL_SPACE.exclusive_access().token(), kernel_stack_top, trap_handler as usize, ); task_control_block } } ​ 1. 这里会解析用户elf文件格式，得到已经用户的虚拟地址空间结构的memory_set, 用户栈的虚拟地址，用户程序的入口地址。 2. 设置用户空间倒数第二个页作为`Trap`上下文的地址。 2. 设置用户程序在内核空间中的内核栈。 2. 生成用户程序的`TaskControlBlock`。 2. 设置在内核上下文中的初始上下文。 elf文件的解析也是常规的解析方式将可以Load的段加载到内存。 impl MemorySet { pub fn from_elf(elf_data: &amp;[u8]) -&gt; (Self, usize, usize) { let mut memory_set = Self::new_bare(); memory_set.map_trampoline(); let elf = xmas_elf::ElfFile::new(elf_data).unwrap(); lef elf_header = elf.header; let magic = elf_header.pt1.magic; assert_eq!(magic, [0x7f, 0x45, 0x4c, 0x46], &quot;invalid elf!&quot;); let ph_count = elf_header.pt2.ph_count(); let mut max_end_vpn = VirtPageNum(0); for i in 0..ph_count { let ph = elf.program_header(i).unwrap(); if ph.get_type().unwrap() == xmas_elf::program::Type::Load { let start_va: VirtAddr = (ph.virtual_addr() as usize).into(); let end_va: VirtAddr = (ph.virtual_addr() + ph.mem_size() as usize).into(); let mut map_perm = MapPermission::U; let ph_flags = ph.flags(); if ph_flags.is_read() { map_perm |= MapPermission::R; } if ph_flags.is_write() { map_perm |= MapPermission::W; } if ph_flags.is_execute() { map_perm |= MapPermission::X; } let map_area = MapArea::new( start_va, end_va, MapType::Framed, map_perm, ); max_end_vpn = map_area.vpn_range.get_end(); memory_set.push( map_area, Some(&amp;self.input[ph.offset() as usize..(ph.offset() + ph.file_size()) as usize]) ); } } let max_end_va: VirtAddr = max_end_vpn.into(); let mut user_stack_bottom: usize = max_end_va.into(); user_stack_bottom += PAGE_SIZE; let user_stack_top = user_stack_bottom + USER_STACK_SIZE; memory_set.push(MapArea::new( user_stack_bottom.into(), user_stack_top.into(), MapType::Framed, MapPermission::R | MapPermission::W | MapPermission::U, ), None); memory_set.push(MapArea::new( TRAP_CONTEXT.into(), TRAMPOLINE.into(), MapType::Framed, MapPermission::R | MapPermission::W, ), None); (memory_set, user_stack_top, elf.header.pt2.entry_point() as usize) } } ​ from_elf的任务就是根据一个elf文件生成一个memory_set。具体来说要做以下几件事 在用户地址空间的最后一项映射到trampoline。 分析每个需要加载的段的地址范围和执行权限，使用memory_set的push方法将这些地址进行映射和内容加载。 设置guard_page和user_stack，最后trap上下文从虚拟地址的倒数第二个页映射到一个页帧。 返回memory_set，用户栈地址，用户程序入口。 执行用户程序 第一次执行程序 fn run_first_task(&amp;self) -&gt; !{ let mut inner = self.inner.exclusive_access(); let next_task = &amp;mut inner.tasks[0]l next_task.task_status = TaskStatus::Running; let next_task_cx_ptr = &amp;next_task.task_cx as *const TaskContext; drop(inner); let mut __unused = TaskContext::zero_init(); unsafe { __switch(&amp;mut _unused as *mut, next_task_cx_ptr); } panic!(&quot;Unreachable in run_first_task!&quot;); } ​ 这里首先会调用__switch函数，这个函数如下： __switch: sd sp, 8(a0) sd ra, 0(a0) .set n, 0 .rept 12 SAVE_SN %n .set n, n+1 .endr ld ra, 0(a1) .set n, 0 .rept 12 LOAD_SN %n .set n, n+1 .endr ld sp, 8(a1) ret 对于第一个任务，这里的sd sp, 8(a0)和sd ra, 0(a0)是没有意义的，我们直接看load相关的部分。将trap_return放入到ra，同时加载kernel stack的虚拟地址。 pub fn trap_return() -&gt; !{ set_user_trap_entry(); let trap_cx_ptr = TRAP_CONTEXT; let user_satp = current_user_token(); extern &quot;C&quot; { fn __alltraps(); fn __restore(); } let restore_va = __restore as usize - __alltraps as usize + TRAMPOLINE; unsafe { asm!( &quot;fence.i&quot;, &quot;jr {restore_va}&quot;, restore_va = in(reg) restore_va, in(&quot;a0&quot;) trap_cx_ptr, in(&quot;a1&quot;) user_satp, options(noreturn) ); } } ​ 这里a0是我们提前构造好的进入用户态所需要的上下文，a1中是我们需要的用户程序的root_ppn。之后进行跳转即可, 由于跳转的虚拟地址是我们自己构造的，我们将这个地址设在最后一个页，如果直接call __restore，会错误地进行跳转，因为__restore由编译器生成的地址不是我们在页表中设定的。 __restore: csrw satp, a1 sfence.vma csrw sscratch, a0 mv sp, a0 ld t0, 32*8(sp) ld t1, 33*8(sp) csrw sstatus, t0 csrw sepc, t1 ld x1, 1*8(sp) ld x3, 3*8(sp) .set n, 5 .rept 27 LOAD_GP %n .set n, n+1 .endr ld sp, 2*8(sp) sret ​ 这里首先将用户的root_ppn加载到satp, 由于TLB的表项无效了，需要使用sfence.vma进行刷新。a0中是用户上下文的地址，它位于用户空间的倒数第二个页帧。根据其中的上下文进行恢复，这里最后一步将用户空间的用户栈的地址赋给sp, sret进入到用户程序的入口地址。 执行系统调用 ​ 我们执行开始程序的时候已经将stvec设置为了trampoline的地址，于是一旦系统调用发生，我们就会跳转到trampoline处代码如下。 .section .text.trampoline .global __alltraps .global __restore .align 2 __alltraps: csrrw sp, sscratch, sp sd x1, 1*8(sp) sd x3, 3*8(sp) .set n, 5 .rept 27 SAVE_GP %n .set n, n+1 .endr csrr t0, sstatus csrr t1, sepc sd t0, 32*8(sp) sd t1, 33*8(sp) csrr t2, sscratch sd t2, 2*8(sp) ld t0, 34*8(sp) ld t1, 36*8(sp) ld sp, 35*8(sp) csrw satp, t0 sfence.vma jr t1 ​ 这里sscratch中是用户空间中trap上下文的地址，由于现在只能访问用户空间，所以我们在用户空间的倒数第二个页专门设置了这样一个位置，用户存储上下文。t0、t1、sp中分别加载kernel_satp、trap_handler、kernel_stack。于是加载t0到satp就可以切换地址空间，之后跳转到trap_handler处。trap_handler结束，又会调用trap_return就和一开始一样了。 ","link":"https://porterlu.github.io/post/rcore-di-si-zhang/"},{"title":"RUST所有权和内存模型 ","content":"借用和引用 ​ 在深入所有权之前，先再次介绍引用。引用是一种Rust提供的指针语义，它基于指针实现，引用可以看作某块内存的别名，它需要满足编译器的各种安全检查规则。我们使用&amp;表示不可变应用类型，同时使用&amp;mut表示可变引用类型。接着介绍借用的概念，通过&amp;接上一个变量，可以实现对于所有权的借用，借用所有权并不会让所有权发生转移，但是所有者会受到如下的限制： 在不可变借用期间，所有者不能修改内存内容，也不能再次出借为可变借用。 在可变借用期间，所有者不能访问内存内容，并且不能再次出借。 ​ 当借用者离开作用域时，所有权就会归还。同时借用有如下的规则： 借用的声明周期不能长于出借方的声明周期。 可变借用不能有多个。 不可变借用不能出借为可变借用。 这里举几个常见例子。 例子1 fn main() { let mut a = vec![1, 2, 3]; let b: &amp;mut Vec&lt;i32&gt; = &amp;mut a; for i in b { println!(&quot;{}&quot;, i); } println!(&quot;{}&quot;, b[0]); } ​ 这里将a声明为一个可变的vec，b接着对a进行一次可变引用。但是b，这里在for i in b这个语句中，会自动调用into_iter(b)导致了所有权的转移，接着使用b[0]就会出错。但是如果改为b.iter()，会自动扩展为(&amp;mut *b).iter()，这样循环结束后，由于再借用结束，b就可以继续使用。 例子2 impl List { //... pub fn push(&amp;mut self, elem: i32) { let new_code = Box::new(Node { elem: next: self.head }) self.head = Link::More(new_node); } } ​ 如上是一个栈的实现代码，这里next: self.head会报错，因为self这里已经被借用，被借用后就不能再移动了，除非所有权被归还。 例子3 #[derive(Debug)] struct Student { age: i32, id: i32, } fn main() { let mut a = Student { age: 1, id: 0 }; let b = &amp;mut a.age; let c = b; *c = 3 } 如上的代码可以被正确的执行，因为它并不违反可变借用只能有一个的原则，这里b对a进行了可变借用，但是b右将所有权交给了c，从始至终只有一个可变引用，不违反借用规则。 例子4 fn main() { let mut a = 1; let b = &amp;a; let c = &amp;a; println!(&quot;{} {}&quot;, b, c); } 这里由于默认是不可变借用，所以根据借用规则，不可变借用可以有多个。 内存模型 ​ 这里介绍了各种只能指针和引用在内存是如何分布的，大部分都是使用一个类似指针的一个变量指向内存的一个位置。 ","link":"https://porterlu.github.io/post/rust-suo-you-quan-he-nei-cun-mo-xing/"},{"title":"rCore第三章 ","content":"加载 ​ 我们需要将程序根据编号加载到内存的对应位置，那么OS就可以根据编号选择对应的任务进行调度。由于不同程序所在的地址不同，所以链接文件的基地址也必须不同，这里使用一个python脚本进行实现。 import os base_address = 0x80400000 step = 0x20000 linker = 'src/linker.ld' app_id = 0 apps = os.listdir('src/bin') apps.sort() for app in apps: app = app[:app.find('.')] lines = [] lines_before = [] with open(linker, 'r') as f: for line in f.readlines(): lines_before.append(line) lines = line.replace(hex(base_address), hex(base_address+step*app_id)) lines.append(line) with open(linker, 'w+') as f: f.writelines(lines) os.system('cargo build --bin %s --release' % app) print('[build.py] application %s start with address %s' % (app, hex(base_address+step*app_id))) with open(linker, 'w+') as f: f.writelines(lines_before) app_id = app_id + 1 ​ 这个python脚本对于每一个app源文件都做相同的操作，将链接文件中的base_address替换为app_id对应的地址使用的地址，之后使用cargo build进行编译，编译完成后就可以恢复链接文件。 ​ 编译这些编译完成后的二进制文件，需要加载到内存中 pub fn load_apps() { extern &quot;C&quot; { fn _num_app(); } let num_app_ptr = _num_app as usize as *const usize; let num_app = get_num_app(); let app_start = unsafe { core::slice::from_raw_parts(num_app_ptr.add(1), num_app+1) }; for i in 0..num_app { let base_i = get_base_i(i); (base_i..base_i+APP_SIZE_LIMIT).for_each(|addr| unsafe { (addr as *mut u8).write_volatile(0) }) let src = unsafe { core::slice::from_raw_parts( app_start[i] as *const u8, app_start[i+1] - app_start[i] ) }; let dst = unsafe { core::slice::from_raw_parts_mut(base_i as *mut u8, src.len()) }; dst.copy_from_slice(src); } unsafe { asm!(&quot;fence.i&quot;::::&quot;volatile&quot;);} } ​ 由于布局信息的格式从地址到高地址为，app_num、第一个程序起始地址、第二个程序的起始地址、...、最后一个程序的起始地址、最后一个程序的结束地址。首先生成首地址，这里使用unsafe去生成一个引用。这里OS的视角最后看到的所有程序，会像一个数组一样分布在内存空间，做这个假设的前提是每个程序占用的空间相同，由于我们现在的程序都不大，所以可以通过一个APP_SIZE_LIMIT进行限制。这个数组的每个区间一开始都要清零，之后通过如下关键语句将代码拷贝到每个区间的首地址。 let src = unsafe { core::slice::from_raw_parts( app_start[i] as *const u8, app_start[i+1] - app_start[i] ) }; let dst = unsafe { core::slice::from_raw_parts_mut(base_i as *mut u8, src.len()) }; dst.copy_from_slice(src); ​ 于是我们执行的时候就可以通过跳转到对应的区间入口，进行程序调度。 任务切换 ​ 一个任务在运行中途可以主动或者被动地交出执行权，为了保障程序的正确执行，我们需要保存其上下文。这里的上下文切换和trap上下文切换不同，它不涉及特权级的转化，一部分寄存器的保存由编译器进行完成。这里的任务切换，实际上是在进入了trap控制流后调用__switch函数，这里的switch和普通函数的区别就是其会换栈。 ​ 如上图所示就是TaskManager的结构，每一个TaskControlBlock负责一个任务的状态管理，而current指向当前正在执行的任务。我们要进行任务切换时会执行类似如下的语句： TaskContext *current_task_cx_ptr = &amp;tasks[current].task_cx; TaskContext *next_task_cx_ptr = &amp;tasks[next].task_cx; ​ 我们执行时在trap中会进行一个换栈，将执行流转移到下一个任务，使用下一个任务的trap上下文返回到用户态。具体执行的代码如下： fn find_next_task(&amp;self) -&gt; Option&lt;usize&gt; { let inner = self.inner.exclusive_access(); let current = inner.current_task; (current+1..current+self.num_app+1) .map(|id| id%self.num_app) .find(|id| inner.tasks[*id].task_status == TaskStatus::Ready) } fn run_next_task(&amp;self) { if let Some(next) = self.find_next_task() { let mut inner = self.inner.exclusive_access(); let current = inner.current_task; inner.tasks[next].task_status = TaskStatus::Running; inner.current_task = next; let current_task_cx_ptr = &amp;mut inner.tasks[current].task_cx as *mut TaskContext; let next_task_cx_ptr = &amp;inner.tasks[next].task_cx as *const TaskContext; drop(inner); unsafe { __switch(current_task_cx_ptr, next_task_cx_ptr); } } else { println!(&quot;All applications completed&quot;); use crate::board::QEMUExit; crate::board::QEMU_EXIT_HANDLE.exit_success(); } } ​ 其中find_next_task这个函数用于找到下一个状态为Ready的任务, 并返回这个任务的id。接下来对run_next_task进行解析，这里使用let Some()语法，直接返回的结果赋值到next中，之后使用current_task_cx_ptr和next_task_cx_ptr表示当前任务和下一个任务的上下文，最后在unsafe中调用__switch函数切换上下文。 .section .text .global __switch __switch: sd sp, 8(a0) sd ra, 0(a0) .set n, 0 .rept 12 SAVE_SN %n .set n, n+1 .endr ld ra, 0(a1) .set n, 0 .rept 12 LOAD_SN %n .set n, n+1 .endr ld sp, 8(a1) ret ​ 这里ra中最后将是一个任务的地址，ret后将执行下一个任务。下图就是switch过程发生的变化，switch在执行load后就转化到了另外一个任务的上下文，trap上下文也是另外一个任务的内核栈。 第一次执行 ​ 第一次执行，我们需要自己构造任务的上下文，然后使用上一章就提到的restore方法进行启动。在TaskManager结构体中实现了方法run_first_task： fn run_first_task(&amp;self) -&gt; !{ let mut inner = self.inner.exclusive_access(); let task0 = &amp;mut inner.task[0]; task0.task_status = TaskStatus::Running; let next_task_cx_ptr = &amp;task0.task_cx as *const TaskContext; drop(inner); let mut _unused = TaskContext::zero_init(); unsafe { __switch(&amp;mut _unused as *mut TaskContext, next_task_cx_ptr); } panic!(&quot;unreachable in run_first_task!&quot;); } ​ 这里直接使用了task[0]，原因是TaskManager使用了lazy_static! 第一次使用时会被初始化。 pub static ref TASK_MANAGER: TaskManager = { let num_app = get_num_app(); let mut tasks = [TaskControlBlock{ task_cx: TaskContext::zero_init(), task_status: TaskStatus::UnInit, }; MAX_APP_NUM]; for (i, task) in tasks.iter_mut().enumerate(){ task.task_cx = TaskContext::goto_restore(init_app_cx(i)); task.task_status = TaskStatus::Ready; } TaskManager { num_app, inner: unsafe { UPSafeCell::new(TaskManagerInner{ tasks, current_task: 0, }) }, } }; ​ 其中TaskContext::goto_restore(init_app_cx(i))用于初始化TaskContext: pub fn init_app_cx(app_id:usize) -&gt; usize { KERNEL_STACK[app_id].push_context(TrapContext::app_init_context( get_base_i(app_id), USER_STACK[app_id].get_sp(), )) } impl TaskContext { //... pub fn goto_restore(kstack_ptr: usize) -&gt; Self { extern &quot;C&quot; { fn __restore(); } Self { ra: __restore as usize, sp: kstack_ptr, s: [0; 12], } } } ​ 这里引用第二章中的内核栈的解析图，如上图所示，在init_app_cx中，执行了一个push_context，这个函数用于将这个生成的上下文内容压入内核栈。但是这里我们需要的是任务上下文，所以有所不同，所以需要进一步的封装。 ​ ​ 这里将entry即ra赋值为restore的地址，同时kernel stack区域即sp赋值为内核栈指针当前位置。之后根据代码可以看到将每一个任务的状态设置为ready后。最后对于整个TaskManager进行初始化，用UnSafeCell对于整个结构进行包裹。 ​ 有了这个任务上下文之后，回看之前的启动代码，就可以进行启动。 unsafe { __switch(&amp;mut _unused as *mut TaskContext, next_task_cx_ptr); } ​ 这里将task0的上下文作为next_task_cx_ptr，调用switch之后就可以切换到task0的上下文。之后由于将ra的地址设置为了restore，所以switch的ret之后会调用restore。restore回使用sp作为内核栈的地址，将内核栈中的内容作为上下文进行还原，于是这个任务就启动了。 系统调用 ​ 这一章实现了两个系统调用，sys_yield和sys_exit。如下是两个系统调用的具体实现： use crate::task::suspend_current_and_run_next; pub fn sys_yield() -&gt; isize { suspend_current_and_run_next(); 0 } pub fn sys_exit(exit_code: i32) -&gt; !{ println!(&quot;[kernel] Application exited with code {}&quot;, exit_code); exit_current_and_run_next(); panic!(&quot;Unreachable in sys_exit!&quot;); } ​ suspend_current_and_run_next() 和 exit_current_and_run_next都调用了TaskManager中的实现。 pub fn suspend_current_and_run_next() { mark_current_suspend(); run_next_task(); } pub fn exit_current_and_run_next() { mark_current_exited(); run_next_task(); } fn mark_current_suspended() { TASK_MANAGER.mark_current_suspended(); } fn mark_current_exited() { TASK_MANAGER.mark_current_exited(); } fn run_next_task() { TASK_MANAGER.run_next_task(); } impl TaskManager { fn mark_current_suspended(&amp;self) { let mut inner = self.inner.borrow_mut(); let current = inner.current_task; inner.tasks[current].task_status = TaskStatus::Ready; } fn mark_current_exited(&amp;self){ let mut inner = self.inner.borrow_mut(); let current = inner.current_task; inner.tasks[current].task_status = TaskStatus::Exited; } } ​ yield和exit的区别就是，yield的任务只是将任务的状态设置为READY，而exit会将任务的状态设置为EXIT导致任务不再被调度。最后这两个系统调用都会将执行权交给下一个任务。 时钟中断 ​ 这里时钟中断用于实现抢占式调度，抢占式调度可以使得应用随时被调度，而不用等待应用主动交出执行权。我们这里要求每个应用只能执行一段时间，之后内核就会强制将它切换出去。 ​ 我们使用时间片轮转调度的机制，这里主要依赖硬件提供的时间中断进行实现： Interrupt Exception Code Description 1 1 Supervisor software interrupt 1 3 Machine software interrupt 1 5 Supervisor timer interrupt 1 7 Machine timer interrupt 1 9 Supervisor external interrupt 1 11 Machine external interrupt 而中断是否屏蔽会根据如下方式进行判断： 如果中断的特权级低于CPU当前的特权级，则该中断会被屏蔽； 如果中断的特权级高于CPU当前特权级或者相同，则会通过sstatus或者mstatus进行判断。 ​ 我们在这里不考虑中断嵌套的情况，我们只考虑S特权级的情况。 ​ 我们使用SBI的来实现相关函数： use riscv::register::time; //获取mtime的计数值 pub fn get_time() -&gt; usize { time::read() } ​ 之后是设置时钟相关的部分： const SBI_SET_TIMER: usize = 0; pub fn set_timer(timer: usize) { sbi_call(SBI_SET_TIMER, timer, 0, 0); } use crate::config::CLOCK_FREQ; const TICKS_PER_SEC: usize = 100; pub fn set_next_trigger() { set_timer(get_time() + CLOCK_FREQ / TICKS_PER_SEC); } ​ 这里的set_timer或设置mtimecmp，于是CLOCK_FREQ/TICKS_PER_SRC就可以设置多少个mtime可以触发一次时钟中断。 ​ 于是我们就可以编写时钟中断的处理函数： match scause.cause() { Trap::Interrupt(Interrupt::SupervisorTimer) =&gt; { set_next_trigger(); suspend_current_and_run_next(); } } ​ 这里直接设置下一次触发时钟中断的时间，同时将当前任务设置为READY并调度下一个任务即可。 ","link":"https://porterlu.github.io/post/rcore-di-san-zhang/"},{"title":"Revisit rCore 第二章 ","content":"trap初始化 ​ 在主函数中清空bss段后，执行了三个函数trap::init、batch::init、batch::run_next_app。首先分析trap::init， pub fn init() { extern &quot;C&quot; { fn __alltraps(); } unsafe { stvec::write(__alltraps as usize, TrapMode::Direct) } } ​ 这段代码首先引入了在汇编代码中说明的__alltraps，这个是trap的入口，之后通过unsafe将这个值写入stvec寄存器，并且模式设置为direct，说明入口函数有且仅有一个。 batch初始化 ​ batch模块中的init函数十分简单： pub fn init() { print_app_info(); } ​ 之后我们可以看看是如何进行输出的，这个是当前模块中AppManager中的pub方法。 impl AppManager { pub fn print_app_info(&amp;self){ println!(&quot;[kernel] num_app = {}&quot;, self.num_app); for i in 0..self.num_app { println!( &quot;[kernel] app_{} [{:#x}, {:#x})&quot;, i, self.app_start[i], self.app_start[i+1] ); } } //... } ​ 这里对于AppManager的结构进行遍历，输出了各个app的地址范围。之所以没有初始化App仍旧能够输出，这里是因为使用lazy_static。 lazy_static! { static ref APP_MANAGER: UPSafeCell&lt;AppManager&gt; = unsafe { UPSafeCell::new({ extern &quot;C&quot;{ fn _num_app(); } let num_app_ptr = _num_app as usize as *const usize; let num_app = num_app_ptr.read_volatile(); let mut app_start:[usize; MAX_APP_NUM+1] = [0; MAX_APP_NUM+1]; let app_start_raw: &amp;[usize] = core::slice::from_raw_parts[num_app_ptr.add(1), num_app+1]; app_start[..=].copy_from_slice(app_start_raw); AppManager { num_app, current_app: 0, app_start, } }) }; } ​ 在第一次使用时会进行初始化，这里根据地址值和长度生成一个引用的结构，因为rust中引用是一个固定长度的结构，由一个地址和一个长度构成，指向堆区。我们读出地址值作为*const usize，通过from_raw_part我们可以直接构造一个引用，最后间数据拷贝到我们的app_start中，最后对AppManager的进行初始化。 调度 ​ 在main函数中，执行完前两个初始化函数后就会执行run_next_app()，它的内容如下： pub fn run_next_app() -&gt; !{ let mut app_manager = APP_MANAGER.exclusive_access(); let current_app = app_manager.get_cuurent_app(); unsafe{ app_manager.load_app(current_app); } app_manager.move_to_next_app(); drop(app_manager); extern &quot;C&quot; { fn __restore(cx_addr: usize); } unsafe { __restore(KERNEL_STACK.push_context(TrapContext::app_init_context( APP_BASE_ADDRESS, USER_STACK.get_sp(), )) as *const _ as usize); } panic!(&quot;Unreachable in batch::run_current_app!&quot;); } ​ 这里调用了AppManager的exclusive_access方法，这个方法用于解决实现全局可访问的可变变量的问题。它的结构如下： UPSafeCell use core::cell::{RefCell, RefMut}; pub struct UPSafeCell&lt;T&gt; { inner: RefCell&lt;T&gt;, } unsafe impl&lt;T&gt; Sync for UPSafeCell&lt;T&gt; {} impl&lt;T&gt; UPSafeCell&lt;T&gt; { pub unsafe fn new(value: T) -&gt; Self { Self { inner: RefCell::new(value), } } pub fn exclusive_access(&amp;self) -&gt; RefMut&lt;'_, T&gt; { self.inner.borrow_mut() } } ​ 我们不使用static mut这种使用方法是unsafe的，所以我们使用RefCell这个方法可以在运行时改变自己的可访问性，通过实现Sync向编译器说明，由我们保证并发时的安全。然后，我们实现exclusive_access方法，返回一个mut，如果同时有两个mut，就会发生panic，所以我们使用必须在申请使用完后，马上进行销毁。 加载运行 ​ 获取到AppManager的结构体后，可以使用load_app方法将app加载到对应的位置。我们可以查看这个函数的实现 unsafe fn load_app(&amp;self, app_id: usize) { if app_id &gt;= self.num_app { println!(&quot;All application completed!&quot;); use crate::board::QEMUExit; crate::board::QEMU_EXIT_HANDLE.exit_success(); } println!(&quot;[kernel] Loading app_{}&quot;, app_id); asm!(&quot;fence.i&quot;); core::slice::from_raw_parts_mut(APP_BASE_ADDRESS as *mut u8, APP_SIZE_LIMIT).fill(0); let app_src = core::slice::from_raw_parts( self.app_start[app_id] as *const u8, self.app_start[app_id+1] - self.app_start[app_id], ); let app_dst = core::slice::from_raw_parts_mut(APP_BASE_ADDRESS as *mut u8, app_src.len()); app_dst.copy_from_slice(app_src); } ​ 这里构造了两个引用进行数据拷贝，又有一个引用负责进行将程序运行的区域进行初始化归零。第一个: core::slice::from_raw_parts_mut(APP_BASE_ADDRESS as *mut u8, APP_SIZE_LIMIT).fill(0); ​ 将这个区域置零，由于我们的测试程序都不大，可以通过一个APP_SIZE_LIMIT直接写死。 let app_src = core::slice::from_raw_parts( self.app_start[app_id] as *const u8, self.app_start[app_id+1] - self.app_start[app_id], ); let app_dst = core::slice::from_raw_parts_mut(APP_BASE_ADDRESS as *mut u8, app_src.len()); app_dst.copy_from_slice(app_src); ​ 直接程序拷贝到其所要运行的地址0x80400000。 ​ 接着开始运行函数如下： unsafe { __restore(KERNEL_STACK.push_context(TrapContext::app_init_context( APP_BASE_ADDRESS, USER_STACK.get_sp(), )) as *const _ as usize); } ​ 首先这里需要创建一个内核栈的上下文。 pub fn push_context(&amp;self, cx: TrapContext) -&gt; &amp;'static mut TrapContext { let cx_ptr: *mut TrapContext = (self.get_sp() - core::mem::size_of::&lt;TrapContext&gt;()) as *mut TrapContext; unsafe { *cx_ptr = cx; } unsafe { cx_ptr.as_mut().unwrap() } } ​ 这里在内核栈的空间上申请了一个上下文大小的空间，将这个空间赋值为cx即我们传入的结构体，之后将这个结构体返回。 ​ 而这个结构体根据上方调用处的代码，是直接初始化的一个上下文结构。根据trap模块中的代码可以直接还原其结构，其中x2寄存器中存储内核栈的地址。 ​ ​ 于是__restore作为入口，通过a0传入内核栈中的上下文地址，加载上下文中的内容到寄存器，最后交换内核栈的地址到sscratch，并将用户栈指针写入sp, 将执行流转移到用户程序。 ​ 系统调用 ​ 在我们的批处理操作系统中存在一个trap_handler, 一旦用户程序进行ecall，有的trap会被委托到S模式，于是根据scause我们可以具体判断要进行哪些处理。这里在第二章中，只用考虑UserEnvCall，首先sepc+4因为，系统调用结束要执行下一条置零，之后分析具体是哪个系统调用。 pub fn trap_handler(cx: &amp;mut TrapContext) -&gt; &amp;mut TrapContext { let scause = scause::read(); let stval = stval::read(); match scause.cause(){ Trap::Exception(Exception::UserEnvCall) =&gt; { cx.sepc += 4; cx.x[10] = syscall(cx.x[17], [cx.x[10], cx.x[11], cx.x[12]]) as usize; } Trap::Exception(Exception::StoreFault) | Trap::Exception(Exception::StorePageFault) =&gt; { println!(&quot;[kernel] PageFault application, kernel killed it.&quot;); run_next_app(); } Trap::Exception(Exception::IllegalInstruction) =&gt; { println!(&quot;[kernel] IllegalInstruction in application, kernel killed it.&quot;); run_next_app(); } _ =&gt; { panic!( &quot;Unsupported trap {:?}, stval = {:#x}!&quot;, scause.cause(), stval ); } } } ​ 具体是哪个系统系统调用的判断位于syscall模块下，代码如下： pub fn syscall(syscall_id: usize, args:[usize; 3]) -&gt; isize { match syscall_id { SYSCALL_WRITE =&gt; sys_write(args[0], args[1] as *const u8, args[2]), SYSCALL_EXIT =&gt; sys_exit(args[0], as i32), _ =&gt; panic!(&quot;Unsupported syscall_id: {}&quot;, syscall_id), } } ​ 根据系统调用号，可以具体判断执行哪个系统调用，write是进行输出，并返回输出长度。而sys_exit会进行run_next_app() ​ 在运行的app中，所有的app都通过如下的模式进行，所以每个程序退出都会自动执行下一个程序。 pin extern &quot;C&quot; fn _start() -&gt; !{ clear_bss(); exit(main()); panic!(&quot;unreachable after sys_exit!&quot;); } ​ 我们可以再次考虑在系统调用的情况下上下文是如何保存和恢复的。我们首先将内核栈的地址转移到sp中，用户栈的地址转移到sscratch中，接着在内核栈中存储app的上下文。于是通过a0做作为上下文指针就可以将整个结构体传递给trap_handler。接着考虑如何进行恢复，返回时a0寄存器中的值是上下文在核内栈中的地址，根据这个上下文进行恢复即可。最后一步再次交换内核栈和用户栈的指针，就可恢复到用户栈。 __alltraps: csrrw sp, sscratch, sp sd x1, 1*8(sp) sd x3, 3*8(sp) .set n, 5 .rept 27 SAVE_GP %n .set n, n+1 .endr csrr t0, sstatus csrr t1, sepc sd t0, 32*8(sp) sd t1, 33*8(sp) csrr t2, sscratch sd t2, 2*8(sp) mv a0, sp call trap_handler __restore: mv sp, a0 ld t0, 32*8(sp) ld t1, 33*8(sp) ld t2, 2*8(sp) csrw sstatus, t0 csrw sepc, t1 csrw sscratch, t2 ld x1, 1*8(sp) ld x3, 3*8(sp) .set n, 5 .rept 27 LOAD_GP %n .set n, n+1 .endr addi sp, sp, 34*8 csrw sp, sscratch, sp sret ​ ","link":"https://porterlu.github.io/post/revisit-rcore-di-er-zhang/"},{"title":"Revisit rCore 第一章 ","content":"入口 ​ 我们要在裸机上进行编程，所以在main.rs中要加入#![no_std]，表示我们不适用标准库，其中!表示编译器内置相关的特性，需要加载文件的一开始。在编写main之前，先介绍这里的链接文件，我们不适用默认的链接模式，因为这时我们编程是要考虑操作系统不存在的情况。 OUTPUT_ARCH(riscv) ENTRY_ADDRESS(_start) BASE_ADDRESS = 0x80200000; ​ 这里设置一个变量BASE_ADDRESS, 将我们的kernel加载的位置设置为0x80200000。下面是我们空间分布： ​ ​ 这里每一个segment都是4k对齐的，通过导出设置在linker中的变量，我们可以知道各个segment的大小和偏移。由于我们在裸机上启动程序，并没有运行时为我们设置堆栈，随意我们需要自己设置。 .section .text.entry .global _start _start: la sp, boot_stack_top call rust_main .section .bss.stack .global boot_stack_lower_bound boot_stack_lower_bound: .space 4096*16 .global boot_stack_top boot_stack_top: ​ 设置完sp后，就可以跳转到main中。在main.rs中，我们引入这段代码： #![no_std] #![no_main] //... use core::arch::global_asm; global_asm!(include_str!(&quot;entry.asm&quot;)); #[no_mangle] pub fn rust_main() -&gt; !{ extern &quot;C&quot; { fn stext(); fn etext(); fn srodata(); fn erodata(); fn sdata(); fn edata(); fn boot_stack_lower_bound(); fn boot_stack_top() } //... } ​ 通过core::arch::global_asm引入这段汇编代码。在rust_main之前有个#[no_mangle]是为了防止编译器对函数名进行一个再命令，之后通过调用的形式，将我们需要的地址引入。 输出 ​ 这里我们通过opensbi进行输出，sbi是通过对ecall进行封装，这样我们就可以通过函数请求opensbi进行字符输出。 use core::arch::asm; #[inline(always)] fn sbi_call(which:usize, arg0:usize, arg1:usize, arg2:usize) -&gt; usize { let mut ret: usize; unsafe { asm!( &quot;ecall&quot;, inlateout(&quot;x10&quot;) arg0 =&gt; ret, in(&quot;x11&quot;) arg1, in(&quot;x12&quot;) arg2, in(&quot;x17&quot;) which, ); } ret } ​ 于是通过sbi_call进一步我们封装具体的功能。 pub fn console_putchar(c: usize) { sbi_call(SBI_CONSOLE_PUTCHAR, c, 0, 0); } ​ 我们需要对print宏进行编写，这样我们才能通过print进行输出。 use crate::sbi::console_putchar; use core::fmt::{self, Write}; struct Stdout; impl Write for Stdout { fn write_str(&amp;mut self, s:&amp;str) -&gt; fmt::Result { for c in s.chars() { console_putchar(c as usize); } Ok(()) } } pub fn print(args: fmt::Arguments) { Stdout.write_fmt(args).unwrap(); } ​ 这里的Write作为一个Trait只有一个方法write_str，编译器会帮我们解析格式化字符，我们只要进行输出就可以了。 ​ 在rust中，我们调用的println实际是一个宏，再去除了标准库后，我们需要自己编写println。 #[macro_export] macro_rules! print{ ($fmt:literal $(, $($(arg:tt)+)?)) =&gt; { $crate:console::print(format_args!($fmt $(, $($arg:tt)+)?)); } } ​ 这里我们匹配的意思是，前面一个字面量作为格式化字符，后面是一串的token用,进行隔开，之后将需要格式化输出的内容放入print中。 panic ​ 我们还需要编写panic相关的信息。 use core::panic::PanicInfo; use crate::sbi::shutdown; #[panic_handler] fn panic(info: &amp;PanicInfo) -&gt; !{ if let Some(location: &amp;Location) = info.location(){ println!( &quot;Panicked at {}:{} {}&quot;, location.file(), location.line(), info.message().unwrap() ) } else { println!(&quot;Panicked: {}&quot;, info.message().unwrap()); } shutdown() } ​ 发生panic时我们这里进行输出，通过#[panic_handler]说明这时panic发生时进行处理的办法，之后通过sbi进行关机 revisit main #![feature(panic_info_message)] //... #[macro_use] mod console; mod land_item; mod sbi; #[no_managle] pub fn rust_main() -&gt; !{ //... clear_bss(); println!(&quot;Hello OS&quot;); println!(&quot;.text [{:#x}, {:#x})&quot;, stext as usize, etext as usize); //... shutdown() } fn clear_bss(){ extern &quot;C&quot; { fn sbss(); fn ebss(); } (sbss as usize ..ebss as usize).foreach|x:size| { unsafe{ (x as *mut u8).write_volatile(0)} }; } ​ #![feature(panic_info_message)]说明如果我们发生panic可以输出信息。之后添加#[macro_use]导入宏，之后清空bss段，之后输出我们的段信息，最后关机。 ","link":"https://porterlu.github.io/post/revisit-rcore-di-yi-zhang/"},{"title":"Linux VM实验","content":"实验环境 CPU： i5-6500 OS: ubuntu20.04 kernel: Linux-6.1.0 模块说明 ​ 我们需要编写一个模块mtest_proc_write ，我们只需要编写.write即可。 static struct proc_ops mtest_proc_fops = { .proc_write = mtest_proc_write } ​ 接着我们需要编写mtest_proc_write函数，函数需要根据输入，判断执行哪个功能。具体操作是使用copy from user，将数组拷贝到内核，然后解析输入的参数，调用相应的函数。 static ssize_t mtest_proc_write(struct file *file, const chat __user *buffer, size_t count, loff_t *data){ unsigned long int addr; unsigned long int value; int copy_return_msg = copy_from_user(proc_buf, buffer, count); if(copy_return_msg != 0){ printk(KERN_ERR &quot;Error occurred when copying from user...\\n&quot;); return -EFAULT; } //... if(strncmp(proc_buf), &quot;findpage&quot;, 8) == 0){ sscanf(proc_buf + 9, &quot;%lx&quot;, &amp;addr); //.. mtest_find_page(addr); } //... } 任务1：进程虚拟地址输出 ​ 在Linux中，每个进程都有自己的进程描述符task_struct，当前进程的进程描述符可以通过current中获取。而task_struct中的mm_struct mm 指向了进程的内存描述符，我们可以在源码中的include/linux/mm_types.h中找到，在该版本中，定义位于512行，描述了进程的虚拟地址空间信息。 struct mm_struct { struct { struct vm_area_struct *mmap; /* list of VMAs */ struct rb_root mm_rb; u64 vmacache_seqnum; /* per-thread vmacache */ //... unsigned long mmap_base; /* base of mmap area */ unsigned long mmap_legacy_base; /* base of mmap area in bottom-up allocations */ //... unsigned long task_size; /* size of task vm space */ unsigned long highest_vm_end; /* highest vma end address */ pgd_t * pgd; } //.. } ​ 而vm_area_struct是一个双向循环链表的结构，它的成员如下： struct vm_area_struct { unsigned long vm_start; /* Our start address within vm_mm. */ unsigned long vm_end; /* The first byte after our end address within vm_mm. */ /* linked list of VM areas per task, sorted by address */ struct vm_area_struct *vm_next, *vm_prev; struct rb_node vm_rb; // ... struct mm_struct *vm_mm; /* The address space we belong to. */ unsigned long vm_flags; /* Flags, see mm.h. */ }; ​ 这个结构体的作用是将进程的虚拟内存地址组织成一个双向循环链表，双向循环链表中每个节点都包含了这段虚拟地址的起始地址vm_start和结束地址vm_end，拥有这个结构我们就可以遍历一个进程的虚拟地址空间。 static void mtest_list_vma(void){ struct vm_area_struct *cur = current-&gt;mm-&gt;mmap; printk(KERN_INFO &quot;Current process ID: %d&quot;, current-&gt;pid); while(cur) { // Permission char permission[5] = &quot;----&quot;; // Read if(cur-&gt;vm_flags &amp; VM_READ) { permission[0] = 'r'; } // Write if(cur-&gt;vm_flags &amp; VM_WRITE) { permission[1] = 'w'; } // Execute if(cur-&gt;vm_flags &amp; VM_EXEC) { permission[2] = 'x'; } // Shared or not if(cur-&gt;vm_flags &amp; VM_SHARED) { permission[3] = 's'; } else { permission[3] = 'p'; } printk(KERN_INFO &quot;0x%lx 0x%lx %s\\n&quot;, cur-&gt;vm_start, cur-&gt;vm_end, permission); cur = cur-&gt;vm_next; } } ​ 通过一个while循环结构，我们可以从头开始x遍历，并输出每个虚拟地址区间的读写执行权限，最后一项用于表达是否是共享的虚拟地址区间。 任务二：虚拟地址转化为物理地址 static struct page* find_page_based_on_vma(struct vm_area_struct *vma, unsigned long addr){ struct mm_struct *mm = vma-&gt;vm_mm; pgd_t *pgd = pgd_offset(mm, addr); p4d_t *p4d = NULL; pud_t *pud = NULL; pmd_t *pmd = NULL; pte_t *pte = NULL; struct page *page = NULL; if(pgd_none(*pgd) || pgd_bad(*pgd)) { return NULL; } p4d = p4d_offset(pgd, addr); if(p4d_none(*p4d) || p4d_bad(*p4d)) { return NULL; } pud = pud_offset(p4d, addr); if(pud_none(*pud) || pud_bad(*pud)) { return NULL; } pmd = pmd_offset(pud, addr); if(pmd_none(*pmd) || pmd_bad(*pmd)) { return NULL; } pte = pte_offset_kernel(pmd, addr); if(pte_none(*pte)) { return NULL; } page = pte_page(*pte); if(!page) { return NULL; } return page; } ​ 这里从页目录开时，将虚拟地址逐级翻译，并通过pgd_none()和 pgd_bad()等进行检查，最后返回页表描述符。 static void mtest_find_page(unsigned long addr) { struct page *page = NULL; unsigned long physical_addr; struct vm_area_struct *vma = find_vma(current-&gt;mm, addr); if(!vma){ printk(KERN_ERR &quot;Error occurred when finding vma...\\n&quot;); return ; } page = find_page_based_on_vma(vma, addr); if(!page) { printk(KERN_ERR &quot;Error occurred when finding page based on vma...\\n&quot;); return ; } physical_addr = page_to_phys(page) | (addr &amp; ~PAGE_MASK); printk(KERN_INFO &quot;Virtual Addres: 0x%lx -&gt; Physic Address: 0x%lx\\n&quot;, addr, physical_addr); } ​ 这里使用find_vma函数，这个函数将通过mm_struct结构体和虚拟地址，找到虚拟地址所在的或者下一个vm_area_struct。找到敌营的vma结构体后，就可以通过之前的函数进行虚实地址转化。这里参考的代码，mm-&gt;vma-&gt;mm-&gt;pgd，实现有点冗余。最后通过页表的基地址加上页偏移获取实际的物理地址。 任务三：写入一个虚拟地址 static void mtest_write_val(unsigned long addr, unsigned long val) { struct page *page = NULL; unsigned long *kernel_addr; struct vm_area_struct *vma = find_vma(current-&gt;mm, addr); if(!vma) { printk(KERN_ERR &quot;Error occurred when finding vma...\\n&quot;); } //If vm cannot be written if(!(vma-&gt;vm_flags &amp; VM_WRITE)) { printk(KERN_ERR &quot;Error occurred when writing to an unwritable page...&quot;); return ; } page = find_page_based_on_vma(vma, addr); if(!page) { printk(KERN_ERR &quot;Error occurred when finding page base on vma...\\n&quot;); return ; } else { kernel_addr = (unsigned long *)page_to_phys(page) | (addr &amp; ~PAGE_MASK); *kernel_addr = val; printk(KERN_INFO &quot;Successfully write %ld into virtual address: 0x%lx\\n&quot;, val, addr); } } ​ 与前面不同的是这里进行了一次写检查。 ","link":"https://porterlu.github.io/post/linux-vm-shi-yan/"},{"title":"Sanctorum：一个轻量级Enclave Secure Monitor","content":"概述 ​ Sanctorum是一个拥有小TCB的通用的Enclave框架，可以用于实现和Intel SGX相似的原语。Sanctorum实现了一套形式化证明过的Enclave实现用例说明，只要根据这套用例说明实现，就可以在一个顺序多核系统上达到基本的安全要求，具体可以参考Sanctum处理器和Keystone enclave框架。Sanctorum的设计模式，保证了硬件可以自由发展，只要硬件设计可以实现最小的隔离原语，那么Security monitor就可以用于保障其安全性。 框架 Security Monitor的可信管理 ​ 一个Enclave二进制文件是值得相信的，只要我们通过Enclave初始化时得到的测量值进行检验就可以了。Security Monitor本身当然也是可信的，通过一套安全引导协议，来为Security Monitor授予证书。硬件平台是无条件相信的，硬件定义了一系列的密码学操作，用于远程证明。在远程证明时，通过Hardware中公私钥和经过其验证的Security Monitor，可以保证这个过程的可信性。 ​ 对于Enclave本身代码，我们信任其不会自己泄露自己的信息，将自己的信息传输给潜在的攻击者。Enclave会和外界进行通信，这个通信至少会泄露时间信息，可能进一步泄露Enclave执行过程中微架构相关的信息。我们只能假设，Enclave谨慎使用通信手段，并且信任硬件。 ​ Security monitor可以将高级的Enclave原语映射到底层的硬件配置。为了达到可信，Security monitor必须检查OS的资源分配抉择，保证可信内存区间的隔离性和唯一性。Security monitor是利用底层硬件实现的隔离，为了防御侧信道攻击，在进程切换期间进行共享硬件的刷新，来保证不同隔离域间不可以通过这些硬件获取信息。 硬件平台的要求 ​ 硬件平台需要提供隔离的手段，来实现一个可信区，使得Secuirty和Enclave与不可信的OS相隔离。同时对于其他潜在的内存访问，如DMA对内存的访问也要进行隔离。 ​ 硬件提供一定手段进行共享资源的隔离，这些资源可以通过分时、分区地共享，Security Monitor进行刷新。例如，在Sanctum处理器上，所有处理器在切换上下文时，需要清空寄存器、L1 Cache等信息，同时通过L2 Cache分区，来完成隔离。 ​ 硬件需要为Security Monitor设定专门的特权级，为了防止恶意的OS对于Security Monitor的攻击，我们必须保证Security Monitor运行在比OS更高的特权级，并且这个特权级可以自由地访问物理内存。同时Security Monitor还必须能知道或者截获任何异常和中断，因为这些时间可能对于整个执行流都由影响，Security Monitor必须进行管理和分配。例如，一个OS可以触发一个软中断，但是Security Monitor必须收到这个中断，然后进行一个enclave之后，之后再转移执行权限给操作系统。 ​ 硬件需要提供密码学相关的证明手段。Enclave可以通过一个可信的随机源来生成随机数，硬件还必须为Security Monitor提供一套公私钥保证证明，这些可以用于远程证明和信根测量。 Security Monitor具体实现 Security Monitor接口 ​ Enclave和OS必须通过Security Monitor API进行资源的分配，这个是通过Machine Mode syscall实现的。Security Monitor还可以插入任何一个异常和中断处理。于是中断和异常的管理可以由下图所示： ​ 如果是本身执行的OS的代码，直接转发给OS进行执行就可以了。如果是Enclave代码正在执行，则要分为两种情况，如果是需要交给OS进行实现的功能，需要进行一个异步退出，保存Enclave状态，将权限交给OS；如果是Enclave本身就实现处理函数的功能，则将给Enclave进行执行。对于一个Machine Mode syscall进行两部检查，一是身份检查，二是请求合法性检查，成功则进行调用。 Security Monitor资源管理 ​ 资源管理的要求就是可信区代码之间要进行强隔离，同时为了防御一些侧信道，通过共享的资源来获取泄露的信息的攻击方法，隔离区域之间不能有重合。这也意味着一个可信区Enclave不能直接修改其他可信区的资源分配，否则一个恶意的OS可以通过移除一部分内存资源，来获取page fault时的信息。但是这也不意味着资源分配是静态的，Enclave可以通过OS进行新的资源的申请，通过通过Security Monitor syscall扩展可信区。 Security Monitor的Enclave管理 ​ Security Monitor保证了enclave可以很强的隔离并且使用硬件的共享资源。一个最小的Enclave至少拥有自己的私有地址空间和页表。同时在Security Monitor的空间还维护了Enclave的元数据，比如线程的ID，Enclave的硬件资源，信箱等等。通过Enclave ID就可以访问这些资源，当然Security Monitor会对访问者的身份进行验证。Enclave通过自己私有的页表进行虚拟地址空间访问，跟上面提到的一样，访问和OS共享的空间可能泄露信息。 上面提到了Enclave所拥有的资源，这些在Enclave初始化时时就要进行说明，这一步在会创建Enclave的数据结构，申请物理地址空间，建立虚拟地址到物理地址的映射，最后通过Security Monitor请求会对这个Enclave进行一个&quot;Seal&quot;，Security Monitor中会对已经建立元数据进行测量，之后就可以调度到一个核上。 ​ Enclave线程会在核上一直执行，除非Security Monitor syscall的enclave退出或者一个Enclave异步退出。在一个Enclave退出发生时，Security Monitor会保存状态，将状态信息保存到Enclave的元数据结构中。当再次进入Enclave，会从入口点进入，从Enclave元数据区恢复执行状态。同时在将执行权交给OS前，要清空核的状态。线程的状态机转移如下图所示，在Security Monitor中会维护线程的元数据信息，包括Enclave信息、线程锁、线程所在的核等等。一旦一个线程被创建可以分配一个Enclave，一旦一个Enclave被销毁，这个线程会被block，这样它就可以被清空或者重新分配给一个Enclave。 Enclave证明 测量 ​ 在每一个Enclave初始化时，Security Monitor都会使用SHA3对Enclave进行一个哈希。这个测量值会覆盖Enclave的配置信息，Enclave私有空间信息，全局状态信息。为了保证这个测量值的唯一性，这里要求虚拟地址到物理地址的映射是唯一的，并且页的数量不能减少。最后，Enclave的页表必须要在数据加载前，已经初始化完毕。对于Enclave初始状态的测量可以用于验证一个Enclave，Security Monitor保证一个Enclave的测量值在初始化完成后不会再改变，在远程验证和本地验证中这点都很重要。 本地证明 ​ Enclave如果想证明或者验证本地的另外一个Enclave，Security Monitor提供了手段。Security Monitor保证了完整性和发送者的身份，这样就不需要通过密码学证明来进行这个验证，因为Security Monitor对于其他软件本身就有可信的权威。具体的实现上，Security Monitor在元数据区为每个Enclave实现了一个Buffer，我们称之为“mailbox”，通过Security Monitor API, 发送者可以接受和发送消息。为了防止一个发送者拒绝服务攻击，接受者需要显示声明它们希望从某个发送人那接受到消息。 远程证明 ​ 进行一次远程的可信通信需要密码学操作的帮助。这个过程开始于一次密钥协议建立的请求，首先在本地生成一个随机数发送给远端，远端Enclave接受到后需要将收到的随机数发送给自己的Signing Enclave，Signing Enclave会对随机数和接受的Enclave的测量值进行签名，接着发送给接受的Enclave。接受的Enclave接受到&quot;mailbox&quot;中的邮件后，附上Security Monitor的证书作为邮件，发送给本地的我们，这里在本地使用处理器的公钥对证书进行验证，之后对邮件内容进行验证，就可以判断远端是否可信。 总结 ​ Sanctum使用了64个DRAM隔离域，每个隔离域是32MB。隔离域之间在L2 Cache就进行隔离，同时专门的PTW保证了Enclave使用自己的页表进行访问，一旦一个隔离域进行再分配，TLB要进行击落。同时Sanctum处理器设计一套安全引导方法，用于建立信根，这个再后面的Keystone和蓬莱同样使用。 ​ Keystone是一个开源TEE实现，利用RISC-V本身的PMP，且不用修改硬件。PMP实现了内存访问的白名单机制，通过PMP，Keystone实现了Sanctorum的相同目标。为了实现内存隔离，Keystone使用PMP为自己隔离了一片区域，只能在M mode下进行访问, OS只能访问其他区域。Enclave也相似地申请这样一块块区域进行隔离，同时Enclave使用自己的页表进行访问。如果要访问共享的区域，Enclave拥有一款共享的Buffer，这块内存同样也映射到页表中。Keystone并没有在微架构层面进行Cache的隔离。 ​ 最后，Sanctum is a great work，但是现在只是为后来者提供养分了。 ","link":"https://porterlu.github.io/post/sanctorumyi-ge-qing-liang-ji-enclave-secure-monitor/"},{"title":"内核进程实验","content":"实验目的 ​ 通过/proc/&lt;PID&gt;/ctx，查看进程被调用的次数。认识linux中的进程结构。 实验环境 宿主机：Ubuntu 20.04 内核版本：v6.1 实验内容 task_struct ​ 在linux中使用task_struct作为进程描述符，用于进程管理。我们可以在include/linux/sched.h中看到它的结构。下面是参考的报告中的内容： struct task_struct { int ctx; //add a type int on_rq; int prio; int static_prio; int normal_prio; unsigned int rt_priority; //... } 进程的创建 ​ linux进程创建的代码位于kernel/fork.c中。 ​ 首先分析Linux 中三个系统调用fork，vfork, clone，这三个系统调用都在设置参数后，通过_do_fork返回。 long _do_fork(struct kernel_clone_args *args){ //... } ​ 可以看出_do_fork()会完成进程的复制。_do_fork()会调用copy_process函数，接着添加lantent entropy，随后调用trace_sched_process_fork()函数唤醒新的线程。 p = copy_process(NULL, trace, NUMA_NO_NODE, args); add_latent_entropy(); if(IS_ERR(p)) return PTR_ERR(p); /* * Do this prior waking up the new thread - the thread pointer * might get invalid affter that point, if the thread exits quickly. */ trace_sched_process_fork(current, p); ​ 加下来是copy_process的函数说明, 我们在这一步进行ctx变量的初始化。 static __latent_entropy struct task_struct *copy_process( struct pid *pid, int trace, int node, struct kernel_clone_args *args) { p-&gt;ctx = 0; retval = sched_fork(clone_flags, p); if(retval) goto bad_fork_cleanup_policy; //... } ​ 每次进程被调度时，我们要对其进行加1的操作，我们定位到kernel/sched/core.c中的schedule函数，我们在每次调用时，对ctx进行加1的操作。 asmlinkage __visible void __sched schedule(void){ struct task_struct *tsk = current; tsk-&gt;ctx ++; sched_submit_work(tsk); do{ preempt_disable(); __schedule(false); sched_preempt_enable_no_resched(); } while(need_resched()); sched_update_worker(tsk); } EXPORT_SYMBOL(schedule); ​ 该函数会创建指向当前进程的指针tsk，并调用sched_submit_work()死锁检测，然后在循环中进行该进程的调度，preempt_disable()用于禁用内核抢占，然后__schedule()函数进行调度。 伪文件接口 ​ linux中proc中pid对应的目录文件实现在fs/proc/base.c中，其中pid_entry数组tgid_base_stuff[]定义了可以访问的pid,我们参照personality的声明格式，定义一个handle function, 最后我们只需要调用cat /proc/[PID]/ctx就可以进行ctx输出 static int proc_pid_ctx(struct seq_file*m, struct pid_namespace *ns, struct pid *pid, struct task_struct *task){ int err = lock_trace(task); if(!err){ seq_printf(m, &quot;%d\\n&quot;, task-&gt;ctx); unlock_trace(task)； } return err; } 说明 ​ 该实验说明书写时，笔者实验环境已经被拆除，并没有进行重现，报告参考[DicardoX/SJTU_CS353_Linux_Kernel (github.com)](https://github.com/DicardoX/SJTU_CS353_Linux_Kernel) ","link":"https://porterlu.github.io/post/nei-he-jin-cheng-shi-yan/"},{"title":"内核模块编程","content":"实验目的 ​ 熟悉内核模块编程和proc文件系统的编程。 内核模块命令 插入模块：insmod hello.ko 移除模块：rmmod hello 列出模块：lsmod 查看模块信息：modinfo hello.ko 插入模块，并自动处理存在依赖关系的模块：modprobe hello.ko 模块的加载和移除 ​ 引入3个基本的头文件，其中的kernel.h中包含了打印函数printk()等基本函数原型。接着module.h的作用是动态地将模块加载到内核中，init.h包含了模块的初始化的宏定义和函数的初始化函数。 #include &lt;linux/kernel.h&gt; #include &lt;linux/module.h&gt; #include &lt;linux/init.h&gt; ​ 可以通过如下代码进行加载和移除模块。 module_init(hello_init); module_exit(hello_exit); ​ 在两个函数在linux/init.h中定义，用于定义了内核模块的出入口。函数体如下： static int __init hello_init(void){ printk(KERN_INFO &quot;Hello Linux Module...\\n&quot;); return 0; } static void __exit hello_exit(void){ printk(KERN_INFO &quot;Bye.\\n&quot;); } ​ 最后某块需要对于一些作者信息，许可证的说明。 MODULE_LICENSE(&quot;GPL&quot;); MODULE_DESCRIPTION(&quot;Module1&quot;); MODULE_AUTHOR(&quot;Porterlu&quot;); 模块参数传递 ​ 接着加入两个头文件linux/moduleparam.h和linux/string.h用于参数传递和字符串处理。 static int int_var = -9999; static char *str_var = &quot;Default&quot;; static int int_array[10]; int arrNum; module_param(int_var, int, 0644); MODULE_PARM_DESC(int_var, &quot;An integer variable&quot;); module_param(str_var, charp, 0644); MODULE_PARM_DESC(str_var, &quot;A string variable&quot;); module_param_array(int_array, int, &amp;arrNum, 0644); MODULE_PARM_DESC(int_array, &quot;An integer array&quot;); ​ 参数说明完毕后，进行函数编写： static int __init hello_init(void){ int i; if(int_var == -9999 &amp;&amp; strcmp(str_var, &quot;Default&quot;) == 0 &amp;&amp; arrNum == 0){ printk(KERN_INFO &quot;No parameters input, exit.\\n&quot;); return 0; } printk(KERN_INFO &quot;The parameters are:\\n&quot;); if(int_var != -9999){ printk(KERN_INFO &quot;Int: %d\\n&quot;, int_var); } if(strcmp(str_var, &quot;Default&quot;) != 0){ printk(KERN_INFO &quot;Str: %s\\n&quot;, str_var); } if(arrNum != 0){ for(i = 0; i &lt; arrNum; i++){ printk(KERN_INFO &quot;Int_array[%d]: %d\\n&quot;, i, int_array[i]); } } } 编译成功后，使用如下的命令进行测试 insmod module.ko int_var = 123 str_var=hello int_array=1,2,3,4,5 只读文件 ​ 需要创建proc文件，读取文件内容，返回信息。 &lt;linux/proc_fs.h&gt;, 包含了一些proc文件系统的读写、创建函数。 &lt;linux/seq_file.h&gt;, 包含了seq_read,seq_lseek等顺序文件处理函数。 linux/sched.h&gt;，任务调度相关，包含了系统时间的全局变量。 static int __init hello_proc_init(void) { printk(KERN_INFO &quot;Test for modules...\\n&quot;); proc_create(&quot;hello module&quot;, 0444, NULL, &amp;hello_proc_fops); return 0; } ​ 在入口函数中，我们注册了一个proc伪文件。其中hello_proc_fops是一个结构体。 static const struct proc_ops hello_proc_fops = { .proc_open = hello_proc_open, .proc_read = seq_read, .proc_lseek = seq_lseek, .proc_release = single_release, }; ​ 除了proc_open，全是已经封装好的函数。接下来我们的hello_proc_open如下： static int hello_proc_open(struct inode *inode, struct file *file){ return single_open(file, hello_proc_show, NULL); } ​ 在open函数中调用了single_open，之后会调用hello_proc_show static int hello_proc_show(struct seq_file *m, void *v){ seq_printf(m, &quot;Current kernel time is %ld\\n&quot;, jiffies); } 读写文件 ​ 在上面模块的基础上，还要加入linux/slab.h和linux/uaccess.h,分别用于内存分配和copy_from_user函数的引入。 ​ 这里初始化添加了目录的创建和删除，要先进行目录的创建： static int __init hello_init(void){ printk(KERN_INFO &quot;Test for module...\\n&quot;); helloDir = proc_mkdir(&quot;helloDir&quot;, NULL); if(!helloDir){ return -ENOMEM; } hello = proc_create(&quot;hello&quot;, 0644, HelloDir, &amp;hello_proc_fops); } static void __exit hello_exit(void){ remove_proc_entry(&quot;hello&quot;, helloDir); remove_proc_entry(&quot;helloDir&quot;, NULL); } ​ 下面是hello_proc_show 和上一板块中讲的一样，它将由single_open进行回调，输出我们write的信息。 static char *message = NULL; struct proc_dir_entry *helloDir, *hello; static int hello_proc_show(struct seq_file *m, void *v){ seq_printf(m, &quot;Successfully read content from proc file!\\n&quot;); seq_printf(m, &quot;The message is:%s\\n&quot;, message); return 0; } ​ write使用copy_from_user将用户的数据拷贝到了内核的数组中进行输出。 static ssize_t hello_proc_write(struct file *file, const char __user *buffer, size_t count, loff_t *f_pos){ char *userBuffer = kzalloc((count+1), GFP_KERNEL); if(!userBuffer){ return -ENOMEM; } if(copy_from_user(userBuffer, buffer, count)){ kfree(userBuffer); return EFAULT; } kfree(message); message = userBuffer; return count; } ​ ","link":"https://porterlu.github.io/post/nei-he-mo-kuai-bian-cheng/"},{"title":"RISC-V轻量级安全引导方案","content":" ​ 现在的安全boot方案要么太过复杂，要么被攻击者攻破。在这种情况下，开源的安全boot架构对于设计者和白帽攻击者就很重要了。这篇文章中提出了一种轻量级的安全boot架构，这种架构高效地使用了ECDSA算法、SHA3算法和DMA。另外，架构中包含一个密钥管理单元，其中包含了一个优化的PUF为密钥在SoC上提供安全的模块。这个架构在RISC-V的SoC上得到了验证。 ​ Boot是一个安全系统的生命周期中关键的一环，很多攻击发生在这个阶段，比如可以将内核替换为一个已经被污染的内核，使得这个系统变得脆弱。所以安全系统中的一个重要准则就是，为从第一阶段引导到安全应用程序建立信任链。本篇文章提出的安全boot架构，这是第一个为RISC-V SoC构建的以硬件为基础的安全boot架构。本篇文章的架构定义了一个中心化的代码认证单元(CAU)，用于验证信任链的完整性，CAU使用了ECDSA和SHA3。高效利用ECDSA使得PUF可以轻松地对生成的非对称密钥并对其进行更管理，同时CAU使用DMA来进行代码的快速读取和认证。它整体框架如下： ​ 这篇文章主要做出了如下贡献： 提出了一种轻量级的安全boot架构来保证系统完整性和对软件提供商进行验证。 框架中包含了一个密钥管理单元，管理单元中实现了物理不可克隆功能(PUF)。 将代码认证单元整合进了RISC-V SoC。 接下来文章介绍了前人做的工作，RISC-V SoC的整体框架，安全boot的整体框架，安全性分析，性能分析。 背景和基本概念 ​ 这里列举提升RISC-V硬件安全的工作，如Shakti-T利用了基地址和界限的概念保证指针只访问合法的区域，SMARTS实现了一个内存保护单元PMU，这些工作保证了DRAM的安全；Keystone是第一个开源的利用RISC-V架构构架的TEE项目；Sanctum介绍了一种基于软件的安全boot和远程认证方式。然而，这些工作都是使用了基于软件的安全boot方式，相较于硬件实现的方式有很多缺点。 什么是Secure boot ​ 在W. A. Arbaugh, D. J. Farber, and J. M. Smith, “A secure and reliable bootstrap architecture,” in Security and Privacy, 1997. Proceedings., 1997 IEEE Symposium on. IEEE, 1997, pp. 65–71.这一篇的paper中提出了构建信任链的概念，即启动过程中每一个阶段都要验证下一个阶段的完整性。在UEFI 2.2版本以后都有安全boot的介绍，UEFI通过在boot的每一个阶段都会计算一个哈希值，然后就可以验证签名。如果这个检查失败了，那么boot就会失败；反之，boot的安全性就得到了保证。 ​ 当然安全boot有很多变种，如Intel处理器支持安全boot的两种模式，measured mode和verified mode，两种模式的信根都是微代码。在measured mode下TPM负责签注密钥，verified mode下前一阶段对于后一阶段的验证。另外一种模式，就是通过专门安全处理器，例如AMD将信根置于ROM中。ROM验证安全引导密钥，之后这个密钥将用于验证更大处理器固件代码（在flash中）。 数字签名 ​ 数字签名技术是广泛应用的技术，它应用于浏览器和邮件等服务。几乎所有的公钥加密系统都有涉及很难解决的数学问题。如，RSA基于大素数分解难题，而ECC基于椭圆曲线离散对数问题。最流行的签名方案是基于椭圆曲线的ECDSA，最受欢迎的加密方案叫ECIES，最受欢迎的密钥协议叫ECDH。 PUF ​ PUF 可以利用开机时的状态生产密钥，在商用的产品中，PUF可以用于生成和用户共享的对称密钥，也可以用于生成非对称密钥的随机种子。 RISC-V SoC ​ 这个SoC是利用lowRISC 来构建的，这个SoC集成了安全boot、加密、片外内存验证、密钥管理、密码学运算加速等功能。当然，一个安全的SoC应该从硬件和软件上都是可靠的，构建一个安全的RISC-V安全系统需要利用各种特性去防御针对硬件的攻击和来自运行在其之上的软件的威胁。 密钥管理单元，管理密钥同时将其放置到各个安全模块上。 代码认证单元，用于防御镜像修改、僵尸攻击等，和Boot-Sequencer模块一起用于实现安全boot。 安全Debug，用于抵御一些密钥窃取，非法的debug，probe等测信道攻击。 可信执行环境，就是图中标为4的单元，为应用创造隔离的执行环境。 可信片外存储，抵御测信道攻击的必要模块，同时为可信执行环境提供使用的存储空间。 安全Boot的框架 ​ 一个安全的SoC要保证从boot一开始就建立信根，并在OS运行之后一直保持。第一阶段的不变的bootloader位于只读的ROM上，这个阶段的引导将从密码学上验证第二阶段bootloader的签名，之后这个第二阶段的bootloader也会验证下一个阶段软件的签名，以此类推。到了直接链的末尾，一个可信的应用就被建立到SoC的可信执行环境上。 Berkeley Boot Loader ​ RISC-V主要的引导过程指令都被定义在Berkeley Boot Loader中，BBL将执行初始化硬件，建立页表，加载内核，启动内核等任务。整个过程是分多阶段执行的。 第一步，在RISC-V SoC中的ROM中包含了一个裸机程序，这个程序用于加载BBL，首先运行这个程序。 第二步，将BBL加载到DDR内存，实际上BBL的大小是大于ROM的，所以在RISC-V中实际上，有两个选项 SD-Boot: BBL 实际上位于外部的SD卡中。 网络启动：这个裸机程序向外发送请求。 ​ 加载进来的程序，将会是一个ELF文件包含BBL 也可能包含内核。 第三步，BBL运行在最高权限，执行一系列的初始化后，加载并执行内核。 信根 ​ boot的过程起始在ROM中的固定指令，然后跳转到BRAM中。这个过程需要使用CAU单元进行认证。 CAU ​ 代码认证单元用于验证程序的签名，通过验证每一个即将被执行的程序，就可以阻止没有被认证或者被修改过的代码的运行。将ROM中的代码可以直接放置在0x100000的位置，这个位置将直接触发CAU的检查，如果失败，则会直接挂起。 RISC-V上的linux ​ 这里使用linux版本为4.2，一个裸机程序已经直接被预先初始化到BRAM中，ROM的程序会直接跳转到BRAM的位置，开始执行将BBL的代码从SD卡拷贝到DDR。就像前面介绍的一样，Linux不是直接启动的，它是通过BBL启动的，ROM在设置完外设的其实地址后，将跳转到BRAM，这样就不需要BIOS来决定或者计算这些地址。 ​ 接下来，在S模式启动linux，当然BBL仍然运行在更高的特权级来服务外设的终端请求。 Boot Sequencer和密钥管理单元 ​ Boot Sequencer 实际上是一个有限状态机，在每个boot stage的时候进行状态切换，Boot Sequencer会调度密钥管理单元将相关的密钥发送给CAU，这样CAU就可以通过ECDSA进行验证签名。现在这里的实现只是一个有限状态机，但是可以优化为一个微控制器。 ​ 密钥管理单元实际上负责密钥的生产和传发。TRNG是随机数生成器，PUF前面已经做了介绍，OTP 是一次性存储模块。它们的协作过程如下，首先随机数生产单元生产对称密钥。并放置在存储单元。PUF取出这个密钥，并将其发送到需要的安全模块。CAU的一次性存储单元中存储了一个非对称加密算法中的私钥。 安全性分析 ​ 对比传统的安全boot方式，这篇文章的工作的优势如下： 硬件实现的验证方式，性能更高更加节能。 在RISC-V核心上进行SHA3和ECDSA运算假设了RISC-V处理器本身是可信的，如果将处理器本身考虑为TCB的一部分，那么随之而来的就是TCB过大的问题，那么就给Meltdown和Spectre等漏洞创造了攻击面。另一方面来说，这篇工作中假设CAU和Boot Sequencer是可信的，但是RISC-V处理器本身是不可信的。 这篇文章中安全boot的过程是有中心化的CAU进行控制的，所以可以地控制整个引导过程。 性能分析 CAU ​ CAU主要包含三个模块，1）DMA; 2)SHA1处理单元；3）ECDSA验证单元。DMA由Boot Sequencer进行配置，从内存中读取代码，同使用SHA3单元进行计算，密钥管理单元将公钥发送到CAU，之后有了代码的哈希，签名还有公钥，就可以进行签名的验证。 ECDSA的性能分析，性能优势非常明显： 通过只存储非0的RC和流水化，提升SHA3执行单元的频率和并减小面积。 特殊的线性反馈寄存器为随机数的生产提高稳定性。 结论 ​ 这个框架利用对于ECDSA, SHA3, PUF等单元的优化，实现了一个轻量级的RISC-V安全boot架构，未来可以做的工作还有，1.支持远程验证，2.将可信执行环境整合进SoC。这篇论文是在看Sanctum时看到的，可以看到对于前人的工作做了很多整合，具体的最大的贡献是提出了CAU和Boot Sequencer来进行验证的操作，有两个最大的优势，第一可以避免处理器漏洞对于验证过程本身可信度的威胁；第二是专门的密码学计算单元肯定是可以加速的，但是只是boot加速；可能还有没有体现出来的一点是中心化，可以配置，论文中提出这是未来可以做的点，最后的最后DMA在在这里的意义是否被体现出来，也是值得思考的问题。 ","link":"https://porterlu.github.io/post/risc-v-qing-liang-ji-an-quan-yin-dao-fang-an/"},{"title":"Spectre实验","content":"Cache timing ​ 我们需要测得Cache timing命中与不命中的时间差别。我们引入两个库： emmintrin.h x86intrin.h ​ 我们首先为了测到被访问数据，需要将cache清空，这样被访问的数据，就会因为已经进入Cache而访问时间短了一些。 int junk = 0; register uint64_t time1, time2; volatile int ui; int i; for(i=0; i&lt;10; i++){ array[i*4096] = 1; } for(i=0; i&lt;256; i++){ _mm_clflush(&amp;array[i*4096]); } array[3*4096] = 1; for(i=0; i&lt;10; i++){ addr = &amp;array[i*4096]; time1 = __rdtscp(&amp;ui); junk = *addr; time2 = __rdtscp(&amp;ui) - time1; printf(&quot;%d\\n&quot;, time2-time1); } ​ 我们通过_mm_clflush刷新cache空间，同时使用__rdtscp来获取运行的时钟周期数。 Flush and Reload ​ flush意味着我们预先将cache内的数据清空，之后我们假设被攻击的函数有如下代码。 void victim(){ temp = array[secret*4096+DELTA]; } ​ 这样我们通过遍历secret的取值范围，通过测量时间，就可以知道secret的值。 Spectre ​ spectre是利用分支预测漏洞和处理器乱序执行，来达到访问原本访问不到的数据。如被攻击函数有如下的代码。 void victim(size_t x){ if(x &lt; size){ temp = array[x*4096 + DELTA]; } } ​ 我们首先训练分支预测器，这里假设size为30， 这样有如下代码： for(int i = 0; i &lt; 30; i++){ victim(i); } ​ 接着执行victim(index_out_bound), 这样这个数据就进入了cache中，这时由于分支预测存在，这里被预测为执行成功，等到知道结果为不执行的时候，数据已经进入了cache，这样通过cache timing就可以知道这个index_out_bound到底是多少。 Attack ​ 当然我们需要获取数据，而不是一个index，我们改进代码。 s = restrictedAccess(index_beyond); if(s!=0) array[s*4096+DELTA] = 1; ​ 这样数据就进入了cache中，我们在通过cache测量获取机密数据，实践中，还可以通过多次测量减小误差。 ","link":"https://porterlu.github.io/post/spectre-shi-yan/"}]}